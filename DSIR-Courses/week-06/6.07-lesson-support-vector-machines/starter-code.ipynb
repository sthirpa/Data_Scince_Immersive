{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "*After this lesson, students will be able to:*\n",
    "1. **Describe** linear separability.\n",
    "2. **Differentiate between** maximal margin, soft-margin, and kernel SVMs\n",
    "3. **Implement** SVMs in `scikit-learn`.\n",
    "4. **Describe** the effects of `C` and kernels on SVMs.\n",
    "\n",
    "## Agenda\n",
    "I. **Support Vector Machines** (60 minutes total)\n",
    "- Intuition\n",
    "- An SVM Play, in Three Acts:\n",
    "    1. Maximal Margin SVMs\n",
    "    1. Soft-margin SVMs\n",
    "    1. Kernel SVMs\n",
    "- Kernel Trick\n",
    "\n",
    "II. **Coding** (30 minutes total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine quality data\n",
    "We'll read in the familiar wine quality data, except this time we'll attempt to predict whether wine is either red or white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (THREAD) Instantiate and fit a gridsearch model for this SVC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel SVMs on the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of SVMs\n",
    "SVMs will have two main hyperparameters: `C` and `kernel`.\n",
    "\n",
    "#### Parameter: `C`\n",
    "`C` controls how much we regularize the boundary that is fit between classes.\n",
    "- **If `C` is small**: We regularize substantially, leading to a less perfect classification of our training data.\n",
    "- **If `C` is large**: We do not regularize much, leading to a more perfect classification of our training data.\n",
    "\n",
    "(THREAD) As we increase `C`, what happens to our bias-variance tradeoff?\n",
    "\n",
    "#### Parameter: `kernel`\n",
    "There are several options:\n",
    "* `linear`, `rbf`, `polynomial`, `sigmoid`, or something custom\n",
    "\n",
    "**Which do we use?**\n",
    "- **Quick answer**: As long as it's not the linear kernel, it actually matters surprisingly little!\n",
    "- **Lazy answer**: Use the RBF kernel, as it's usually best (or no different from other options).\n",
    "- **Better answer**: It's a model parameter that you're free to gridsearch over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding with [MNIST Digits Dataset](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits.\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Check out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations do we have?\n",
    "# NOTE: Data is in a NumPy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the actual class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate support vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit support vector machine to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance based on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is accuracy the best metric to use here? Why or why not?</summary>\n",
    "\n",
    "- Accuracy is likely the best metric to use here. Improperly classifying a number is equally bad, no matter what number you incorrectly predict. For example, misclassifying a `4` as a `3` or `5` or `9` is equally bad.\n",
    "- Many of our other classification metrics (like sensitivity and specificity) don't easily generalize to classification with more than two classes.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spend three minutes trying different hyperparameter values. (Feel free to guess and check or use GridSearch/RandomizedSearch!) We'll report our best values in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SVM.\n",
    "\n",
    "# Fit on training data.\n",
    "\n",
    "# Evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with column for predicted values.\n",
    "\n",
    "# Create column for observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all indices where predicted and true results \n",
    "# aren't the same, then save in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that shows image, then \n",
    "# prints predicted and true values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for loop to visualize all mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines aren't even just for classification!\n",
    "\n",
    "SVMs can be used for regression problems as well! The main ideas are the same - we still specify a cost tolerance `C` and a kernel - but [it's a bit more complicated](https://www.saedsayad.com/support_vector_machine_reg.htm). (For example, visualizing a \"margin\" is easier to do when we're separating two classes than when we're trying to predict some continuous outcome.) For this reason, we won't get into the mathematical details of support vector machines applied to regression, but we can instantiate a model using `svr = sklearn.svm.SVR()` and `.fit()`, `.predict()` like we do with all of our other models! Check out the documentation for [regression SVMs here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of SVMs\n",
    "\n",
    "#### Pros\n",
    "- Exceptional perfomance (historically widely used)\n",
    "- Effective in high-dimensional data\n",
    "- Can work with non-linear boundaries\n",
    "- Fast to compute with most datasets (kernel trick)\n",
    "- Low risk of overfitting\n",
    "\n",
    "#### Cons\n",
    "- Black box method\n",
    "- Can be slow on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks and Fun Facts\n",
    "* SVMs are fantastic models if all you care about is predictive ability.\n",
    "* They are complete and total black boxes, sorry.\n",
    "* You must scale your data.\n",
    "* By the way, doing a kernel SVM with polynomial kernel degree = 2 has been shown to work really well for NLP data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

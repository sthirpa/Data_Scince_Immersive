{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Clustering and K-Means\n",
    "\n",
    "_Authors: Tim Book (DC), Dave Yerrington (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/kYWumd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Understand basic unsupervised clustering problems\n",
    "- Perform a K-Means Clustering Analysis\n",
    "- Evaluate clusters for fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson Guide\n",
    "- [Unsupervised learning](#unsupervised)\n",
    "- [Introduction to clustering](#intro)\n",
    "- [Uses for Clustering](#uses)\n",
    "- [What is clustering?](#what)\n",
    "- [KNN review](#knn)\n",
    "- [Clustering algorithms](#algos)\n",
    "- [K-means clustering](#k-means)\n",
    "- [K-Means step-by-step](#km-steps)\n",
    "- [K-Means: a visual example](#vis)\n",
    "- [K-Means caveats and pitfalls](#caveats)\n",
    "- [Centroid Initialization](#init-centroid)\n",
    "- [Choosing K](#choose-k)\n",
    "- [A note on K-Means convergence](#converge)\n",
    "- [K-Means in sklearn](#sklearn)\n",
    "- [OPTIONAL: Understanding the objective function](#obj)\n",
    "- [Metrics: inertia and the silhouette coefficient](#sil)\n",
    "- [Practice: use K-Means on the \"Isotopic Composition Plutonium Batches\" data](#pluto)\n",
    "    - [How does scaling affect fit?](#scaling)\n",
    "- [Conclusion: K-Means tradeoffs](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='unsupervised'></a>\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "---\n",
    "\n",
    "Up until now, we've been doing **supervised learning** - that is, modeling of the form:\n",
    "\n",
    "> Given X, predict Y\n",
    "\n",
    "(THREAD) Supervised learning comes in two flavors: can you remind me what they are? And how are they different?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Definitions:</summary>\n",
    "\n",
    "* Regression models predict a **continuous response** (numerical).\n",
    "* Classification models predict a **discrete response** (categorical).\n",
    "</details>\n",
    "\n",
    "When we don't have a Y variable to predict, we are in the realm of **unsupervised learning**. Since there is no Y variable, unsupervised learning has no measurable \"goal\". Instead, unsupervised learning seeks to **represent the data in new ways**. Today we're introducing **clustering**; however, there are many other types of unsupervised learning.\n",
    "\n",
    "> Data without a Y variable are sometimes referred to as **unlabeled** data. This is because the Y variable is often refered to as a **label**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Issues\n",
    "Since there is no Y variable to \"supervise\" our learning, unsupervised learning presents us with some new issues we've never had to work through:\n",
    "\n",
    "* **What is \"correct\"?** Since there's no Y variable, we don't have an easy to way know if we're even doing a good job.\n",
    "* **Tuning parameter selection.** Many unsupervised models have tuning parameters. How do we tune them if we don't know how to evaluate our model?\n",
    "* **Unpredictability (clustering).** In clustering, it is very difficult to predict what our model will give us. It's possible that a clustering algorithm won't give us actionable results. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "# Introduction to Clustering\n",
    "\n",
    "---\n",
    "\n",
    "**Clustering** is a task in which we seek to group our observations in **homogenous clusters**. Since it's unsupervised, it is up to us, the data scientists, to decide what we mean by \"homogenous\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"uses\"></a>\n",
    "## Uses for Clustering\n",
    "### Marketing\n",
    "Marketing teams do a lot of data-driven research into who does and does not buy their product (and why). As a marketing data scientist, you might collect demographic information about people in a survey and their spending habits. After clustering, you do some EDA and you might discover:\n",
    "\n",
    "> People in Cluster A aren't buying our product, but people in Cluster B are. Why?\n",
    "\n",
    "After some digging, you might make the conclusion and recommendation:\n",
    "\n",
    "> People in Cluster A have characteristic X, but people in Cluster B do not. In order to sell to Cluster A, we should target our marketing with respect to X.\n",
    "\n",
    "Maybe X = \n",
    "* They don't have cable\n",
    "* Their political beliefs\n",
    "* They live in cities\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/BdfATE.jpg\" style=\"width: 500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political Polling\n",
    "In the same vein as the above example, instead of buying a product, maybe it's **voting for a certain candidate**.\n",
    "\n",
    "The popular data-drive journalism website FiveThirtyEight did a lot of research into the then-upcoming 2020 Democratic primaries. They have done some (subjective!) clustering techniques to divide Democratic voters into five clusters:\n",
    "\n",
    "* Party Loyalists\n",
    "* The Left\n",
    "* Millennials and Friends\n",
    "* Black voters\n",
    "* Hispanic voters (sometimes in combination with Asian voters)\n",
    "\n",
    "They then graded each candidate based on their favorability of these clusters:\n",
    "\n",
    "![](./assets/imgs/five-corners.png)\n",
    "\n",
    "Their methodology is more speculation than science, but it still provides a good example into how real political polling works. You can read more about it [here](https://fivethirtyeight.com/features/the-5-key-constituencies-of-the-2020-democratic-primary/).\n",
    "\n",
    "_NOTE:_ Unlike the kinds of clustering we'll see in our class, someone might fall into multiple categories here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "Online retailers cluster their items by similarity. If you buy (or search for) a few items in a given cluster, they may recommend you other similar items in that same cluster.\n",
    "\n",
    "![](./assets/imgs/recs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://snag.gy/YUt5RO.jpg\" style=\"float: left; margin-right: 25px; width: 250px\">\n",
    "\n",
    "## What other problems do you think can make use of clustering?\n",
    "\n",
    "(THREAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"algos\"></a>\n",
    "# Clustering algorithms \n",
    "\n",
    "---\n",
    "\n",
    "The are many different algorithms that can perform clustering given a dataset. \n",
    "\n",
    "These algorithms nearly always reduce to difficult optimization problems which may converge on a local minimum (similarly to gradient descent).\n",
    "\n",
    "- **K-Means** (mean centroids)\n",
    "- **DBSCAN** (density based)\n",
    "- **Hierarchical** (nested clusters by merging or splitting successively)\n",
    "- **Affinity Propagation** (graph based approach to let points 'vote' on their preferred 'exemplar')\n",
    "- **Mean Shift** (can find number of clusters)\n",
    "- **Spectral Clustering**\n",
    "- **Agglomerative Clustering** (suite of algorithms all based on applying the same criteria/characteristics of one cluster to others)\n",
    "\n",
    "\n",
    "Image from scikit-learn: [\"Comparing different clustering algorithms on toy datasets.\"](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)\n",
    "\n",
    "![](./assets/imgs/sklearn-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, note that $k$-Means clustering:\n",
    "- is really fast!\n",
    "- can only create convex clusters. This implies that its clusters can always be linearly separated.\n",
    "- may have to be run multiple times to get the best clusters.\n",
    "\n",
    "> - \"We argue that there are many clustering algorithms, because the notion of \"cluster\" cannot be precisely defined.\"<br>\n",
    "> - \"Therefore, comparing clustering algorithms, must take into account a careful understanding of the inductive principles involved.\"<br>\n",
    "> - \"The nature of clustering is exploratory, rather than confirmatory.\"\n",
    ">\n",
    "> From: Vladimir Estivill-Castro. [\"Why so many clustering algorithms -- A Position Paper\" (PDF)](http://web.cs.iastate.edu/~honavar/clustering-survey2.pdf)\n",
    "\n",
    "Today we're going to look only at one of the algorithms: **k-means**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='k-means'></a>\n",
    "# K-Means clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### K-Means is the most popular clustering algorithm\n",
    "\n",
    "K-means is one of the easier methods to understand and other clustering techniques use some of the same assumptions that k-means relies on.\n",
    "\n",
    "- **k** is the number of clusters.\n",
    "- **Means** refers to the mean points of the k clusters.\n",
    "\n",
    "The goal is to partition the data into sets of points, such that the total sum of squared distances from each point to the mean point of the cluster is minimized.\n",
    "\n",
    "Similarly to k-nearest neighbors, the resulting centroids partition the space into [Voronoi cells](https://en.wikipedia.org/wiki/Voronoi_diagram). For example:\n",
    "![](./assets/imgs/sklearn-kmeans-voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from scikit-learn: [\"A demo of K-Means clustering on the handwritten digits data\"](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html).\n",
    "\n",
    "**You must choose $k$, the number of clusters, in advance.** Note that this is a huge issue, since we rarely have an intuition for this number. And since we're in unsupervised territory, it's hard for us to know if we've picked correctly! We'll talk about how to pick $k$ later.\n",
    "\n",
    "The algorithm takes your entire dataset and iterates over its features and observations to determine clusters based around center points. These center points are known as **centroids**. \n",
    "\n",
    "**What does K-means do?**\n",
    "\n",
    "> $K$-means partitions the data into sets of points (clusters). These clusters minimize the within-cluster sum-of-squares.\n",
    "\n",
    "We will examine this in more detail later!\n",
    "\n",
    "**K-means iterative fitting:**\n",
    "1. Pick a value for $k$ (the number of clusters to create).\n",
    "2. Initialize $k$ 'centroids' (starting points). These do not have to be actual data points!\n",
    "3. Create clusters by assigning each data point to its nearest centroid.\n",
    "4. Make your clusters better. Reassign each centroid to the center of its cluster.\n",
    "5. Repeat steps 3-4 until the centroids converge and do not change across iterations.\n",
    "\n",
    "$K$-means is guaranteed to converge.\n",
    "\n",
    "> Check out a demo of this algorithm [here](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='caveats'></a>\n",
    "## Sounds great - what could go wrong?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scenario 1: Wispy Flat Blobs\n",
    "![](./assets/imgs/stripes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2:  The Lone Wolf Point\n",
    "![](./assets/imgs/lone-wolf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: When it doesn't make sense to cluster at all!\n",
    "In general, k-means will converge to a solution and return a partition of k clusters, even if no natural clusters exist in the data.  It's entirely possible – in fact, *common* – that the clusters do not mean anything at all. \n",
    "\n",
    "![](./assets/imgs/blob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many K?\n",
    "\n",
    "Sometimes it's obvious, sometimes it's not!  What do you think?\n",
    "\n",
    "- /poll \"How many K:  Figure 1\" \"1\" \"2\" \"3\" \"4\" \":think:\"\n",
    "- /poll \"How many K:  Figure 2\" \"1\" \"2\" \"3\" \"4\" \":think:\"\n",
    "- /poll \"How many K:  Figure 3\" \"1\" \"2\" \"3\" \"4\" \":think:\"\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/4rU39.png\"><br>1</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/gq28F.png\"><br>2</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"https://snag.gy/cWPgno.jpg\"><br>3</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"init-centroid\"></a>\n",
    "## Initializing Centroids\n",
    "\n",
    "---\n",
    "\n",
    "There are different methods of initializing centroids. For instance:\n",
    "\n",
    "- Randomly\n",
    "- Manually\n",
    "- Special KMeans++ method in Sklearn (_This initializes the centroids to be generally distant from each other._)\n",
    "\n",
    "**Depending on your problem, you may find some of these are better than others.**\n",
    "\n",
    "> **Note:** Manual is recommended if you know your data well enough to see the clusters without much help, but rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='choose-k'></a>\n",
    "## Choosing $k$\n",
    "This still remains an open question. After all, we're tuning a tuning parameter with no metric for success! Here are three ideas:\n",
    "\n",
    "* Make an educated guess\n",
    "    - Industry knowledge (there are five kinds of Democrats...)\n",
    "    - Visualization (probably impossible if you have more than 2 variables)\n",
    "* Judge based on a pseudo-evaluation metric, like the **silhouette score**.\n",
    "* If you're using the resulting cluster labels as input to a supervised learning method later, you can tune $k$ to have the best supervised learning model. This is **transfer learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='converge'></a>\n",
    "## A note on K-Means convergence\n",
    "\n",
    "---\n",
    "\n",
    "**Knowing your domain and dataset is essential. Evaluating the clusters visually is a must (if possible).**\n",
    "\n",
    "> _\"Given enough time, k-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\"_ [sklearn Clustering Guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"sklearn\"></a>\n",
    "## K-Means in sklearn\n",
    "\n",
    "---\n",
    "\n",
    "Below we will implement K-Means using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Let's make some more blobs to test K-Means on\n",
    "data, color = make_blobs(n_samples=100, random_state=29, centers=3, cluster_std=1.5)\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"x1\", \"x2\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the scatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare our X matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# (THREAD) How many K here?\n",
    "# I do: Fit a k-means clustering model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After we fit our data, we can get our predicted labels from `model.labels_` and the center points`model.cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Check out centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Class attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# attach predicted cluster to original points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do: Show me the mean values for x1 and x2 WITHIN EACH CLUSTER for our data. (thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visually verifying cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe for cluster_centers (centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Plot scatter by cluster / color, and centroids\n",
    "color_map = {0:\"red\", 1:\"green\", 2:\"blue\"}\n",
    "df['color'] = df['cluster'].map(color_map)\n",
    "\n",
    "ax = df.plot(    \n",
    "    kind=\"scatter\", \n",
    "    x=\"x1\",\n",
    "    y=\"x2\",\n",
    "    figsize=(10,8),\n",
    "    c = 'color'\n",
    ")\n",
    "\n",
    "centroids.plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"x1\",\n",
    "    y=\"x2\", \n",
    "    marker=\"*\",\n",
    "    c=centroids.index.map(color_map),\n",
    "    s=550,\n",
    "    ax=ax\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='obj'></a>\n",
    "## OPTIONAL: Understanding the objective function\n",
    "\n",
    "---\n",
    "\n",
    "In data science, you will typically see an **objective function** (the function to minimize or maximize) expressed mathematically. So, it is useful to get practice reading them.\n",
    "\n",
    "#### Sum of Squared Errors\n",
    "\n",
    "- In linear regression, logistic regression, and neural networks we often look for a prediction function $\\hat{y}$ that minimizes the sum of squared errors (SSE) on a training dataset. \n",
    "- We start with:\n",
    "    - $N$ data points. \n",
    "    - The $i$th data point consists of a vector of features $\\mathbf{x}_i$ and a target $y_i$.\n",
    "    - Before optimizing, we specify the exact form of $\\hat{y}$. For example, we could define $\\hat{y}(\\mathbf{x}) := \\beta_0 + \\beta_1x_1 + \\beta_2x_2$.\n",
    "\n",
    "- Given that, here is the objective function for SSE:\n",
    "$$\\underset{\\hat{y}}{\\text{argmin}} \\sum^N_{i=1} (y_i - \\hat{y}(\\mathbf{x}_i))^2$$\n",
    "\n",
    "- $\\underset{\\hat{y}}{\\text{argmin}}[SSE]$: We seek a function $\\hat{y}$ that minimizes $SSE$.\n",
    "    - Given the example definition of $\\hat{y}$ above, this gives the parameters $\\beta_0$, $\\beta_1$, and $\\beta_2$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### $K$-Means Clustering\n",
    "\n",
    "- In $K$-means clustering, we look for a $K$-tuple of clusters that minimize the sum of squared distances (SSD) from each point to its cluster mean. We start with:\n",
    "    - We are solving for the $K$ cluster means (vectors) given by $\\mathbf{\\mu} = (\\mathbf{\\mu_1}, \\mathbf{\\mu_2}, \\cdots, \\mathbf{\\mu_K})$.\n",
    "    - Derived from $\\mathbf{\\mu}$, the training dataset is partitioned into a set of clusters $\\mathbb{C}$. The $i$th cluster is $\\mathbb{C}_i$.\n",
    "    - $||\\mathbf{x} - \\mathbf{\\mu}_i ||$ is the squared distance between the vectors $\\mathbf{x}$ and $\\mathbf{\\mathbf{\\mu}_i}$. \n",
    "\n",
    "- Given that, here is the objective function for $K$-means clustering:\n",
    "$$\\underset{\\mathbf{\\mu}}{\\text{argmin}} \\sum^k_{i=1}\\sum_{\\mathbf{x} \\in \\mathbb{C_i}} ||\\mathbf{x} - \\mathbf{\\mu}_i||$$\n",
    "\n",
    "+ $\\underset{\\mathbf{\\mu}}{\\text{argmin}}[SSD]$: We seek the centroids $\\mathbf{\\mu}$ that minimize $SSD$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='sil'></a>\n",
    "\n",
    "## Metrics: inertia and the silhouette score\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will look at the Silhouette Score -- one method of evaluating our results. Clustering metrics are most useful for (1) comparing to other clustering algorithms, and (2) warning us about bad clustering. As discussed above, since there is no definition for clustering, a metric cannot reliably tell us about whether the clustering is good. \n",
    "\n",
    "**A high score does not necessarily indicate good clustering.** Instead, it measures at best the difference between the metric and what our clustering algorithm optimized for.\n",
    "\n",
    "\n",
    "**Inertia** -- sum of squared errors for each cluster. A low inertia = dense cluster.\n",
    "\n",
    "> As the number of clusters $k$ increases, inertia decreases. \n",
    "\n",
    "**Silhouette Score** \n",
    "\n",
    "- Each point has a silhouette score.\n",
    "- The score is high if is it:\n",
    "    - similar to other points in its cluster (low **cohesion**) -AND-\n",
    "    - dissimilar from points in neighboring clusters (high **separation**).\n",
    "\n",
    "You know clustering is poor if many points have low scores. For example, notice that `dolphin` and `porpoise`, the last two points, could have been clustered better:\n",
    "\n",
    "![](./assets/imgs/silhouette-wikipedIa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from: Wikipedia, [\"Silhouette (clustering)\"](https://en.wikipedia.org/wiki/Silhouette_(clustering)).\n",
    "\n",
    "The average Silhouette Score is the average of each point's score.\n",
    "\n",
    "- **cohesion** = Average distance of points within clusters\n",
    "- **separation** = Average distance of points in one cluster to points in other clusters\n",
    "\n",
    "And it's calculated as:\n",
    "\n",
    "$$s_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a_i$ = Average distance from point $x_i$ to all other points **in the same cluster**.\n",
    "* $b_i$ = Smallest mean distance from point $x_i$ to all points **in any other cluster**.\n",
    "\n",
    "And the resulting silhouette score is the average of all the scores:\n",
    "\n",
    "* -1 = Bad\n",
    "* 0 = Meh\n",
    "* 1 = Good\n",
    "\n",
    "The silhouette score evaluates the viability of your clusters. It is a way to quantify the \"cohesion\" of local points to their clusters and the separation to other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy to calculate Silhouette in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programatically Inspecting $k$\n",
    "\n",
    "Since we're familliar with gridsearch now, let's try it out!  Gridsearch is typically used for supervised learning with some scoring objective for optimizing parameters in terms of `best_model`.  We don't really care about the best model as much as we want to see how either inertia or silhouette score changes based on $k$.\n",
    "\n",
    "We will write a basic process for exploring inertia and silhouette, that you can feel free to adapt to any other ML process you might write in Sklearn for doing custom scoring (using GridsearchCV is better for supervised learning because it has cross validation built-in!).\n",
    "\n",
    "> Visually inspecting your clusters is vastly superior to relying on programatic parameter searching for an ideal cluster for most data.  You can also invent your own evalutation metrics other than inertia or silhoette.  The world is also bigger than k-means when it comes to cluster applications.  It's a great first stop in your journey of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it called \"silhouette\" score?\n",
    "![](./assets/imgs/silhouette.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use yellowbrick to create a silhouette plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion: K-Means tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "**K-Means:**\n",
    "- Unsupervised clustering model\n",
    "- Iteratively finds labels given k\n",
    "- Easy to implement in sklearn\n",
    "- Sensitive to shape, scale of data **(!!!)**\n",
    "- Optimal k hard to evaluate\n",
    "\n",
    "---\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "| -- | -- |\n",
    "| k-Means is popular because it's simple and computationally efficient. | However, k-Means is highly scale dependent and isn't suitable for data of varying shapes and densities. |\n",
    "| Easy to see results / intuitive. | Evaluating results is more subjective, requiring much more human evaluation than trusted metrics. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-Means is sensitive to outliers\n",
    "\n",
    "![](https://snag.gy/WFNMQY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-Means is sensitive to centroid initialization\n",
    "\n",
    "![](https://snag.gy/5sigCD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS Exercise: Old Faithful\n",
    "The classic Old Faithful dataset describes the durations of eruptions and the amount of waiting time since the last eruption for the Old Faithful geyser. Plot this data. Do you think this data could benefit from clustering? How many clusters are there? Repeat the steps of this lab for this dataset on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geyser = pd.read_csv(\"./assets/datasets/geyser.csv\")\n",
    "geyser.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = geyser.plot(kind='scatter', x='waiting', y='duration', c=\"black\", s=5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

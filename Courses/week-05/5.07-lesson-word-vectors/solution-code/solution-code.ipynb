{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Word Vectors\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Describe word vectors and understand the shortcomings of bag-of-words methods.\n",
    "- Describe word embeddings.\n",
    "- Apply Word2Vec, GloVe, and BERT embedding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will start by importing what we need for Word2Vec, GloVe, and the transformer models.** (Downloading the pre-trained Word2Vec embeddings can take a while! We are using the [gensim.downloader](https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html) module for this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/upgrade Gensim & transformers\n",
    "# !pip install gensim --upgrade\n",
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a vector?\n",
    "\n",
    "There are lots of ways to think about a vector.\n",
    "\n",
    "<img src=\"../images/vector.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "In **physics**, vectors are arrows.\n",
    "\n",
    "<img src=\"../images/vector.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "In **computer science** and **statistics**, vectors are columns of values, like one numeric Series in a DataFrame.\n",
    "\n",
    "#### It turns out that these are equivalent.\n",
    "\n",
    "<img src=\"../images/vector_on_graph.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "[This video](https://www.youtube.com/watch?v=fNk_zzaMoSs) does an exceptional job explaining vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what is a word vector?\n",
    "\n",
    "A word vector, simply, is a way for us to represent words with vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How have we technically already done this?</summary>\n",
    "    \n",
    "- CountVectorizer and TFIDFVectorizer. By representing each word as a new column in our DataFrame, we have represented words with vectors.\n",
    "\n",
    "![](../images/countvectorizer.jpeg)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise, we can think of each word as its own dimension or axis. In the example below, we have represented the horizontal axis with a vector for `cat` and the vertical axis with a vecvtor for `hat`.\n",
    "\n",
    "<img src=\"../images/cat_hat.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "This is exactly what CountVectorization and TFIDFVectorization have done; we are now just representing it geometrically/visually! Each column in our DataFrame corresponds to a new axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of vectorization of words (turning each word into its own column) is known as \"1-of-N encoding.\"\n",
    "\n",
    "<img src=\"../images/one-hot-new.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "For example:\n",
    "- the vector for the word `dog` would be [1, 0, 0, 0, 0].\n",
    "- the vector for the word `cat` would be [0, 1, 0, 0, 0].\n",
    "- the vector for the word `puppy` would be [0, 0, 1, 0, 0].\n",
    "- the vector for the word `kitten` would be [0, 0, 0, 1, 0].\n",
    "- the vector for the word `pug` would be [0, 0, 0, 0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above vectors are independent of one another. Thinking purely about language and the way we use it, **should** dog and puppy be independent of one another? **Should** dog and pug be independent of one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you think?</summary>\n",
    "    \n",
    "- Probably not!\n",
    "- Dog and puppy have similar meanings. (Really, only the age is different.)\n",
    "- Dog and cat have similar meanings. (i.e. I know that \"dog\" and \"cat\" are more similar than \"dog\" and \"book\" or \"cat\" and \"car.\")\n",
    "- Our current data science strategy for NLP (CountVectorization, TFIDFVectorization) is good in that it allows us to get computers to understand natural language in a way similar to how humans do... but our current strategy has its limitations!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Rather than creating a whole new dimension each time we encounter a new word and treating it as independent of all other words, can we instead come up with \"new axes\" that allow us to better understand meanings and relationships among words?\n",
    "- YES.\n",
    "\n",
    "**Word embedding** is a term used to describe representing words in mathematical space.\n",
    "- One word embedding technique is CountVectorization.\n",
    "- A more advanced word embedding technique is `Word2Vec`.\n",
    "\n",
    "## Non-contextual Word Embeddings\n",
    "\n",
    "### Word2Vec\n",
    "- Word2Vec is an approach that takes in observations (sentences, tweets, books) and maps them into some other space using a neural network.\n",
    "\n",
    "Going back to our previous example, try to \"think\" of a five-dimensional space. \n",
    "- The horizontal axis corresponds to `dog`.\n",
    "- The vertical axis corresponds to `cat`.\n",
    "- The axis extending out toward you corresponds to `puppy`.\n",
    "- Given that we live in 3D space, we can't really visualize higher dimensions.\n",
    "\n",
    "Instead of giving each word its own axis, the `Word2Vec` algorithm will take all of our words and map them to another set of axes that accounts for these relationships.\n",
    "\n",
    "<img src=\"../images/word-vectors-new.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care?\n",
    "The structure of language has a lot of valuable information in it! The way we organize our text/speech tells us a lot about what things mean.\n",
    "\n",
    "By using machine learning to \"learn\" about the structure and content of language, our models can now organize concepts and learn the relationships among them.\n",
    "- Above, we did not explicitly tell the computer what \"dog\" or \"puppy\" or \"cat\" or \"kitten\" actually mean. But by learning from the data, our model can quantify the relationship among these entities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Word2Vec work?\n",
    "\n",
    "#### Basic Answer:\n",
    "The idea is that we can use the position of words in sentences (i.e. see which words were commonly used together) to understand their relationships.\n",
    "- If \"dog\" and \"puppy\" are used near one another a lot, then it suggests that there may be some sort of relationship between them.\n",
    "- If \"cat\" and \"dog\" are used near similar words a lot (i.e. \"pet\"), then it suggests that there may be some sort of relationship between them.\n",
    "\n",
    "#### More Advanced Answer:\n",
    "There are two algorithms that use neural networks to learn these relationships: Continuous Bag-of-Words (CBOW) and Continuous Skip-grams.\n",
    "\n",
    "![](../images/cbow.png)\n",
    "\n",
    "**CBOW (BONUS)**\n",
    "\n",
    "A continuous Bag-of-Words model is a two-layer neural network that:\n",
    "- takes the surrounding \"context words\" as an input.\n",
    "- generates the \"focus word\" as the output.\n",
    "\n",
    "<img src=\"../images/word2vec-cbow.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "**Skip-Gram (BONUS)**\n",
    "\n",
    "A Continuous Skip-gram model is a two-layer neural network that:\n",
    "- takes the \"focus word\" as an input.\n",
    "- generates the surrounding \"context words\" as the output.\n",
    "\n",
    "<img src=\"../images/skipgram.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "([image source](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### What is the vector for \"cat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08927273,  0.50178224,  0.5737835 , -0.68450093, -0.11695965,\n",
       "       -0.9776795 , -1.3833832 ,  0.29959467, -0.67036283,  0.15542029,\n",
       "       -1.2341528 , -0.03289595, -0.09040691,  2.1864526 , -1.9866585 ,\n",
       "       -0.38490614,  1.0652466 , -0.9454393 ,  0.9680908 ,  0.04675735,\n",
       "       -1.3229346 ,  1.1588073 ,  0.18646714, -1.594005  , -0.255257  ,\n",
       "        0.11279747, -0.28223416, -0.75207424, -0.03520761, -0.73562336,\n",
       "        1.6915892 , -1.3436104 ,  0.53710234, -0.10980084, -0.37153494,\n",
       "       -0.63238984, -0.38514853, -0.01789735, -1.8029379 , -2.5579617 ,\n",
       "       -0.6234836 , -0.39335245, -0.9148902 ,  1.250455  ,  0.9027701 ,\n",
       "       -0.6562443 ,  0.2729199 , -0.23239459,  1.2081391 ,  1.0095335 ,\n",
       "        0.359972  ,  0.6906699 , -0.24408974,  0.48777083, -1.4030666 ,\n",
       "       -0.14940621, -0.7826522 , -0.41791376, -1.6827799 , -0.04798492,\n",
       "       -0.1674054 ,  0.45640594, -2.5486326 ,  1.6861234 , -0.74135435,\n",
       "       -1.370901  , -0.48914954, -1.1248355 , -1.0403992 ,  0.37833458,\n",
       "        0.6988081 ,  0.60681325,  0.02360613,  0.87780225,  1.2142192 ,\n",
       "        0.01916341,  0.8487177 , -0.5950504 ,  0.02062986,  0.98779976,\n",
       "       -1.859876  , -0.43092093, -0.9156215 ,  0.57596344, -2.8036633 ,\n",
       "       -0.99457306, -0.50458115,  1.0800254 ,  1.3567951 ,  0.36793348,\n",
       "        1.2090119 , -1.0863186 , -0.14769949,  0.95819503,  0.25097382,\n",
       "        0.93088377, -0.46046406,  0.81102926, -0.4662643 ,  1.006933  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 1: Which of these is not like the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elephant'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['dog', 'fish', 'cat', 'hamster', 'elephant']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salad'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['taco', 'salad', 'burrito', 'quesadilla'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vienna'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['london', 'chicago', 'madrid', 'vienna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doctor'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['king', 'princess', 'doctor', 'duke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['physics', 'math', 'english', 'statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own and share the most mind-blowing one in a thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application of this**: Suppose you're attempting to automatically detect spam emails or detect plagiarism based on words that don't belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 2: What is most alike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('venice', 0.7711915373802185),\n",
       " ('leipzig', 0.734907865524292),\n",
       " ('vienna', 0.7236834764480591),\n",
       " ('munich', 0.7227122187614441),\n",
       " ('louvre', 0.6973660588264465),\n",
       " ('villa', 0.6914119124412537),\n",
       " ('milan', 0.6844666004180908),\n",
       " ('bologna', 0.6833786368370056),\n",
       " ('naples', 0.6812750697135925),\n",
       " ('rouen', 0.669109046459198)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mechanics', 0.7980954647064209),\n",
       " ('chemistry', 0.7608328461647034),\n",
       " ('quantum', 0.7518066763877869),\n",
       " ('optics', 0.7433465719223022),\n",
       " ('mathematics', 0.7342231869697571),\n",
       " ('astronomy', 0.7289931178092957),\n",
       " ('newtonian', 0.7266224026679993),\n",
       " ('electromagnetism', 0.7215863466262817),\n",
       " ('theoretical', 0.7152252793312073),\n",
       " ('cosmology', 0.7069436311721802)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('physics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('detention', 0.6877532005310059),\n",
       " ('safety', 0.6576682329177856),\n",
       " ('surveillance', 0.6453025937080383),\n",
       " ('unlawful', 0.6447994112968445),\n",
       " ('monitoring', 0.637326180934906),\n",
       " ('civilian', 0.6153445839881897),\n",
       " ('contraception', 0.6146061420440674),\n",
       " ('evacuation', 0.6105533838272095),\n",
       " ('shelter', 0.6097783446311951),\n",
       " ('admission', 0.6036332249641418)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('emergency') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application of this**: Suppose you're building out a process to detect when people are tweeting about an emergency. They may not just use the word \"emergency.\" Rather than manually creating a list of words people could use, you may want to learn from a much larger corpus of data than just your personal experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Create Word2Vec word vectors from your own corpus! (BONUS)\n",
    "\n",
    "### NOTE: This will usually take a *long* time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Word2Vec\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# # If you want to use gensim's data, import their downloader\n",
    "# # and load it.\n",
    "# import gensim.downloader as api\n",
    "# corpus = api.load('text8')\n",
    "\n",
    "# # If you have your own iterable corpus of cleaned data, you can \n",
    "# # read it in as corpus and pass that in.\n",
    "\n",
    "# # Train a model! \n",
    "# model = Word2Vec(corpus,      # Corpus of data.\n",
    "#                  size=100,    # How many dimensions do you want in your word vector?\n",
    "#                  window=5,    # How many \"context words\" do you want?\n",
    "#                  min_count=1, # Ignores words below this threshold.\n",
    "#                  sg=0,        # SG = 1 uses SkipGram, SG = 0 uses CBOW (default).\n",
    "#                  workers=4)   # Number of \"worker threads\" to use (parallelizes process).\n",
    "\n",
    "# # Do what you'd like to do with your data!\n",
    "# model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the documentation for Gensim's implementation of [Word2Vec here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GloVe\n",
    "\n",
    "GloVe stands for Global Vectors for Word Representation. It is an unsupervised technique that maps words to vector representations where the distance between the vectors represents semantic similarities. This is done using a co-occurrence matrix which shows us how often pairs of words occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 400000,\n",
       " 'file_size': 69182535,\n",
       " 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
       " 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       " 'parameters': {'dimension': 50},\n",
       " 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       " 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       " 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "  'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       " 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       " 'file_name': 'glove-wiki-gigaword-50.gz',\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 1: Which of these is not like the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiger'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.doesnt_match(['dog', 'fish', 'cat', 'hampster', 'tiger']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 2: What is most alike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plastic', 0.79425048828125),\n",
       " ('metal', 0.7708716988563538),\n",
       " ('walls', 0.7700635194778442),\n",
       " ('marble', 0.7638523578643799),\n",
       " ('wood', 0.7624280452728271),\n",
       " ('ceramic', 0.7602593302726746),\n",
       " ('pieces', 0.7589112520217896),\n",
       " ('stained', 0.7528817653656006),\n",
       " ('tile', 0.748193621635437),\n",
       " ('furniture', 0.7463858723640442)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.most_similar(\"glass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Contextualized/Dynamic Word Embeddings\n",
    "\n",
    "What are some shortcomings of `Word2Vec`? It takes into consideration the meaning of words based on context in the corpus, but what about words with different meanings?\n",
    "\n",
    "How many meanings can you think of for the word \"set\"? This word [holds the record](https://www.guinnessworldrecords.com/world-records/english-word-with-the-most-meanings/) for the most number of meanings in the English language. Even a word like \"apple\" can take on vastly different meanings in today's age. `Word2Vec` assigns one vector for each word.\n",
    "\n",
    "**Dynamic Word Embeddings** overcome this shortcoming by assigning an embedding to each word after looking at the sentence of the words. This means that the same words (e.g. \"apple\" in a sentence about fruit and \"Apple\" in a sentence about computers) can be represented by different vectors based on their contexts. One of the first popular models that did this was called **ELMo**. Another popular one is named **BERT**.\n",
    "\n",
    "<img src=\"../images/bert.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "[BERT](https://github.com/google-research/bert) (Bidirectional Encoder Representations from Transformers) was created by Google in late 2018 and continues to outperform other language representation models. It combined ELMo and several other transformers and is fully bidirectional allowing words to have different vectors based on the context of the word.\n",
    "\n",
    "BERT is an example of a Transformer model. The following is from [Wikipedia](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)):\n",
    "\n",
    "> Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in order. For example, if the input data is a natural language sentence, the Transformer does not need to process the beginning of it before the end. Due to this feature, the Transformer allows for much more parallelization than RNNs and therefore reduced training times.  \n",
    "Since their introduction, Transformers have become the model of choice for tackling many problems in NLP, replacing older recurrent neural network models such as the long short-term memory (LSTM).\n",
    "\n",
    "We will use Hugging Face's [transformers](https://github.com/huggingface/transformers) for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 1: Fill in the blank\n",
    "We will use the BERT model here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'i want to go for a walk in the park',\n",
       "  'score': 0.9612388014793396,\n",
       "  'token': 3328,\n",
       "  'token_str': 'walk'},\n",
       " {'sequence': 'i want to go for a run in the park',\n",
       "  'score': 0.008957321755588055,\n",
       "  'token': 2448,\n",
       "  'token_str': 'run'},\n",
       " {'sequence': 'i want to go for a ride in the park',\n",
       "  'score': 0.006247078534215689,\n",
       "  'token': 4536,\n",
       "  'token_str': 'ride'},\n",
       " {'sequence': 'i want to go for a stroll in the park',\n",
       "  'score': 0.0060235848650336266,\n",
       "  'token': 27244,\n",
       "  'token_str': 'stroll'},\n",
       " {'sequence': 'i want to go for a swim in the park',\n",
       "  'score': 0.0045113274827599525,\n",
       "  'token': 9880,\n",
       "  'token_str': 'swim'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('I want to go for a [MASK] in the park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'no smoking allowed in class',\n",
       "  'score': 0.10337366908788681,\n",
       "  'token': 9422,\n",
       "  'token_str': 'smoking'},\n",
       " {'sequence': 'no weapons allowed in class',\n",
       "  'score': 0.045239537954330444,\n",
       "  'token': 4255,\n",
       "  'token_str': 'weapons'},\n",
       " {'sequence': 'no drugs allowed in class',\n",
       "  'score': 0.021998990327119827,\n",
       "  'token': 5850,\n",
       "  'token_str': 'drugs'},\n",
       " {'sequence': 'no alcohol allowed in class',\n",
       "  'score': 0.02136874757707119,\n",
       "  'token': 6544,\n",
       "  'token_str': 'alcohol'},\n",
       " {'sequence': 'no music allowed in class',\n",
       "  'score': 0.010899786837399006,\n",
       "  'token': 2189,\n",
       "  'token_str': 'music'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"No [MASK] allowed in class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 2: Sentiment Analysis\n",
    "This was trained on [sst2](https://www.tensorflow.org/datasets/catalog/glue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_56']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sent = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997475147247314}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent('This was the worst movie ever!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998810291290283}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent('I love this so much')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 3: Question Answering\n",
    "This was trained on [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_76']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Assembly is a pioneer in education and career transformation, specializing in today's most in-demand coding, business, data, and design skills. With 30+ campuses around the world, we provide award-winning, dynamic training to a global community of professionals pursuing careers they love. Named one of Fast Company’s most innovative education companies, GA offers full- and part-time courses for career climbers both on campus and online. Through our corporate training programs, we also help companies compete for the future by sourcing, assessing, and growing their talent. All of these offerings are developed and led by industry experts.\n"
     ]
    }
   ],
   "source": [
    "ga = 'General Assembly is a pioneer in education and career transformation, specializing in today\\'s most in-demand coding, business, data, and design skills. With 30+ campuses around the world, we provide award-winning, dynamic training to a global community of professionals pursuing careers they love. Named one of Fast Company’s most innovative education companies, GA offers full- and part-time courses for career climbers both on campus and online. Through our corporate training programs, we also help companies compete for the future by sourcing, assessing, and growing their talent. All of these offerings are developed and led by industry experts.'\n",
    "print(ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.506829559803009,\n",
       " 'start': 157,\n",
       " 'end': 186,\n",
       " 'answer': '30+ campuses around the world'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question(context = ga, question = 'Where is General Assembly located?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1903563290834427,\n",
       " 'start': 99,\n",
       " 'end': 150,\n",
       " 'answer': 'in-demand coding, business, data, and design skills'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question(context = ga, question = 'What can I learn at General Assembly?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 4: Summarization\n",
    "By default, this uses a [Bart](https://medium.com/analytics-vidhya/assesing-barts-syntactic-abilities-and-bert-s-part-1-cbf0983f6ea4) model that was trained on CNN/Daily Mail data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.upi.com/Odd_News/2020/10/22/Bear-opens-car-door-climbs-inside-in-Tennessee/9821603398162/\n",
    "news = \"\"\"\n",
    "An Indiana family visiting Tennessee captured video of a black bear wandering up to their unoccupied car, opening a door and climbing inside.\n",
    "The Franczak family said they traveled from Crown Point, Ind., to Sevierville, Tenn., to celebrate a grandmother's birthday. \"One of our bucket list things was to see a bear,\" father Brian Franczak told WBBM-TV.\n",
    "The family said they were shocked, however, when a bear came walking up the driveway of their vacation home and headed for their SUV.\n",
    "\"I just screamed, 'Oh my God! The bear is here! The bear is in the driveway,'\" mom Carly Franczak said.\n",
    "The family captured video as the bear opened a back door of the vehicle and climbed inside.\n",
    "\"I was at go-carts racing and my grandpa got a call about that there's a bear in their car,\" daughter Olivia Franczak said, \"and we couldn't believe it at first. We thought my uncle got dressed up as a bear and went into the car.\"\n",
    "The Tennessee Wildlife Resources Agency recommends residents and visitors keep vehicle doors locked at all times and make sure food and trash are secured where the animals can't reach.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'an Indiana family captured video of a black bear wandering up to their unoccupied car . the family said they traveled from Crown Point, ind., to sevierville, Tenn. a bear came walking up the driveway of their vacation home .'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neat application 5: Text Generation\n",
    "Using [GPT-2](https://en.wikipedia.org/wiki/OpenAI#GPT-2)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I would like to thank everyone who donated to this website. Please support me on Patreon so that I can continue getting more books out - so that we can continue to write about how wonderful and wonderful I am. Please click on the \"Help!\" button'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"I would like to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Applying this to your data\n",
    "\n",
    "Want to use a pre-trained model on your own text data? Due to hardware and time limitations, we will not do this in class, but below are several tutorials that can walk you through this. Warning: these models take a lot of time/memory - you may need a GPU for this! ([Google Colab offers free use of a GPU!](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm))\n",
    "\n",
    "- [Example of BERT in Keras](https://colab.research.google.com/drive/1934Mm2cwSSfT5bvi78-AExAl-hSfxCbq#scrollTo=gsscu_BluPLE)\n",
    "- [BERT tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)\n",
    "- [Predicting movie review sentiment with BERT](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=dCpvgG0vwXAZ)\n",
    "- [Classification with BERT in PyTorch](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO)\n",
    "- [Classification with GloVe embeddings](https://medium.com/analytics-vidhya/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81)\n",
    "- [Using pre-trained word embeddings in Keras](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "- Series of three articles on using word embeddings (e.g. Word2Vec and FastText) to transform text data for classification models (e.g. MultinomialNB). This particular series uses some more advanced methodologies as well, but the overall process described still applies.\n",
    "    - [Part 1](https://towardsdatascience.com/word-embeddings-and-document-vectors-part-1-similarity-1cd82737cf58)\n",
    "    - [Part 2](https://towardsdatascience.com/word-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c)\n",
    "    - [Part 3](https://towardsdatascience.com/word-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

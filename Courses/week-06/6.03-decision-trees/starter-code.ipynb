{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build Decision Trees\n",
    "Authors: Patrick Wales-Dinan; Matt Brems\n",
    "\n",
    "<img src=\"./images/sms1.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand what a decision tree is.\n",
    "- Calculate Gini Impurity.\n",
    "- Describe how decision trees use Gini Impurity to make decisions.\n",
    "- Fit, generate predictions from, and evaluate decision tree models.\n",
    "- Interpret and tune `max_depth`, `min_samples_split`, `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should we do today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Activity': ['Movies', 'Netflix', 'Beach', 'Netflix', 'Netflix', 'Beach'],\n",
    "    'Day': ['Weekend', 'Weekday', 'Weekend', 'Weekday', 'Weekday', 'Weekend'],\n",
    "    'Weather': ['Rainy', 'Sunny', 'Sunny', 'Rainy', 'Rainy', 'Sunny']\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a weekday. Based on our past behavior, what do you think we'll do today?</summary>\n",
    "\n",
    "- Watch Netflix.\n",
    "- In 100% of past cases where it's a weekday we've watched Netflix!\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Netflix  |      Weekday     |   Sunny  |\n",
    "|   Netflix  |      Weekday     |   Rainy  |\n",
    "|   Netflix  |      Weekday     |   Rainy  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a weekend. Based on our past behavior, what do you think we'll do?</summary>\n",
    "\n",
    "- Either go to the movies or got to the beach... but we can't say with certainty whether we'd go to the beach or go to the movies.\n",
    "- Based on our past behavior, we go to the movies on 1/3 of weekend days and we go to the beach on 2/3 of weekend days. (You can think of `.predictproba()`)\n",
    "- If I **had** to make a guess here, I'd probably predict that we would go to the beach, but we may want to use additional information to be certain.\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Movies |      Weekend     |   Rainy  |\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's the weekend and the weather is sunny! Based on our past behavior, what do you think we'll do?</summary>\n",
    "\n",
    "- Go to the beach.\n",
    "- In 100% of past cases where the weather is sunny and where it's a weekend, we've gone to the beach.\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Overview\n",
    "\n",
    "A decision tree:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that partitions (splits) our data into smaller datasets such that\n",
    "- by the bottom of the tree, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "We frequently see decision trees represented by a graph.\n",
    "\n",
    "<img src=\"./images/decision_tree_1.png\" alt=\"what_to_do\" width=\"750\"/>\n",
    "\n",
    "- (This image was created using [Draw.io](https://www.draw.io/).)\n",
    "\n",
    "### Terminology\n",
    "Decision trees look like upside down trees. \n",
    "- What we see on top is known as the \"root node,\" through which all of our observations are passed.\n",
    "- At each internal split, our dataset is partitioned.\n",
    "- A \"parent\" node is split into two or more \"child\" nodes.\n",
    "- At each of the \"leaf nodes\" (colored blue), we contain a subset of records that are as pure as possible.\n",
    "    - In the example above, each leaf node is perfectly pure. Once we get to a leaf node, every observation in that leaf node has the exact same value of $Y$!\n",
    "    - There are ways to quantify the idea of \"purity\" here so that we can let our computer do most of the tree-building (model-fitting) process... we'll come back to this later.\n",
    "\n",
    "Decision trees are also called \"**Classification and Regression Trees**,\" sometimes abbreviated \"**CART**.\"\n",
    "- [DecisionTreeClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Questions\n",
    "\n",
    "If you aren't familiar with the game [20 Questions](https://en.wikipedia.org/wiki/Twenty_Questions), it's a game with two players (or teams). \n",
    "- Player 1 thinks of an item.\n",
    "- Player 2 then tries to guess what the item is by asking a series of 20 questions that have a yes or no answer.\n",
    "- If player 2 correctly guesses the item within those 20 questions, then player 2 wins!\n",
    "- If player 2 does not correctly guess the item within the 20 question limit, then player 1 wins!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision trees operate in a fashion that's pretty similar to 20 Questions.\n",
    "- Decisions are made in a sequential fashion. Once you know a piece of information, you use that piece of information when asking future questions.\n",
    "    - Example: If you know that the item you're trying to guess is a person, then you can use that information to ask better subsequent questions.\n",
    "- It's possible to get lucky by making very specific guesses early, but it's pretty unlikely that this is a winning strategy.\n",
    "    - Example: If you asked, \"Is it an airplane? Is it a boat? Is it a car?\" as your first three questions, it's not very likely that you'll win the game.\n",
    "\n",
    "When fitting a decision tree, we're effectively getting our computer to play a game of 20 Questions. We give the computer some data and it figures out the best $X$ variable to split on at the right time.\n",
    "- Above, our \"what should we do today?\" decision tree first asked what type of day it was a weekend or a weekday, **then** asked what the weather was like.\n",
    "- If we had asked \"what's the weather out?\" first, we'd have ended up with a slightly more complicated decision tree.\n",
    "\n",
    "Just like with all of our models, in order for the computer to learn which $X$ variable to split on and when, the computer needs a loss function to quantify how good a particular split is. This is where the idea of **purity** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity in Decision Trees\n",
    "\n",
    "When quantifying how \"pure\" a node is, we want to see what the distribution of $Y$ is in each node, then summarize this distribution with a number.\n",
    "\n",
    "<img src=\"./images/decision_tree_1.png\" alt=\"what_to_do\" width=\"750\"/>\n",
    "\n",
    "- For continuous $Y$ (i.e. using a decision tree to predict income), the default option is mean squared error.\n",
    "- This is the `criterion = 'mse'` argument in `DecisionTreeRegressor`.    \n",
    "\n",
    "- For discrete $Y$, the default option is the Gini impurity. *(Bonus: This is not quite the same thing as the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient).)*\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "\\text{Gini impurity (2 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "\\text{Gini impurity (3 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "\\text{Gini impurity (4 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 - P(\\text{class 4})^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our y variable from our \"what should we do\" dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gini function, called gini.\n",
    "\n",
    "    \n",
    "    # Create a list to store my probs\n",
    "\n",
    "    \n",
    "    # Iterate through each class\n",
    "    # The set gives unique values\n",
    "        \n",
    "        # Calc the prob. of each class\n",
    "        \n",
    "        \n",
    "        # Append that squarred prob to the list \n",
    "#         print(prob)\n",
    "        \n",
    "        \n",
    "#         print(prob_sum)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if your Gini function is correct on the \n",
    "# \"what should we do tonight?\" data. (Should get 0.6111.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Practice\n",
    "\n",
    "<details><summary>What is the Gini impurity of a node when every item is from the same class?</summary>\n",
    "    \n",
    "- Our Gini impurity is zero.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 \\\\\n",
    "&=& 1 - 1^2 \\\\\n",
    "&=& 1 - 1 \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Gini when every item is from the same class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Gini when we have two classes, each with two items?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with three items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Gini when we have two classes, each with three items?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have three classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.6667.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{9} - \\frac{1}{9} - \\frac{1}{9} \\\\\n",
    "&=& 1 - \\frac{1}{3} \\\\\n",
    "&=& \\frac{2}{3}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Gini when we have three classes, each with two items?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Summary of Gini Impurity Scores</summary>\n",
    "\n",
    "- In the binary case, Gini impurity ranges from 0 to 0.5.\n",
    "- If we have three classes, Gini impurity ranges from 0 to 0.66667.\n",
    "- If we have $k$ classes, Gini impurity ranges from 0 to $1-\\frac{1}{k}$.\n",
    "- In all cases, a Gini impurity of 0 means maximum purity - all of our observations are from the same class!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "# Generate x values (for percentage of obs. in class A).\n",
    "percent_in_class_A = np.linspace(0, 1, 200)\n",
    "percent_in_class_B = 1 - percent_in_class_A\n",
    "\n",
    "# Calculate Gini values.\n",
    "gini_values = 1 - np.square(percent_in_class_A) - np.square(percent_in_class_B)\n",
    "\n",
    "# Plot line.\n",
    "plt.plot(percent_in_class_A,\n",
    "         gini_values)\n",
    "\n",
    "# Establish title, axes, and labels.\n",
    "plt.title('Gini Score in Binary Classification', fontsize = 24)\n",
    "plt.xlabel('Percent of Observation in Class A', fontsize = 20)\n",
    "plt.ylabel('Gini Score', fontsize = 20, rotation = 0, ha = 'right')\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### So how does a decision tree use Gini to decide which variable to split on?\n",
    "\n",
    "- At any node, consider the subset of our dataframe that exists at that node.\n",
    "- Iterate through each variable that could potentially split the data.\n",
    "- Calculate the Gini impurity for every possible split.\n",
    "- Select the variable that causes the greatest decrease in Gini impurity from the parent node to the child node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details><summary>What is the decrease in Gini impurity if we split on $X_1$? (Weekend vs. Weekday)</summary>\n",
    "\n",
    "- Answer: 0.389\n",
    "\n",
    "<img src=\"./images/gini_decrease_1.png\" alt=\"gini_decrease\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details><summary>What is the decrease in Gini impurity if we split on $X_2$? (Sunny Day vs. Rainy Day)</summary>\n",
    "    \n",
    "- Answer: 0.167\n",
    "\n",
    "<img src=\"./images/gini_decrease_2.png\" alt=\"gini_decrease\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One consequence of this is that a decision tree is fit using a **greedy** algorithm. Simply put, a decision tree makes the best short-term decision by optimizing at each node individually. _This might mean that our tree isn't optimal in the long run!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Titanic data.\n",
    "\n",
    "\n",
    "# Change sex to int.\n",
    "\n",
    "\n",
    "# Create embarked_S column.\n",
    "\n",
    "\n",
    "# Create embarked_C column.\n",
    "\n",
    "\n",
    "# Conduct train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out first five rows of X_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What conclusion would you make here?</summary>\n",
    "\n",
    "- Our model is **very** overfit to the data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting a decision tree, your model will always grow until it nearly perfectly predicts every observation!\n",
    "- This is like playing a game of 20 questions, but instead calling it \"Infinite Questions.\" You're always going to be able to win!\n",
    "\n",
    "<details><summary>Intuitively, what might you try to do to solve this problem?</summary>\n",
    "    \n",
    "- As with all models, try to gather more data.\n",
    "- As with all models, remove some features.\n",
    "- Is there a way for us to stop our model from growing? (Yes!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Decision Trees\n",
    "There are three hyperparameters of decision trees that we may commonly tune in order to prevent overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "    - By default, the nodes are expanded until all leaves are pure (or some other argument limits the growth of the tree).\n",
    "    - In the 20 questions analogy, this is like \"How many questions we can ask?\"\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - By default, the minimum number of samples required to split is 2. That is, if there are two observations in a node and if we haven't already achieved maximum purity, we can split it!\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree).\n",
    "    - By default, the minimum nubmer of samples required in a leaf node is 1. (This should ring alarm bells - it's very possible that we'll overfit our model to the data!)\n",
    "\n",
    "[Source: Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with:\n",
    "# - a maximum depth of 5.\n",
    "# - at least 7 samples required in order to split an internal node.\n",
    "# - at least 3 samples in each leaf node.\n",
    "# - random state of 42.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model.\n",
    "print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's GridSearch to try to find the best tree.\n",
    "\n",
    "- Check [2, 3, 5, 7] for `max_depth`.\n",
    "- Check [5, 10, 15, 20] for `min_samples_split`.\n",
    "- Check [2, 3, 4, 5, 6] for `min_samples_leaf`.\n",
    "- Run 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many models are being fit here?</summary>\n",
    "\n",
    "4 * 4 * 6 * 5 = 480 models.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = \n",
    "\n",
    "params = {\n",
    "    \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jupyter magic\n",
    "\n",
    "\n",
    "# Let's GridSearch over the above parameters on our training data.\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our best decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were the best parameters of our decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the cross-validated score of the above decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity/recall/ true pos rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate specificity/true neg rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "- Decision trees can give us the relative importance of features: the higher the number, the more important the predictor was to deciding splits at nodes.\n",
    "- From the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_): \"The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Output of Decision Trees\n",
    "\n",
    "One advantage to using a decision tree is that you can easily visualize them in `sklearn`. The two functions used to do this are `plot_tree` and `export_text`.\n",
    "- [`plot_tree` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)\n",
    "- [`export_text` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot_tree from sklearn.tree module.\n",
    "\n",
    "\n",
    "# Establish size of figure.\n",
    "\n",
    "\n",
    "# Plot our tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import export_text from sklearn.tree module.\n",
    "\n",
    "\n",
    "# Print out tree in plaintext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all visualizations, just because we _can_ doesn't mean that we _should_. If our depth is much larger than 2 or 3, the tree may be unreadable.\n",
    "\n",
    "While these visuals may be helpful to us, it may be helpful to clean it up before presenting it to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use a decision tree?\n",
    "\n",
    "\n",
    "### 1. We don't have to scale our data.\n",
    "The scale of our inputs don't affect decision trees.\n",
    "\n",
    "### 2. Decision trees don't make assumptions about how our data is distributed.\n",
    "Is our data heavily skewed or not normally distributed? Decision trees are nonparametric, meaning we don't make assumptions about how our data or errors are distributed.\n",
    "\n",
    "### 3. Easy to interpret.\n",
    "The output of a decision tree is easy to interpret and thus are relatable to non-technical people.\n",
    "\n",
    "### 4. Speed.\n",
    "Decision trees fit very quickly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not use a decision tree?\n",
    "\n",
    "\n",
    "### 1. Decision trees can very easily overfit.\n",
    "Decision trees often suffer from high error due to variance, so we need to take special care to avoid this. (There are lots of algorithms designed to do exactly this!)\n",
    "\n",
    "### 2. Decision trees are locally optimal.\n",
    "Because we're making the best decision at each node (greedy), we might end up with a worse solution in the long run.\n",
    "\n",
    "### 3. Decision trees don't work well with unbalanced data.\n",
    "We often will bias our results toward the majority class. We need to take steps to avoid this as well! (Check out the `class_weight` parameter if you're interested.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If you're comparing decision trees and logistic regression, what are the pros and cons of each?</summary>\n",
    "    \n",
    "(Answers may vary; this is not an exhaustive list!)\n",
    "- **Interpretability**: The coefficients in a logistic regression model are interpretable. (They represent the change in log-odds caused by the input variables.) However, this is complicated and not easy for non-technical audiences. Decision trees are interpretable; it is easy to explain to show a picture of a decision tree to a client or boss and get them to understand how predictions are made.\n",
    "- **Performance**: Decision trees have a tendency to easily overfit, while logistic regression models usually do not overfit as easily.\n",
    "- **Assumptions**: Decision trees have no assumptions about how data are distributed; logistic regression does make assumptions about how data are distributed.\n",
    "- **Frequency**: Logistic regression is more commonly used than decision trees.\n",
    "- **Y variable**: Decision trees can handle regression and classification problems; logistic regression is only really used for classification problems.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "0b28c6b3b13649718658d43e965c8062": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

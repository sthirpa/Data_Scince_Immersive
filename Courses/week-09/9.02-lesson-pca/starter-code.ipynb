{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "_Authors: Justin Pounders, Matt Brems, Noelle Brown, Patrick Wales-Dinan_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "By the end of the lesson, students should be able to:\n",
    "1. Differentiate between feature selection and feature extraction.\n",
    "2. Describe the PCA algorithm.\n",
    "3. Implement PCA in `scikit-learn`.\n",
    "4. Calculate and interpret proportion of explained variance.\n",
    "5. Identify use cases for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import from sklearn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Set a random seed.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction of Problem\n",
    "\n",
    "Today, we're going to be using the [wine quality](http://www3.dsi.uminho.pt/pcortez/wine/) dataset by Cortez, Cerdeira, Almeida, Matos and Reis.\n",
    "\n",
    "Specifically, we are going to use physicochemical properties of the wine in order to predict the quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the wine quality datasets.\n",
    "\n",
    "\n",
    "# Stack datasets together. (They have the same column names!)\n",
    "\n",
    "\n",
    "# Check out head of our dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a multiple linear regression model in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set y to be the quality column.\n",
    "\n",
    "\n",
    "# Set X as all other columns.\n",
    "\n",
    "\n",
    "# How much missing data do we have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show off the strength of PCA, we're going to make many, many more features.\n",
    "\n",
    "\n",
    "# Fit and transform our X data using Polynomial Features.\n",
    "\n",
    "\n",
    "# How many features do we have now?\n",
    "\n",
    "\n",
    "# How many features did we start out with?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Score on training set. (We'll use R^2 for the score today.)\n",
    "\n",
    "\n",
    "# Score on testing set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: What is the problem with this?</summary>\n",
    "    \n",
    "- We've clearly overfit our model to the data (so much so that our model's performance is really bad)!\n",
    "- We have a lot of columns relative to our number of rows! (If you have $n$ rows and you're fitting a linear model, it's often advised to keep your number of columns below $\\sqrt{n}$.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: How can we overcome this problem?</summary>\n",
    "\n",
    "- We can drop features from our model. (However, this loses any benefit we'd get from including those features! It can also be time-consuming and/or require subject-matter expertise.)\n",
    "- Maybe we can combine features together so that we can get the benefits of most/all of our features. (This is what PCA will do.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "[Dimensionality reduction](https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/) refers to (approximately) reducing the number of features we use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Dimensionality reduction has a number of advantages:</summary>\n",
    "\n",
    "- Increases computational efficiency when fitting models.\n",
    "- Can help with addressing a multicollinearity problem.\n",
    "- Makes visualization simpler (or feasible).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Dimensionality reduction can suffer from some drawbacks, though:</summary>\n",
    "\n",
    "- We've invested our time and money into collecting information... why do we want to get rid of it?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a way to get the advantages of dimensionality reduction while minimizing the drawbacks?\n",
    "\n",
    "Dimensionality reduction can generally be broken down into one of two categories:\n",
    "\n",
    "<img src=\"./images/dim_red.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "- **Feature Selection**\n",
    "    - We drop variables from our model.\n",
    "- **Feature Extraction**\n",
    "    - In feature extraction, we take our existing features and combine them together in a particular way. We can then drop some of these \"new\" variables, but the variables we keep are still a combination of the old variables!\n",
    "    - This allows us to still reduce the number of features in our model **but** we can keep all of the most important pieces of the original features!\n",
    "\n",
    "<img src=\"./images/feast.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "### $$\n",
    "\\begin{eqnarray*}\n",
    "X_1, \\ldots, X_p &\\Rightarrow& Z_1, \\ldots, Z_p \\\\\n",
    "\\\\\n",
    "\\text{most important: }Z_1 &=& w_{1,1}X_1 + w_{1,2}X_2 + \\cdots + w_{1,p}X_p \\\\\n",
    "\\text{slightly less important: }Z_2 &=& w_{2,1}X_1 + w_{2,2}X_2 + \\cdots + w_{2,p}X_p \\\\\n",
    "&\\vdots&\\\\\n",
    "\\text{least important: }Z_p &=& w_{p,1}X_1 + w_{p,2}X_2 + \\cdots + w_{p,p}X_p \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- We don't usually care about the values of weights here. They aren't very meaningful and we don't try to interpret them.\n",
    "- You can think of $Z_1$ as a \"high performance\" predictor, where $Z_1$ has all of the best pieces of our original predictors $X_1$ through $X_p$.\n",
    "- As we move down the list toward $Z_p$, the variables will consist of the more \"redundant\" parts of our $X$ variables. \n",
    "- You can think of $Z_p$ as a \"low performance\" predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If I'm going to keep three of my new predictors, which three would I keep?</summary>\n",
    "    \n",
    "- The first three: $Z_1$, $Z_2$, and $Z_3$.\n",
    "- This is how we do feature extraction.\n",
    "    - We take our old features $X_1$, $X_2$, $X_3$, and $X_4$.\n",
    "    - We turn them into new features $Z_1$, $Z_2$, $Z_3$, and $Z_4$.\n",
    "    - The new features are combinations of our old features.\n",
    "    - If we drop new features, we're doing dimensionality reduction, but we also keep parts of every old feature!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction can be used as an exploratory/unsupervised learning method or as a pre-processing step for supervised learning later.\n",
    "\n",
    "**Principal component analysis** is one algorithm for doing feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How would you describe the difference between feature selection and feature extraction?</summary>\n",
    "\n",
    "- Feature selection is a process of dropping original features from our model.\n",
    "- Feature extraction is a process of transforming our original features into \"new\" features, then dropping some of the \"new\" features from our model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "### Big picture, what is PCA doing?\n",
    "1. We are going to look at how all of the $X$ variables relate to one another and summarize these relationships.\n",
    "2. Then, we will take this summary and look at which combinations of our $X$ variables are most important.\n",
    "3. We can also quantify how important each combination is and rank these combinations.\n",
    "\n",
    "Once we've taken our original $X$ data and transformed it into $Z$, we can then drop the columns of $Z$ that are \"least important.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are this [Whale shark](https://en.wikipedia.org/wiki/Whale_shark):\n",
    "\n",
    "<img src=\"./images/whaleshark.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "And you want a snack. Which way would you tilt your head to eat the most krill at once?\n",
    "\n",
    "<img src=\"./images/krill.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Above artwork by [@allison_horst](https://twitter.com/allison_horst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pca.gif\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[Source](https://rpubs.com/jormerod/594859)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visually...**\n",
    "\n",
    "> Think of our data floating out in $p$-dimensional space. Each observation is a dot and you can imagine this massive cloud of dots that exists somewhere. PCA is a way to rotate this cloud of dots (formally, a [coordinate transformation](http://farside.ph.utexas.edu/teaching/336k/Newtonhtml/node153.html)). The old axes are the original $X_1$, $X_2$, $\\ldots$ features. **The new axes are the principal components from PCA**.\n",
    "\n",
    "The principal components are the most concise, informative descriptors of our data as a whole.\n",
    "- What does this mean?\n",
    "- If we wanted to take our full data set and condense it into one dimension (think like our $X$ axis), we'd only use $Z_1$.\n",
    "- If we wanted to take our full data set and condense it into two dimensions (think like our $X$ and $Y$ axes), we'd use $Z_1$ and $Z_2$.\n",
    "\n",
    "Let's head to [this site](http://setosa.io/ev/principal-component-analysis/). Play around with the 2D data. Take 2-3 minutes.\n",
    "1. As you interact with the data, how would you describe the red line?\n",
    "2. As you interact with the data, how would you describe the green line?\n",
    "\n",
    "---\n",
    "\n",
    "### Principal Components\n",
    "\n",
    "- We are looking for new *directions*. ([Insert Glee joke here](https://glee.fandom.com/wiki/New_Directions).)\n",
    "- Each consecutive direction tries to explain the maximum *remaining variance* in our $X$ data.\n",
    "- Each direction is *orthogonal* to all the others.\n",
    "\n",
    "**These new *directions* are the \"principal components.\"**\n",
    "\n",
    "> Applying PCA to your data *transforms* your original data columns (variables) onto the new principal component axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two notes:\n",
    "\n",
    "1. Train/test split **before** applying PCA!\n",
    "2. Standardize our data **before** applying PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our StandardScaler.\n",
    "\n",
    "\n",
    "# Standardize X_train.\n",
    "\n",
    "\n",
    "# Standardize X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA.\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (BONUS) Why decomposition?\n",
    "The way PCA works \"under the hood\" is it takes one matrix and **decomposes** that matrix into multiple matrices.\n",
    "\n",
    "Written out, we might take some matrix $\\mathbf{A}$ and break it down into multiple matrices like this:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{A} &=& \\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Check out [the Wikipedia article](https://en.wikipedia.org/wiki/Matrix_decomposition) for a list of ways to decompose matrices.\n",
    "- A specific method of decomposition commonly used for PCA is known as the [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or spectral decomposition of a matrix. However, eigendecompositon requires [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix) matrices. To generalize this to non-square/non-diagonalizable matrices, we more commonly use the [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) for PCA. [PCA in Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) uses SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA.\n",
    "\n",
    "\n",
    "# Fit PCA on the training data.\n",
    "\n",
    "\n",
    "# Transform PCA on the training data.\n",
    "\n",
    "\n",
    "# Don't forget to transform the test data!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform PCA on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the resulting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to transform the test data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, like, big picture, what is PCA doing?\n",
    "Well, we're transforming our data. Specifically, we are:\n",
    "1. We are going to look at how all of the $X$ variables relate to one another, then summarize these relationships. (This is done with the **covariance matrix**.)\n",
    "2. Then, we will take this summary and look at which combinations of our $X$ variables are most important. (We will decompose our covariance matrix into its **eigenvectors**, which is a linear algebra term that allows us to understand the most important \"directions\" in our data, which are our principal components!)\n",
    "3. We can also see exactly how important each combination is, then rank these combinations. (With each eigenvector, we get an **eigenvalue**. This eigenvalue is a number that tells us how important each \"direction\" or principal component is.)\n",
    "    - Want a better understanding of eigenvectors and eigenvalues? [Check this 3Blue1Brown video out!](https://www.youtube.com/watch?v=PFDu9oVAE-g)\n",
    "\n",
    "Remember that one of our goals with PCA is to do dimensionality reduction (a.k.a. get rid of features).\n",
    "\n",
    "We can: \n",
    "- measure how important each principal component is using the eigenvalue, \n",
    "- rank the columns of `Z_train` by their eigenvalues,\n",
    "- then drop the columns with small eigenvalues (little importance) but keep the columns with big eigenvalues (very important).\n",
    "    - In `sklearn`, when transformed by PCA, the columns will already be sorted by their eigenvalues from biggest to smallest! The first column will be the most important, the second column will be the next most important, and so on.\n",
    "\n",
    "#### But how many features do we discard?\n",
    "\n",
    "A useful measure is the **proportion of explained variance**, which is calculated from the **eigenvalues**. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ \\text{explained variance of }PC_k = \\bigg(\\frac{\\text{eigenvalue of } PC_k}{\\sum_{i=1}^p\\text{eigenvalue of } PC_i}\\bigg)$$\n",
    "\n",
    "Rather than write out \"$\\text{eigenvalue of } PC_k$\", we usually just write $\\lambda_k$.\n",
    "\n",
    "If I want to calculate the proportion of explained variance by retaining $PC_1$ and $PC_2$, I would calculate this as:\n",
    "\n",
    "### $$ \\text{explained variance of } PC_1 \\text{ and } PC_2 = \\bigg(\\frac{\\lambda_1 + \\lambda_2}{\\sum_{i=1}^p \\lambda_i} \\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the explained variance attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the cumulative explained variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total explained variance vs components  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: If I wanted to explain at least 80% of the variability in my data with principal components, what is the smallest number of principal components that I would need to keep? </summary>\n",
    "\n",
    "- Only six! \n",
    "- I could keep $Z_1, Z_2, \\ldots, Z_6$ in my model, and this would explain 80.8% of the variability in my $X$ data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare our PCA'ed performance to our original performance!\n",
    "\n",
    "#### Original performance:\n",
    "\n",
    "<img src=\"./images/lr_performance.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Regression performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA with 10 components.\n",
    "\n",
    "\n",
    "# Fit PCA to training data.\n",
    "\n",
    "\n",
    "# Instantiate linear regression model.\n",
    "\n",
    "\n",
    "# Transform Z_train and Z_test.\n",
    "\n",
    "\n",
    "# Fit on Z_train.\n",
    "\n",
    "\n",
    "# Score on training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model here is:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "[\\text{quality}] &=& \\beta_0 + \\beta_1Z_1 + \\beta_2Z_2 + \\cdots + \\beta_{10}Z_{10} \\\\\n",
    "\\\\\n",
    "\\text{where } Z_1 &=& \\gamma_1X_1 + \\gamma_2X_2 + \\gamma_3X_3 + \\cdots \\gamma_pX_p \\\\\n",
    "\\text{and } Z_2 &=& \\delta_1X_1 + \\delta_2X_2 + \\delta_3X_3 + \\cdots \\delta_pX_p \\\\\n",
    "&\\vdots& \\\\\n",
    "\\text{and } Z_{10} &=& \\eta_1X_1 + \\eta_2X_2 + \\eta_3X_3 + \\cdots \\eta_pX_p \\\\\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a PCA dataframe\n",
    "columns = [f'PCA_{i+1}' for i in pd.DataFrame(Z_train).columns]\n",
    "z_df = pd.DataFrame(data = Z_train, columns=columns)\n",
    "z_df['Wine_Quality'] = y_train.values\n",
    "z_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA_1 vs. PCA_10\n",
    "sns.lmplot(\n",
    "    x=\"PCA_1\",\n",
    "    y=\"PCA_10\",\n",
    "    data=z_df, \n",
    "    fit_reg=False, \n",
    "    hue='Wine_Quality', # color by cluster\n",
    "    legend=True,   \n",
    "    scatter_kws={\"s\": 30} # specify the point size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two assumptions that PCA makes:**\n",
    "1. **Linearity:** PCA detects and controls for linear relationships, so we assume that the data does not hold nonlinear relationships (or that we don't care about these nonlinear relationships).\n",
    "    - We are using our covariance matrix to determine important \"directions,\" which is a measure of the linear relationship between observations!\n",
    "    - There are other types of feature extraction like [t-SNE](https://lvdmaaten.github.io/tsne/) and [PPA](https://towardsdatascience.com/interesting-projections-where-pca-fails-fe64ddca73e6), though we won't formally cover those in a global lesson.\n",
    "    \n",
    "    \n",
    "2. **Large variances define importance:** If data is spread in a direction, that direction is important! If there is little spread in a direction, that direction is not very important.\n",
    "    - That aligns with what we saw [here](http://setosa.io/ev/principal-component-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Use Cases for PCA\n",
    "- Situations where $p \\not\\ll n$. (Situations where $p$ is not substantially smaller than $n$.)\n",
    "- Situations in which there are variables with high multicollinearity. (Can be traditional models or models with highly correlated inputs by design, like images.)\n",
    "- Situations in which there are many variables, even without explicit multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Explain PCA to me.</summary>\n",
    "\n",
    "- Principal component analysis is a method of dimensionality reduction that **identifies important relationships** in our data, **transforms the existing data** based on these relationships, and then **quantifies the importance** of these relationships so we can keep the most important relationships and drop the others!\n",
    "\n",
    "<details><summary>How can I remember the above?</summary>\n",
    "\n",
    "Matt's \"Three Signposts:\"\n",
    "- Covariance Matrix\n",
    "- Eigenvectors\n",
    "- Eigenvalues\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In what cases would I not use PCA?</summary>\n",
    "\n",
    "- Since PCA distorts the interpretability of our features, we should not use PCA if our goal is to interpret the output of our model.\n",
    "- If we have relatively few features as inputs, PCA is unlikely to have a large positive impact on our model.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ffcd6f-3daa-4d5e-88b7-b9ffe280c426",
   "metadata": {},
   "source": [
    "### Getting back to the perceptron: \n",
    " **Note that, the input layer is given biase, `b`, by introducing an extra input node that always has a value 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96453d38-dd33-4918-a8fd-b72b8ed2ad5b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph5.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07324027-b400-47d0-8020-c1479650890b",
   "metadata": {},
   "source": [
    "*Graph drawn by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874208f-672d-49a4-a72c-edb07c751ecb",
   "metadata": {},
   "source": [
    "Simplifying our example, let's assume $X = [x_1, x_2]$ and $W = [3, -2]$.<br>\n",
    "Then our $\\hat y = \\sigma(b + X^TW)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8ea98-c658-477f-8bc7-b23204ca35c7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/eqn1.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dc828-8003-4364-9c45-2d4f0829c703",
   "metadata": {},
   "source": [
    "*Picture by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0b1be-f20b-49d1-bd35-48e40ef45873",
   "metadata": {},
   "source": [
    "* Now, let's draw the equation of the hyperplane ( a line in 2D):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98112-8710-443c-b23b-35403190019a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph6.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb7dab-4f0f-4041-8272-5c2e71681163",
   "metadata": {},
   "source": [
    "*Graph drawn by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7694f8b-9f43-482b-826b-122c1f1be3c1",
   "metadata": {},
   "source": [
    "* The hyperplane (labeled by broken-line) corresponds to the decesion line that the Neural network makes to classify a given input from the ($X_1, X_2$) plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38f7e5-69d5-4fb5-8c4f-30185b054711",
   "metadata": {},
   "source": [
    "Example: Assume we have an input value $X = [-1, 2]$, and substituting $x_1 = -1$ and $x_2 = 2$ into the previous equation, $\\sigma (1 + 3x_1 - 2x_2) = \\sigma(-6)$, assume our activation function is `sigmoid`, we have: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cfc28-0adf-4993-baed-6eb8b6a722a7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    " = \\frac{1}{1+e^6} = 0.0025\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c1737-13ce-42d0-ad79-88df13470120",
   "metadata": {},
   "source": [
    "Now, since $\\hat y = \\sigma (1 + 3x_1 - 2x_2) \\approx 0.0025 < 0.5$, the activation function, `Sigmoid`, assigns our $X$ to the left of our linear classifier (the broken-line in the $(x_1,x_2)$ plane above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d148a2-0f4e-4863-b526-85501980a7bc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph1.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83500418-2872-47e4-98de-841e4ab583fa",
   "metadata": {},
   "source": [
    "*Graph1. Sigmoid function (source from DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a0fa2-b187-4060-b849-f74450f5352c",
   "metadata": {},
   "source": [
    "* With this basic concept of perceptron, we intuitively conclude that multilayer perceptrons are just a stack of single layer perceptrons and hence the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966858d-af6a-4210-b36e-0f9f15e9a404",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv20.png\" width=\"600\"/> \n",
    "<div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f45210-cbf0-4c6b-b409-81cdd99d366a",
   "metadata": {},
   "source": [
    "*Fig.9: source: [Multi-Layer Neural Networks with Sigmoid Function](https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23130e-9882-4b28-8b74-0be3dfff7c73",
   "metadata": {},
   "source": [
    "- Now if we want to define a multi-output NN, we can simply add another perceptron to this above picture so instead of having one perceptron now we have two perceptrons and so on. Here is an example of a multi-output perceptron. Note that perceptron is stacked and there are two outputs ([source](https://vitalflux.com/how-do-we-build-deep-neural-network-using-perceptron/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2233a30-9d6b-4e2f-bbe4-9446e356fd55",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv21.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7777f86-c1bc-41dd-958b-9df64b215d22",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv22.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5f5f3-11f7-4b57-b177-f05702d1930f",
   "metadata": {},
   "source": [
    "*Fig.10 Source: [Data Analytics](https://vitalflux.com/how-do-we-build-deep-neural-network-using-perceptron/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940339c1-b9ae-40ff-8210-11a7b1f655f3",
   "metadata": {},
   "source": [
    "* Activation and Loss Functions are discussed in the next subtopic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621bb66-e864-428b-b800-3fe100c6e353",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d7eda-894d-479b-b22e-f87e9e8e5589",
   "metadata": {},
   "source": [
    "The entire DL model works around the idea of extracting useful features that clearly define the objects in the image. Machine learning models are only as good as the features you provide. That means coming up with good features is an important job in building ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d64e3-cce4-4677-ba96-2b1fad232777",
   "metadata": {},
   "source": [
    ">`DEFINITION`: <br>\n",
    "A feature in machine learning is an individual measurable property or characteristic of an observed phenomenon. Features are the input that you feed to your ML model to output a prediction or classification. Suppose you want to predict the price of a house: your input features (properties) might include `square_foot`, `number_of_rooms`, `bathrooms`, and `so on`, and the model will output the predicted price based on the values of your features. Selecting good features that clearly distinguish your objects increases the predictive power of ML algorithms. <br> - In Computer Vision, a feature is a measurable piece of data in your image that is unique to that specific object. It may be a distinct color or a specific shape such as a line, edge, or image segment. A good feature is used to distinguish objects from one another (Mohammed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f1d8e3-3b24-45d1-bec2-c1654ee75ee8",
   "metadata": {},
   "source": [
    "**FEATURE GENERALIZABILITY**: A very important characteristic of a feature is repeatability.BUT, WHAT MAKES A GOOD FEATURE FOR OBJECT RECOGNITION? \n",
    "* Identifiable\n",
    "\n",
    "* Easily tracked and compared\n",
    "\n",
    "* Consistent across different scales, lighting conditions, and viewing angles\n",
    "\n",
    "* Still visible in noisy images or when only part of an object is visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a5a37-c828-43b9-8b60-43250adb777b",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "I would like to start with an example from a book, `Deep Learning for Vision Systems`, by Mohammed. <br> \n",
    "Suppose we have a database of U.S presidents and we want to build a classification pipeline to tell us which president this image is of. So we feed this image that we can see on the left hand side (`fig.7` below) to our model and we wanted to output the probability that this image is of any of these particular presidents that this dataset consists of.\n",
    "In order to classify these images correctly though, our pipeline needs to be able to tell what is actually unique about a picture of Abraham Lincoln vs a picture of any other president like George Washington or Jefferson, or Obama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480caeb-db6f-45c6-85cf-408fa0273eab",
   "metadata": {},
   "source": [
    "* Remember, **Features make pictures unique**. <br>\n",
    "Let's identify high level key features in the human, auto, and house image categories: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74007acb-9600-4814-aa2a-ece14f513687",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"images/cv6.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640ad0f-3a51-46b8-ab48-65e459ad0ac5",
   "metadata": {},
   "source": [
    "*Fig.12: Source from [Convolutional Neural Networks](http://introtodeeplearning.com)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3d29b-e308-41ff-b16a-53a76e31ad88",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv5.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83341c-53ee-4a3d-981e-e1793f9c52d8",
   "metadata": {},
   "source": [
    "*Fig.11: Source from [Convolutional Neural Networks](http://introtodeeplearning.com)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686407b-bc14-4fca-b079-6b3ede0dbbd1",
   "metadata": {},
   "source": [
    "- This way computers classify images by assigning the corresponding probabilities based on features of pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da84934-68da-40cf-b74a-85e14d867bc8",
   "metadata": {},
   "source": [
    "## Convolution Layers\n",
    "Now, suppose each feature is like a mini image; it's a patch. It's also a small 2D array of values and we'll use `filters` to pick up on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a791bf8-b4cd-4f33-be3d-6b4579a250db",
   "metadata": {},
   "source": [
    ">Convolution Layer:<br>\n",
    "The convolution layer is where we pass a filter over an image and do some calculation at each step. Specifically, we take pixels that are close to one another, then summarize them with one number. The goal of the convolution layer is to identify important features in our images, like edges.\n",
    "Source: [Here](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9f21e-de55-4d1c-ac2b-daefe289e972",
   "metadata": {},
   "source": [
    "Let's  use a $3X3$ `edge-detection` filter that amplifies the edges to the image below, then this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc3bf9-9e5a-4523-9dde-da6d2bbaf1bf",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 0\\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a5d68-157e-4de5-89cc-0b5ebd3811a6",
   "metadata": {},
   "source": [
    "`kernel` is convoluted with the input image, say $F(x,y)$, it creates a new convolved image (a feature map) that amplifies the edges (See `Fig.9` below). Zooming-in, we see `Fig.10` where a small piece of an image shows how the convolution operation is applied to get the new pixel value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560c5f9-5780-4ac6-ba1e-ad3390be39c5",
   "metadata": {},
   "source": [
    "![](images/cv7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb351676-79d2-424d-be4f-5ea199f682b9",
   "metadata": {},
   "source": [
    "![](images/cv8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2bb5a-345a-4488-a434-ff26d9b6d6c5",
   "metadata": {},
   "source": [
    "`Fig.conv: Applying Filter - source from (Mohammed 109-110)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b56ec-0882-4041-9a27-7e81140205c4",
   "metadata": {},
   "source": [
    ">Other filters can be applied to detect different types of features. For example, some filters detect `horizontal edges`, others detect `vertical edges`, still others detect more complex shapes like corners, and so on. The point is that these filters, when applied in the convolutional layers, yield feature-learning behavior: first they learn simple features like edges and straight lines, and later layers learn more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7e428-99dc-4f14-9626-038e95ee4084",
   "metadata": {},
   "source": [
    "Here are the three elements that enter into the convolution operation:\n",
    "\n",
    "* Input image\n",
    "* Feature detector or `kernel`, or `filter` used interchangeably \n",
    "* Feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5ac18-59c2-492a-bdee-c0643a48166a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv9.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d1783-3a5c-4e5e-b435-415ae178877e",
   "metadata": {},
   "source": [
    "Fig.15: [source](https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3bc501-cd6c-4d2a-8d16-f60c50782c3f",
   "metadata": {},
   "source": [
    "The example we gave above is a very simplified one, though. In reality, convolutional neural networks develop multiple feature detectors and use them to develop several feature maps which are referred to as convolutional layers.\n",
    "Through training, the network determines what features it finds important in order for it to be able to scan images and categorize them more accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187b545-2b8f-433e-a93f-6a1381ab61df",
   "metadata": {},
   "source": [
    "![](https://media3.giphy.com/media/i4NjAwytgIRDW/200.webp?cid=ecf05e471vftp51bx55s3lbh1el698xc1bv7l7rhy0igcpz3&rid=200.webp&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92825c-a06b-47a0-a265-3ceb5e49d3c6",
   "metadata": {},
   "source": [
    "### For a 3D array convolution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea78b0-fa3f-4932-85a0-6eb1f3d3aebc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/conv.gif\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a201c-895e-49d4-87b6-8cd5bfcc9acc",
   "metadata": {},
   "source": [
    "Source for the two `gif` images: [A Comprehensive Guide to Convolutional Neural Networks â€” the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948cc45-462d-48ee-b397-10a828bb16ce",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"img/cnn15.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb638a-d27a-465b-ae02-208dacb1386e",
   "metadata": {},
   "source": [
    "Fig.16: [Source](https://pylessons.com/Logistic-Regression-part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fcae3-88a9-4e15-9f4a-50aca72bb2b4",
   "metadata": {},
   "source": [
    "- Putting all together:<br>\n",
    "The term **`convolution`** refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f957d0-aa88-433d-b94c-057f624ef6be",
   "metadata": {},
   "source": [
    ">We perform a series `convolution + pooling operations, followed by a number of fully connected layers`. If we are performing multiclass classification the output is softmax.[fig.*source*](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2) <br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0b8ef-4619-45ed-8d00-53961983d96d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/architecture.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a5968-2574-40e2-8944-d99969edd395",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/CNN_architecture.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679371a9-6000-495e-b6a1-12e9b896ffaf",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/CNN_from_Scratch.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476a8c9-796b-4920-a56a-ee8533436a81",
   "metadata": {},
   "source": [
    "*Fig.17: CNN Architecture ([Source](https://www.mathworks.com/videos/introduction-to-deep-learning-what-are-convolutional-neural-networks--1489512765771.html)).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21de92-3902-455b-9b9b-fb63a168fc29",
   "metadata": {},
   "source": [
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6202edf-ea04-4841-8235-b43a7ed0bd9b",
   "metadata": {},
   "source": [
    ">It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 .\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97308b8b-8fbd-494a-aea5-0b72aa7335b5",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv14.png\" width=\"500\"/> \n",
    "<div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7048de-fc7a-49cc-9aa8-e30009aaf9bd",
   "metadata": {},
   "source": [
    "Fig.18 [Source](https://cs231n.github.io/convolutional-networks/#conv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d4951-a9d1-4308-a367-ed36191fb962",
   "metadata": {},
   "source": [
    ">Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square)(Source:[Convolutional Neural Networks for Visual recognition](https://cs231n.github.io/convolutional-networks/#conv/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a969a-92a2-46b2-801a-d60d42a483ac",
   "metadata": {},
   "source": [
    "* We'll get back to the [code notebook](https://github.com/sthirpa/Data_Scince_Immersive-at-General-Assembly-/blob/Hirpa/CIFAR-10-SH.ipynb) for the implementation of this theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176043b-1129-485d-9ebd-4a1852c64ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

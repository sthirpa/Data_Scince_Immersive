{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39d2002-0981-44b8-bab1-2f271d06a8cc",
   "metadata": {},
   "source": [
    "# Building Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a0d19-c29b-467f-bf94-70ec43edf82c",
   "metadata": {},
   "source": [
    "### Let's start with building an Artificial Neural Network(ANN)\n",
    ">In figure below, we can see an analogy between biological neurons and artificial systems. Both contain a main processing element, a `neuron`, with input signals $(x_1, x_2, ..., x_n)$ and an output (Mohammed 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba744b-7497-4c21-89de-3c0b7db80896",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv15.png\" width=\"400\"/> \n",
    "<div>\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c689d3-a112-4b15-8404-10cca1747f08",
   "metadata": {},
   "source": [
    "Fig.ANN1: Single Neuron - Single layer Network (`Perceptron`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd0d69-2199-4e04-b862-348fb75bd643",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"images/cv16.png\" width=\"600\"/> \n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4067a7-4148-4a05-8375-d77a39921aac",
   "metadata": {},
   "source": [
    "Fig.ANN2: Multilayer Neuron\n",
    "Deep learning involves layers of neurons in a network or  `Multilayer perceptron` <br>\n",
    "Fig.7 ( for both `ANN1` and `ANN2`, above) source from (Mohammed 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ca0ab-890b-4fe3-9584-6172c69fcdff",
   "metadata": {},
   "source": [
    "* **ANN is imitation of how information is processed in human brain; when millions of neurons (`perceptrons in ANN`) are stacked in layers and connected together,a multilayer neural network is called `deep learning`.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92dbff-06a3-435c-9330-8196ca846728",
   "metadata": {},
   "source": [
    "# So, what is a `Perceptron`? \n",
    "* **Let's zoom in to a `Multilayer perceptron (MLP)` above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036bac58-5373-4622-92f2-687a61e9a8fb",
   "metadata": {},
   "source": [
    "### Perceptron is the fundamental building blocks of `Neural Networks` in Deep Learning. \n",
    "- If we really want to know how neural network works, we better learn closely how perceptron works.<br>\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Perceptron) definition, a `Perceptron` is an algorithm for learning a binary classifier called a `threshold function`: a function that maps its input $X$ (a real-valued vector) to an output value $f(X)$ (a single binary value):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b324e-4e3c-4e85-9f3a-c161a43b0d3d",
   "metadata": {},
   "source": [
    "$$\n",
    "f(X) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1  \\quad \\text{if} \\quad W.X + b > 0, \\\\ 0 \\quad \\text{otherwise} \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165f890-8715-40f7-b50d-55b447f04c90",
   "metadata": {},
   "source": [
    "where $W$ is a vector of real-valued weights, $W.X$ is the dot product $\\sum_{i=1}^{m}w_ix_i$, where $m$ is the number of inputs to the perceptron, and $b$ is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value. <br>\n",
    "The value of $f(X)$ ($0$ or $1$) is used to classify $X$  as either a positive or a negative instance, in the case of a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e143e8-7e0c-4ac7-b7f1-8a02915f04bf",
   "metadata": {},
   "source": [
    "In the context of neural networks, a perceptron is an `artificial neuron` using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest feedforward neural network (source: [Wikipedia](https://en.wikipedia.org/wiki/Perceptron))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fab4f-9e89-4281-9f44-3b76d632963a",
   "metadata": {},
   "source": [
    "## Notation:\n",
    "The dot product of two vectors `A` and `B`: <br>\n",
    "$A = [a_1, a_2, ..., a_n]$ and $B = [b_1, b_2, ..., b_n]$ is given by:<br>\n",
    "$$\n",
    "A.B = \\sum_{i=1}^{n}a_i*b_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15536f84-0a77-406d-b959-54ad9ddb4612",
   "metadata": {},
   "source": [
    "which is simply $A^T*B$, a matrix multiplication (source: [Dot product in matrix notation](https://mathinsight.org/dot_product_matrix_notation))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b53b8-9b03-4972-bec4-c3ae9f80c9f4",
   "metadata": {},
   "source": [
    "Now, getting back to our perceptron concept, assume we have the following vectors: `X` and `W` where $X= [x_1, x_2, x_3]$ for input vectors and $W = [w_1, w_2, w_3]$ for weight vector. For the sake of simplicity, let's assume $x_1 = 3, x_2 = -2$ and $w_0 = 1$ be the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63556f09-43f8-4fba-8eeb-2942b138f7f2",
   "metadata": {},
   "source": [
    ">You might get the impression that neural networks only understand the most useful features, but that’s not entirely true. Neural networks scoop up all the features available and give them random weights. During the training process, the neural network adjusts these weights to reflect their importance and how they should impact the output prediction. The patterns with the highest appearance frequency will have higher weights and are considered more useful features. Features with the lowest weights will have very little impact on the output (Mohammed 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea590c-44b6-442d-8900-7a20e1422915",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv18.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee8eaa-f647-4564-bb6f-25991b848243",
   "metadata": {},
   "source": [
    "   Fig.8: Source from (Mohammed 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242fb3e-4669-4428-8afb-953711e65471",
   "metadata": {},
   "source": [
    ">In both artificial and biological neural networks, a neuron does not just output the bare input it receives. Instead, there is one more step, called an activation function; this is the decision-making unit of the brain. In ANNs, the activation function takes the same weighted sum input from before ($z = Σxi · wi + b$) and activates (fires) the neuron if the weighted sum is higher than a certain `threshold`. This activation happens based on the activation function calculations (Mohammed 42). <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dce01f-7db2-40a6-9384-214313727c1a",
   "metadata": {},
   "source": [
    "### As we can see, a perceptron consists of 4 parts:\n",
    "    1. Input values or One input layer\n",
    "    1. Weights and Bias\n",
    "    1. Net sum\n",
    "    1. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03c6f4-af10-4736-869b-4e8b3b8cf934",
   "metadata": {},
   "source": [
    "According to `Mohammed`, the perceptron's learning logic goes like this:<br>\n",
    ">1. The neuron calculates the weighted sum and applies the activation function to\n",
    "make a prediction $\\hat y$. This is called the `feedforward process`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bacf78d-41cc-403b-b31f-f983bd4189bb",
   "metadata": {},
   "source": [
    "$$\\hat y = activation(\\sum x_i · w_i + b)$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c883b-63c3-47ef-99bd-4cd88b887929",
   "metadata": {},
   "source": [
    ">2. It compares the output prediction with the correct label to calculate the error:<br> $e r r o r = y – \\hat y$. <br>\n",
    "\n",
    "\n",
    ">3. It then updates the weight. If the prediction is too high, it adjusts the weight to make a lower prediction the next time, and vice versa. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14110c2d-04b7-44eb-8000-a7729ab18656",
   "metadata": {},
   "source": [
    ">4. Repeat! <br>\n",
    ">This process is repeated many times, and the neuron continues to update the weights to improve its predictions until step 2 produces a very small error (close to zero), which means the neuron’s prediction is very close to the correct value. At this point, we can stop the training and save the weight values that yielded the best results to apply to future cases where the outcome is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f7b09-d3e5-46ff-a70e-d17fb5956d7b",
   "metadata": {},
   "source": [
    "## Activation Functions:\n",
    "**a) Hiden layer activayion functions:** <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ff945-2e24-430b-b18c-8f80a592e733",
   "metadata": {},
   "source": [
    "*Example 1*: <br>\n",
    "*Sigmoid Activation Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5aabe-4dd6-4fc1-9990-58656dd3774c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60257b9-c249-4196-b459-cf48f90308ac",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph1.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27faf22-6a99-47fe-9f52-da48aa894a6a",
   "metadata": {},
   "source": [
    "*Graph1. Sigmoid function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fd71a-c8e1-4fb7-91cf-9c7f9ddc0493",
   "metadata": {},
   "source": [
    "*Example 2:* <br>\n",
    "ReLU (Rectified Linear Unit): ReLU is prefereable activation function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf86af9-a898-4b8e-a970-ca8d62879774",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = max\\{0, z\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f731e-8c35-40a5-8952-d3b1754cbce8",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph2.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f5fc8-806d-410c-86d1-90fe18c79e02",
   "metadata": {},
   "source": [
    "*Graph2. ReLU function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d980c-d5d4-46ca-abc5-74241d4eca16",
   "metadata": {},
   "source": [
    "**b) Output layer activation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10775ad0-b941-44da-bd14-ed00461dad97",
   "metadata": {},
   "source": [
    "*Exampe*: <br>\n",
    " **Softmax**:for multiclass classification <br>\n",
    "$$\n",
    " \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d434ed-f957-4031-8b0b-76554f8f3993",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph3.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33fe528-911f-43fa-97b3-08c28f5b8650",
   "metadata": {},
   "source": [
    "*Graph3. SoftMax function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0eb60-8758-42e6-aca8-9a830679a4c0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph4.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280705e6-bb5f-4f62-b59d-68a224d95086",
   "metadata": {},
   "source": [
    "Graph4. SoftMaxfunction explained ([source](https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60))\n",
    "where, according to wikipedia, it applies the standard exponential function to each element $z_i$ of the input vector $z$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma(z)$ is 1(source: [Softmax function](https://en.wikipedia.org/wiki/Softmax_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd92ed1-fce6-46e4-9d85-13770e579bff",
   "metadata": {},
   "source": [
    ">Activations in the hidden layer provide a transformation that allow the neural net\n",
    "to learn more complex relationships as calculations propagate through the\n",
    "network(Source: DSIR-111 Classnote)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b9b30-7e3d-4db4-80f8-8c87e8e0d4ae",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e023916-cb1e-46b5-8228-c91fb12f562e",
   "metadata": {},
   "source": [
    "**Loss** is a measure of performance of a model. The lower, the better. When learning, the model aims to get the lowest loss possible. We use `crossentropy` for multiclass classification([source](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496743f3-b08f-490a-9d9e-5ecd58265de1",
   "metadata": {},
   "source": [
    "## Examples:\n",
    "a) For Binary Classification: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7e8cb-766a-478c-ab11-821667a90afb",
   "metadata": {},
   "source": [
    "$$\n",
    "LOSS = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i * log(\\hat y_i) + (1-y_i) * log(1 - \\hat y_i)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c613d69-73ab-4963-b863-45df716a7bf1",
   "metadata": {},
   "source": [
    "b) For Multi-class Classification:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac10626-4c89-46b1-a4b8-f7d9c03aa260",
   "metadata": {},
   "source": [
    "$$\n",
    "LOSS = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i * log(\\hat y_i)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71790623-428b-4b5a-9b90-f86dc5c5aed9",
   "metadata": {},
   "source": [
    "Source: [Most Common Loss Functions in Machine Learning](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a969a-92a2-46b2-801a-d60d42a483ac",
   "metadata": {},
   "source": [
    "* We'll get back to the [code notebook](https://github.com/sthirpa/Data_Scince_Immersive-at-General-Assembly-/blob/Hirpa/CIFAR-10-SH.ipynb) for the implementation of this theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176043b-1129-485d-9ebd-4a1852c64ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe4e40a-d6a8-4f23-ba6d-427436fb9a64",
   "metadata": {},
   "source": [
    "## Activation Functions and Loss Functions:\n",
    "**a) Hiden layer activayion functions:** <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af391ff-0c6e-425a-b089-6b4cffa9a162",
   "metadata": {},
   "source": [
    "*Example 1*: <br>\n",
    "*Sigmoid Activation Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e529e-d678-48cb-bd37-70ef3f7d4447",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fa07b-261c-41c9-90f7-b154ea14b7d2",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph1.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed521a3-8f2d-44ae-9768-a7dfea9280d4",
   "metadata": {},
   "source": [
    "*Graph1. Sigmoid function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cda0e-0e52-4ccb-886f-6c752ff3739b",
   "metadata": {},
   "source": [
    "*Example 2:* <br>\n",
    "ReLU (Rectified Linear Unit): ReLU is prefereable activation function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d853e1-8612-4598-ba0b-e9abc6df4c2e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = max\\{0, z\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77833959-5c54-4695-93f3-e0e9cb457a89",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph2.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b23d8-e0a3-476c-9312-cba58b55e548",
   "metadata": {},
   "source": [
    "*Graph2. ReLU function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04296fef-1d13-45b3-9bba-925426c82b7d",
   "metadata": {},
   "source": [
    "**b) Output layer activation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c918922-e324-4651-b142-20f59f00f9da",
   "metadata": {},
   "source": [
    "*Exampe*: <br>\n",
    " **Softmax**:for multiclass classification <br>\n",
    "$$\n",
    " \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc03d1-2fce-4238-9471-3151acf45a68",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph3.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78f688-ec6a-45a1-9ba0-3582d873f863",
   "metadata": {},
   "source": [
    "*Graph3. SoftMax function (source: DSIR-111 presentation slide)*\n",
    "\n",
    "<div>\n",
    "<img src=\"images/graph4.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6baa387-4495-410b-ad72-1528567fb6ca",
   "metadata": {},
   "source": [
    "Graph4. SoftMaxfunction explained ([source](https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60))\n",
    "where, according to wikipedia, it applies the standard exponential function to each element $z_i$ of the input vector $z$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma(z)$ is 1(source: [Softmax function](https://en.wikipedia.org/wiki/Softmax_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57445b-c6ba-4786-9f84-da0fb8824f2d",
   "metadata": {},
   "source": [
    ">Activations in the hidden layer provide a transformation that allow the neural net\n",
    "to learn more complex relationships as calculations propagate through the\n",
    "network(Source: DSIR-111 Classnote)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60cb8e-5f95-4033-abea-fe07072becf9",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "**Loss** is a measure of performance of a model. The lower, the better. When learning, the model aims to get the lowest loss possible. We use `crossentropy` for multiclass classification([source](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f51043-9660-431b-ae2f-28975d9fe193",
   "metadata": {},
   "source": [
    "## Examples:\n",
    "a) For Binary Classification: <br>\n",
    "\n",
    "$$\n",
    "LOSS = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i * log(\\hat y_i) + (1-y_i) * log(1 - \\hat y_i)) \n",
    "$$\n",
    "\n",
    "b) For Multi-class Classification:\n",
    "\n",
    "$$\n",
    "LOSS = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i * log(\\hat y_i)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81627c19-d31f-4eb5-9dba-34f0212b8377",
   "metadata": {},
   "source": [
    "Source: [Most Common Loss Functions in Machine Learning](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffcd6f-3daa-4d5e-88b7-b9ffe280c426",
   "metadata": {},
   "source": [
    "### Getting back to the perceptron: \n",
    " **Note that, the input layer is given biase, `b`, by introducing an extra input node that always has a value 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96453d38-dd33-4918-a8fd-b72b8ed2ad5b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph5.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07324027-b400-47d0-8020-c1479650890b",
   "metadata": {},
   "source": [
    "*Graph drawn by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874208f-672d-49a4-a72c-edb07c751ecb",
   "metadata": {},
   "source": [
    "Simplifying our example, let's assume $X = [x_1, x_2]$ and $W = [3, -2]$.<br>\n",
    "Then our $\\hat y = \\sigma(b + X^TW)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8ea98-c658-477f-8bc7-b23204ca35c7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/eqn1.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dc828-8003-4364-9c45-2d4f0829c703",
   "metadata": {},
   "source": [
    "*Picture by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0b1be-f20b-49d1-bd35-48e40ef45873",
   "metadata": {},
   "source": [
    "* Now, let's draw the equation of the hyperplane ( a line in 2D):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98112-8710-443c-b23b-35403190019a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph6.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb7dab-4f0f-4041-8272-5c2e71681163",
   "metadata": {},
   "source": [
    "*Graph drawn by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7694f8b-9f43-482b-826b-122c1f1be3c1",
   "metadata": {},
   "source": [
    "* The hyperplane (labeled by broken-line) corresponds to the decesion line that the Neural network makes to classify a given input from the ($X_1, X_2$) plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38f7e5-69d5-4fb5-8c4f-30185b054711",
   "metadata": {},
   "source": [
    "Example: Assume we have an input value $X = [-1, 2]$, and substituting $x_1 = -1$ and $x_2 = 2$ into the previous equation, $\\sigma (1 + 3x_1 - 2x_2) = \\sigma(-6)$, assume our activation function is `sigmoid`, we have: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cfc28-0adf-4993-baed-6eb8b6a722a7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    " = \\frac{1}{1+e^6} = 0.0025\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c1737-13ce-42d0-ad79-88df13470120",
   "metadata": {},
   "source": [
    "Now, since $\\hat y = \\sigma (1 + 3x_1 - 2x_2) \\approx 0.0025 < 0.5$, the activation function, `Sigmoid`, assigns our $X$ to the left of our linear classifier (the broken-line in the $(x_1,x_2)$ plane above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d148a2-0f4e-4863-b526-85501980a7bc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph1.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83500418-2872-47e4-98de-841e4ab583fa",
   "metadata": {},
   "source": [
    "*Graph1. Sigmoid function (source from DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a969a-92a2-46b2-801a-d60d42a483ac",
   "metadata": {},
   "source": [
    "* We'll get back to the [code notebook](https://github.com/sthirpa/Data_Scince_Immersive-at-General-Assembly-/blob/Hirpa/CIFAR-10-SH.ipynb) for the implementation of this theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176043b-1129-485d-9ebd-4a1852c64ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

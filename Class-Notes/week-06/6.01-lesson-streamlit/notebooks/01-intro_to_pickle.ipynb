{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pickling\n",
    "\n",
    "---\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Learn what serialization and deserialization are\n",
    "- Learn what \"pickling\" is in Python\n",
    "- Review using `with` statements to safely handle file operations\n",
    "- Pickle and unpickle scikit-learn models in Python\n",
    "\n",
    "---\n",
    "\n",
    "## What is pickling?\n",
    "\n",
    "If you're talking about food, pickling is a method of preserving food for the future. If you're talking about Python, pickling is a method of preserving **objects** for the future, including functions and classes. Since sklearn models are instances of classes, they can be pickled.\n",
    "\n",
    "To pickle an object, it needs to be **serialized**. Serialization is when we transform an object into byte streams. (Byte streams are collections of bytes. One byte is made up of eight zeros or ones.) To unpickle an object so that it can be used in Python again, it needs to be **deserialized**.\n",
    "\n",
    "If you've ever saved your progress in a video game, you've already serialized data without knowing it. A save file is your serialized save state. When you load the save, you deserialize the data so you can resume the game right where you were before you quit.\n",
    "\n",
    "### Some warnings:\n",
    "\n",
    "Just like you can't open a [Pokemon: Red](https://en.wikipedia.org/wiki/Pok%C3%A9mon_Red_and_Blue) savefile in [Pokemon: Sun](https://en.wikipedia.org/wiki/Pok%C3%A9mon_Sun_and_Moon), you have to unpickle an object in the same version of Python that you pickled it in. \n",
    "\n",
    "**Pickle objects can contain malicious code**. Never unpickle an object you don't trust!\n",
    "\n",
    "## Why pickle?\n",
    "\n",
    "Pickling makes a lot of sense any time you have a model you want to work with that you don't want to refit. Today, we'll pickle a fitted pipeline so that we can import it into a Streamlit web app, but pickling is useful in many other situations as well.\n",
    "\n",
    "If you have a model that took twelve hours to fit, you might want to analyze its residuals, work with its coefficients, or make predictions off of it. But without saving it some fashion, you'd need to refit the model every time you restarted your notebook. Pickling the model allows you to load the fitted model _without_ needing to re-run the code where you fit it.\n",
    "\n",
    "Notes:\n",
    "- Pickling does **not** compress your model, meaning that some pickled models can end up being fairly large file sizes. Think of K-nearest neighbors, which requires every data point to be stored inside the model (though sklearn _does_ optimize the way the data is stored for speed and efficiency, models can still be large.) \n",
    "- Keras has its own [`save` method](https://www.tensorflow.org/guide/keras/save_and_serialize) on models. If you want to save a neural network fit in keras, use that instead of pickle.\n",
    "- Don't pickle data frames. Export them as csv files instead. Generally, if there's another way to save something, use it.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling a simple datatype\n",
    "\n",
    "Before we pickle a full model, let's demonstrate pickling on a simple list.\n",
    "\n",
    "Create a list called `my_vegetables` that contains some strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the pickled list to disk\n",
    "\n",
    "Let's review [this link](https://www.pythonforbeginners.com/files/with-statement-in-python) to go over why `with` is such a good tool for file operations.\n",
    "\n",
    "Let's use `with` to write the list to disk as a `.pkl` file. We'll need to use `open`, pass in a file name, and also tell Python we're **writing** to the file, and writing as **bytes**. The pickle method we'll use is called `dump`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'with statement'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the pickled list\n",
    "\n",
    "Let's use `with` to open the pickled file and save it as a new variable, `list_from_pickle`. Remember to tell Python that we're **reading** from the file, and that we're reading in **bytes**. The pickle method we'll use is called `load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pickle a fitted pipeline\n",
    "\n",
    "Let's start by building a model to determine whether someone writes more like [Edgar Allen Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) or [Jane Austen](https://en.wikipedia.org/wiki/Jane_Austen).\n",
    "\n",
    "Our end goal will be a fitted pipeline. But before we export our pipeline, we'll need to settle on a model.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model decision: count vectorizer vs TF-IDF vectorizer\n",
    "\n",
    "Recall that a count vectorizer converts documents into vector representations of word occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We'll keep stopwords for right now, but we would drop them by instantiating the CountVectorizer as\n",
    "\n",
    "```Python\n",
    "cv = CountVectorizer(min_df=2, stopwords='english')\n",
    "```\n",
    "and would do so if we thought the stopwords weren't going to be of much use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to perform EDA on word counts, it may be useful to add the original author's name as a column:\n",
    "\n",
    "**Note**: It might be a mistake to add this information as `cv_text_df['author']` or `cv_text_df['label']`. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_text_df contains the transformed X_train -- so y_train\n",
    "# has the labels we should pair the words with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "> **Reminder**: \"Document\" means \"one natural language observation.\" Here, it means \"one paragraph from either Jane Austen or Edgar Allen Poe.\" \"Corpus\" means \"the whole natural language dataset that we're using\" -- so here it means \"the collected works of Jane Austen and Edgar Allen Poe, as scraped from Gutenberg and split into paragraphs.\"\n",
    "\n",
    "An alternative to count vectorization is **TF-IDF vectorization**, where TF-IDF stands for \"term frequency-inverse document frequency.\" Instead of just counting the words in each document, TF-IDF _weights_ the words in each document.\n",
    "\n",
    "The general idea behind TF-IDF is that words used many times in a document should matter more, unless they're also used many times in very many documents across the corpus! The TF-IDF thus weights words by both the **term frequency**, which is the number of times the word is used in the document, and the **inverse document frequency**, which measures how important a term is across the corpus. To compute the TF-IDF of one word used in one document, we divide the term frequency by the inverse document frequency. We do this for each word in each document.\n",
    "\n",
    "The formula for the term frequency of a term is written as\n",
    "\n",
    "$$\n",
    "\\text{tf}(t, d) = t/n\n",
    "$$\n",
    "\n",
    "where $t$ is the number of times the term is used in the document, and $n$ is the total number of words in the document.\n",
    "\n",
    "The formula for inverse document frequency is written as\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of documents in the corpus, and $\\text{df}(t)$ is the number of documents in the corpus that contain the term $t$.\n",
    "\n",
    "The TF-IDF itself is then computed as\n",
    "\n",
    "$$\n",
    "\\text{tf}(t,d) \\times \\text{idf}(t)\n",
    "$$\n",
    "\n",
    "which is the product of the term frequency and the inverse document frequency.\n",
    "\n",
    "> **Note**: There are a few other ways to formulate a TF-IDF. Some other implementations may calculate the inverse document frequency differently. You can see other formulations [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition). The formula above is the most common implementation and is the implementation used by scikit-learn.\n",
    "\n",
    "### TF-IDF in scikit-learn\n",
    "\n",
    "The `TfidfVectorizer` functions very similarly to `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does not make sense to sum the terms of the TF-IDF representation of our data.\n",
    "\n",
    "We can still do some exploration of the TF-IDF scores! The TF-IDF vectorizer has a fitted attribute `.idf_` which stores the inverse document frequency for each word in the corpus. Here, we will construct a data frame of the words in the corpus, and pair them with their IDF scores. Which words have large IDF scores, and which words have small IDF scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing models\n",
    "\n",
    "We could use either the TF-IDF vectorizer or the count vectorizer, and one might work better than the other, so let's try both - and let's try both alongside logistic regression and multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "My best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winner winnner chicken dinner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Pickle a fitted pipeline\n",
    "\n",
    "\n",
    "### Export the fitted pipeline to `models` as `author_pipe.pkl`\n",
    "\n",
    "This time, let's export to the `models` folder in this repository.\n",
    "\n",
    "> Why bother? First, if you'll have lots of serialized objects, or if your serialized objects take up lots of disk space, you might not want to add and commit them to Github. Keeping them all in the same folder makes it easier to stay organized and not commit them. Second, it's also just a good way to organize a repository!\n",
    "\n",
    "Just like before, we'll use a `with` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try to un-pickle in the `02-read_a_pickle` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

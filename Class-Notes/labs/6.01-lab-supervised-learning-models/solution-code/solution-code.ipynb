{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../401ksubs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9275, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "- It would be helpful to have variables that predict financial stability, such as a) how much money is in one's savings account, b) how much money is in one's checking account, and c) what is one's credit score.\n",
    "- Owning a house versus renting a residence may be a good indicator of someone able and willing to save money. (The same goes for a car, too!)\n",
    "- Knowing whether or not the person has made investments into stocks, bonds, or Certificates of Deposits (CDs) would be helpful to predict eligibility for 401(k)s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "Putting race into a model is often an unethical thing to do. In this particular scenario, using race to predict whether or not someone is eligible for a 401k is using race to target who should qualify for a particular product or service. It is unacceptable to discriminate on the basis of race here, even if race by itself wouldn't immediately disqualify somebody from being targeted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** Trivially, we wouldn't use income to predict income - because we wouldn't need to predict if we always possessed the data we wanted to predict! Similarly, if we had access to `incsq`, then we wouldn't need to predict `inc`. I would not use either of those feataures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** `incsq` and `agesq` appear to already have been feature engineered. Assuming that this was done by subject-matter experts, it's likely that they found there's some quadratic (squared) relationship between income and eligibility for a 401k. (The same goes for age and eligibility for a 401k.) Perhaps people who are older or have higher incomes are exponentially more likely to be eligible for an IRA, so these terms account for that nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** There appear to be two errors! `inc` is defined as `inc^2` and `age` is defined as `age^2`. While these are correct for `incsq` and `agesq`, `inc` should refer to one's income (not sure if it's household or individual) and `age` should refer to one's age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "    \n",
    "    - a multiple linear regression model (Yes, we can understand influence of features.)\n",
    "    - a $k$-nearest neighbors model (No, we cannot understand influence of features.)\n",
    "    - a decision tree (Yes, we can understand influence of features.)\n",
    "    - a set of bagged decision trees (Yes, we can understand influence of features.)\n",
    "    - a random forest (Yes, we can understand influence of features.)\n",
    "    - a set of extremely randomized trees (Yes, we can understand influence of features.)\n",
    "    - an Adaboost model (Yes, we can understand influence of features.)\n",
    "    - an XGBoost model (Yes, we can understand influence of features.)\n",
    "    - a support vector regressor (Yes, we can understand influence of features.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a $k$-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the case where I'm predicting `inc`, I'm excluding `e401k`, `p401k`, `pira`, `inc`, and `incsq` as predictors.\n",
    "- Given the small number of predictors (6) and the large sample size (9275), I'm comfortable training my model on 80% of my data. Testing on the remaining 20% still leaves nearly 2,000 observations to test on, which should be sufficient!\n",
    "- **Important**: Because at least one of my models is distance-based ($k$-nn) and in case we want to regularize any models, we should scale our features. (If we don't scale our features, then $k$-nn will likely perform poorly because nearest neighbors will be driven by variables with small scales rather than large scales.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"e401k\", \"p401k\", \"pira\", \"inc\", \"incsq\"])\n",
    "y = df[\"inc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, I'm going to use the defauls on all seven models. However, if we wanted to do so, we could GridSearch over parameters of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "cart_reg = DecisionTreeRegressor()\n",
    "cart_reg.fit(X_train, y_train)\n",
    "\n",
    "bagged_reg = BaggingRegressor()\n",
    "bagged_reg.fit(X_train, y_train)\n",
    "\n",
    "random_forest_reg = RandomForestRegressor()\n",
    "random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "adaboost_reg = AdaBoostRegressor()\n",
    "adaboost_reg.fit(X_train, y_train)\n",
    "\n",
    "support_vector_reg = SVR()\n",
    "support_vector_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** Bootstrapping is a method of sampling with replacement. We usually use it in order to simulate many different samples or to empirically estimate the sampling distribution of a statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** With a set of bagged decision trees, we have bootstrapped $k$ different samples and grown one decision tree on each bootstrapped sample, then our predictions are aggregated. With one decision tree, we only use the original sample and grow exactly one decision tree. No aggregation of predictions occurs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** In bagged decision trees, every variable is considered as a \"candidate\" for splitting at each node in the decision tree. In random forests, a random subset of variables is considered as candidataes for splitting at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** In a random forest, we randomly select which features go into each split. This effectively makes our individual decision trees less correlated, which decreases the variance of our predictions after aggregating the different decision trees. Thus, a random forest usually has less variance than bagged decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(model, X_train, X_test, y_train, y_test):\n",
    "    mse_train = mean_squared_error(y_true = y_train,\n",
    "                                  y_pred = model.predict(X_train))\n",
    "    mse_test = mean_squared_error(y_true = y_test,\n",
    "                                  y_pred = model.predict(X_test))\n",
    "    rmse_train = mse_train ** 0.5\n",
    "    rmse_test = mse_test ** 0.5\n",
    "    \n",
    "    print(\"The training RMSE for \" + str(model) + \" is: \" + str(rmse_train))\n",
    "    print(\"The testing RMSE for \" + str(model) + \" is: \" + str(rmse_test))\n",
    "    return (rmse_train, rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) is: 20.164244947447397\n",
      "The testing RMSE for LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) is: 20.89741661081878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20.164244947447397, 20.89741661081878)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(linear_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                    weights='uniform') is: 16.504194714767504\n",
      "The testing RMSE for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                    weights='uniform') is: 20.17751459453569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16.504194714767504, 20.17751459453569)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(knn_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best') is: 2.2638130048030134\n",
      "The testing RMSE for DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best') is: 27.291507864878007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.2638130048030134, 27.291507864878007)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(cart_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                 max_features=1.0, max_samples=1.0, n_estimators=10,\n",
      "                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                 warm_start=False) is: 8.836132228331097\n",
      "The testing RMSE for BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                 max_features=1.0, max_samples=1.0, n_estimators=10,\n",
      "                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                 warm_start=False) is: 20.971622330961786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.836132228331097, 20.971622330961786)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(bagged_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False) is: 7.727428163317682\n",
      "The testing RMSE for RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False) is: 20.328214017089596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.727428163317682, 20.328214017089596)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(random_forest_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "                  n_estimators=50, random_state=None) is: 23.058373419991902\n",
      "The testing RMSE for AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "                  n_estimators=50, random_state=None) is: 24.090942234844796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23.058373419991902, 24.090942234844796)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(adaboost_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) is: 19.791523022677538\n",
      "The testing RMSE for SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) is: 20.515564669076976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19.791523022677538, 20.515564669076976)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(support_vector_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Putting the training and testing RMSE in a table so we can directly compare them all:\n",
    "\n",
    "|           Model          | Training RMSE | Testing RMSE |\n",
    "|:------------------------:|:-------------:|:------------:|\n",
    "|     Linear Regression    |     0.837     |     0.868    |\n",
    "|    k-Nearest Neighbors   |     0.686     |     0.837    |\n",
    "|       Decision Tree      |     0.094     |     1.130    |\n",
    "|   Bagged Decision Trees  |     0.366     |     0.876    |\n",
    "|       Random Forest      |     0.376     |     0.879    |\n",
    "|         AdaBoost         |     0.878     |     0.924    |\n",
    "| Support Vector Regressor |     0.786     |     0.820    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** It appears that every model is overfit to the training data. We recognize this because our testing RMSE is worse (higher) than our training RMSE. Our models are generalizing poorly to held-out/unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "When I have a series of models from which I can pick, I usually do the following:\n",
    "1. List out all of the models.\n",
    "2. Any model that cannot solve my problem, I remove.\n",
    "    - In this case, $k$-nn is not appropriate because I can't easily identify which features affect income. (Remember that my original problem is \"What features best predict one's income?\") Even if it's a very predictive model, my goal isn't to come up with the best predictions.\n",
    "3. Among the models that _can_ solve my problems, I want to find the one that performs the best based on a metric of my choice.\n",
    "    - In this case, the models that seem to overfit the least (i.e. the gap between training and testing RMSE) are linear regression, AdaBoost, and the Support Vector Regressor.\n",
    "4. At this stage, it becomes a judgment call with no perfect guide to make the final decision.\n",
    "    - Do you have time to tune the models to try and eke out better performance?\n",
    "    - Is one model substantially better at solving the problem you wanted to solve?\n",
    "    - Do you need something understandable by a lay audience? (i.e. Linear regression is more common and more easily understood than AdaBoost or Support Vector Machines.)\n",
    "    \n",
    "Personally (if given time), I would try tuning the three remaining models to see if one performs substantially better than the other. If they all have roughly the same performance, I generally go with the simplest/easiest to understand model. In this case, I will go with the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "1. As mentioned above, I would tune my models using GridSearch.\n",
    "    - Using model defaults, then picking one model, and only tuning that is an example of a greedy approach. Instead, if we were to GridSearch all seven models, we might arrive at a model that is better than only having tuned the one model whose default was better than all other defaults.\n",
    "2. I would look at higher-order terms. For example, since we have `incsq` and `agesq` automatically created for us, there may be other higher-order terms that could be predictive. I would explore these to see if we can make better predictions.\n",
    "3. I would consider transforming my $Y$ variable. Income is often skewed, which means there will usually be a handful of very high incomes that might skew any linear model. I would consider transforming income (likely using $\\log$) so that income is \"un-skewed\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** We're predicting whether or not someone is eligible for a 401k. If we include whether or not someone already possesses a 401k, then every person with a 401k must by definition be eligible for a 401k. However, this probably doesn't contribute to our understanding of what makes someone eligible for a 401k and will only confound what we're actually interested in studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "    - a logistic regression model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a $k$-nearest neighbors model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a Naive Bayes model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a decision tree (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a set of bagged decision trees (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a random forest (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a set of extremely randomized trees (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - an Adaboost model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - an XGBoost model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "    - a support vector classifier (Yes, we can predict whether or not one is eligible for a 401(k).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a $k$-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"e401k\", \"p401k\"])\n",
    "y = df[\"e401k\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_class = LogisticRegression()\n",
    "logreg_class.fit(X_train, y_train)\n",
    "\n",
    "knn_class = KNeighborsClassifier()\n",
    "knn_class.fit(X_train, y_train)\n",
    "\n",
    "cart_class = DecisionTreeClassifier()\n",
    "cart_class.fit(X_train, y_train)\n",
    "\n",
    "bagged_class = BaggingClassifier()\n",
    "bagged_class.fit(X_train, y_train)\n",
    "\n",
    "random_forest_class = RandomForestClassifier()\n",
    "random_forest_class.fit(X_train, y_train)\n",
    "\n",
    "adaboost_class = AdaBoostClassifier()\n",
    "adaboost_class.fit(X_train, y_train)\n",
    "\n",
    "support_vector_class = SVC()\n",
    "support_vector_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- False positives are people we incorrectly predict to be eligible for a 401k.\n",
    "- False negatives are people we incorrectly predict to be ineligible for a 401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "If I'm spending marketing dollars on all the people I predict to be eligible for a 401k, I would probably want to make sure my number of false positives (those people I predict to be eligible who are ineligible) is as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** If I want to minimize false positives, then I want to minimize specificity.\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{N} = \\frac{TN}{TN + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "F_1 &=& \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}} \\\\\n",
    "&=& \\frac{2}{\\frac{1}{\\frac{TP}{TP + FP}} + \\frac{1}{\\frac{TP}{TP + FN}}} \\\\\n",
    "&=& \\frac{2}{\\frac{TP + FP}{TP} + \\frac{TP + FN}{TP}} \\\\\n",
    "&=& \\frac{2}{\\frac{TP + FP + TP + FN}{TP}} \\\\\n",
    "&=& \\frac{2}{2 + \\frac{FP + FN}{TP}}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "It balances our false positives and false negatives. As either false positives or false negatives increase, the denominator increases while the numerator stays fixed, meaning our $F_1$-score decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_scorer(model, X_train, X_test, y_train, y_test):\n",
    "    f1_train = f1_score(y_true = y_train,\n",
    "                        y_pred = model.predict(X_train))\n",
    "    f1_test = f1_score(y_true = y_test,\n",
    "                       y_pred = model.predict(X_test))\n",
    "    \n",
    "    print(\"The training F1-score for \" + str(model.__class__.__name__) + \" is: \" + str(f1_train))\n",
    "    print(\"The testing F1-score for \" + str(model.__class__.__name__) + \" is: \" + str(f1_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** At first, I threw an error because I defined a function called `f1_score()` and had accidentally overwritten the `f1_score` I imported from `sklearn.metrics`! Make sure to not define a function that overwrites what you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score for LogisticRegression is: 0.4727870199219552\n",
      "The testing F1-score for LogisticRegression is: 0.4773869346733668\n",
      "\n",
      "The training F1-score for KNeighborsClassifier is: 0.6514866390666164\n",
      "The testing F1-score for KNeighborsClassifier is: 0.49661399548532736\n",
      "\n",
      "The training F1-score for DecisionTreeClassifier is: 1.0\n",
      "The testing F1-score for DecisionTreeClassifier is: 0.4705882352941176\n",
      "\n",
      "The training F1-score for BaggingClassifier is: 0.9741168240643582\n",
      "The testing F1-score for BaggingClassifier is: 0.4786324786324786\n",
      "\n",
      "The training F1-score for RandomForestClassifier is: 1.0\n",
      "The testing F1-score for RandomForestClassifier is: 0.5436746987951807\n",
      "\n",
      "The training F1-score for AdaBoostClassifier is: 0.5621436716077537\n",
      "The testing F1-score for AdaBoostClassifier is: 0.5688487584650113\n",
      "\n",
      "The training F1-score for SVC is: 0.4707470747074707\n",
      "The testing F1-score for SVC is: 0.45347786811201446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_scorer(logreg_class, X_train, X_test, y_train, y_test)\n",
    "f1_scorer(knn_class, X_train, X_test, y_train, y_test)\n",
    "f1_scorer(cart_class, X_train, X_test, y_train, y_test)\n",
    "f1_scorer(bagged_class, X_train, X_test, y_train, y_test)\n",
    "f1_scorer(random_forest_class, X_train, X_test, y_train, y_test)\n",
    "f1_scorer(adaboost_class, X_train, X_test, y_train, y_test)\n",
    "f1_scorer(support_vector_class, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the training and testing `f1_score` in a table so we can directly compare them all:\n",
    "\n",
    "|           Model          | Training F1 | Testing F1 |\n",
    "|:------------------------:|:----------:|:---------:|\n",
    "|     Logistic Regression  |    0.473   |    0.477  |\n",
    "|    k-Nearest Neighbors   |    0.653   |    0.498  |\n",
    "|       Decision Tree      |    1.000   |    0.470  |\n",
    "|   Bagged Decision Trees  |    0.972   |    0.496  |\n",
    "|       Random Forest      |    0.969   |    0.498  |\n",
    "|         AdaBoost         |    0.562   |    0.567  |\n",
    "| Support Vector Classfier |    0.472   |    0.452  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** We want our $F_1$ score to be as high as possible. Thus, overfitting occurs when we have a high training $F_1$ score and a low testing $F_1$ score. Models that appear to be overfit are the $k$-nearest neighbors classifier, the decision tree, the bagged decision trees, the random forest, and (slightly) the support vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "Applying the same algorithm as I wrote above...\n",
    "1. List out all of the models.\n",
    "2. Any model that cannot solve my problem, I remove.\n",
    "    - In this case, all models can solve my problem. I want to maximize my ability to correctly predict whether or not someone is eligible for a 401(k).\n",
    "3. Among the models that _can_ solve my problems, I want to find the one that performs the best based on a metric of my choice.\n",
    "    - In this case, the models that seem to overfit the least (i.e. the gap between training and testing $F_1$) are logistic regression and AdaBoost.\n",
    "4. At this stage, it becomes a judgment call with no perfect guide to make the final decision.\n",
    "    - Do you have time to tune the models to try and eke out better performance?\n",
    "    - Is one model substantially better at solving the problem you wanted to solve?\n",
    "    - Do you need something understandable by a lay audience? (i.e. Logistic regression is more common and more easily understood than AdaBoost.)\n",
    "    \n",
    "Personally (if given time), I would try tuning the three remaining models to see if one performs substantially better than the other. If they all have roughly the same performance, I generally go with the simplest/easiest to understand model. In this case, I will go with the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** _(Answers may vary.)_\n",
    "\n",
    "1. I would tune the hyperparameters of my models using GridSearch. This is an answer I gave above that still applies to classification problems.\n",
    "    - Using model defaults, then picking one model, and only tuning that is an example of a greedy approach. Instead, if we were to GridSearch all of the models, we might arrive at a model that is better than only having tuned the one model whose default was better than all other defaults.\n",
    "2. I would look at higher-order terms. For example, since we have `incsq` and `agesq` automatically created for us, there may be other higher-order terms that could be predictive. I would explore these to see if we can make better predictions. This recommendation was included above but still applies for classification problems.\n",
    "3. I would consider building a separate model using only data where `p401k` = 1. This subset of data would allow me to see what features predict people who _do_ invest in a 401(k). I could use this model in conjunction with the eligibility model to predict people who are eligible and, given that they are eligible, are likely to invest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

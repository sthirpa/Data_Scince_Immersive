{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da84934-68da-40cf-b74a-85e14d867bc8",
   "metadata": {},
   "source": [
    "## Convolution Layers\n",
    "Now, suppose each feature is like a mini image; it's a patch. It's also a small 2D array of values and we'll use `filters` to pick up on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a791bf8-b4cd-4f33-be3d-6b4579a250db",
   "metadata": {},
   "source": [
    ">Convolution Layer:<br>\n",
    "The convolution layer is where we pass a filter over an image and do some calculation at each step. Specifically, we take pixels that are close to one another, then summarize them with one number. The goal of the convolution layer is to identify important features in our images, like edges.\n",
    "Source: [Here](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9f21e-de55-4d1c-ac2b-daefe289e972",
   "metadata": {},
   "source": [
    "Let's  use a $3X3$ `edge-detection` filter that amplifies the edges to the image below, then this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc3bf9-9e5a-4523-9dde-da6d2bbaf1bf",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 0\\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a5d68-157e-4de5-89cc-0b5ebd3811a6",
   "metadata": {},
   "source": [
    "`kernel` is convoluted with the input image, say $F(x,y)$, it creates a new convolved image (a feature map) that amplifies the edges (See `Fig.9` below). Zooming-in, we see `Fig.10` where a small piece of an image shows how the convolution operation is applied to get the new pixel value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560c5f9-5780-4ac6-ba1e-ad3390be39c5",
   "metadata": {},
   "source": [
    "![](images/cv7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb351676-79d2-424d-be4f-5ea199f682b9",
   "metadata": {},
   "source": [
    "![](images/cv8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2bb5a-345a-4488-a434-ff26d9b6d6c5",
   "metadata": {},
   "source": [
    "`Fig.conv: Applying Filter - source from (Mohammed 109-110)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b56ec-0882-4041-9a27-7e81140205c4",
   "metadata": {},
   "source": [
    ">Other filters can be applied to detect different types of features. For example, some filters detect `horizontal edges`, others detect `vertical edges`, still others detect more complex shapes like corners, and so on. The point is that these filters, when applied in the convolutional layers, yield feature-learning behavior: first they learn simple features like edges and straight lines, and later layers learn more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7e428-99dc-4f14-9626-038e95ee4084",
   "metadata": {},
   "source": [
    "Here are the three elements that enter into the convolution operation:\n",
    "\n",
    "* Input image\n",
    "* Feature detector or `kernel`, or `filter` used interchangeably \n",
    "* Feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5ac18-59c2-492a-bdee-c0643a48166a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv9.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d1783-3a5c-4e5e-b435-415ae178877e",
   "metadata": {},
   "source": [
    "Fig.15: [source](https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3bc501-cd6c-4d2a-8d16-f60c50782c3f",
   "metadata": {},
   "source": [
    "The example we gave above is a very simplified one, though. In reality, convolutional neural networks develop multiple feature detectors and use them to develop several feature maps which are referred to as convolutional layers.\n",
    "Through training, the network determines what features it finds important in order for it to be able to scan images and categorize them more accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187b545-2b8f-433e-a93f-6a1381ab61df",
   "metadata": {},
   "source": [
    "![](https://media3.giphy.com/media/i4NjAwytgIRDW/200.webp?cid=ecf05e471vftp51bx55s3lbh1el698xc1bv7l7rhy0igcpz3&rid=200.webp&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92825c-a06b-47a0-a265-3ceb5e49d3c6",
   "metadata": {},
   "source": [
    "### For a 3D array convolution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea78b0-fa3f-4932-85a0-6eb1f3d3aebc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/conv.gif\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a201c-895e-49d4-87b6-8cd5bfcc9acc",
   "metadata": {},
   "source": [
    "Source for the two `gif` images: [A Comprehensive Guide to Convolutional Neural Networks â€” the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948cc45-462d-48ee-b397-10a828bb16ce",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"img/cnn15.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb638a-d27a-465b-ae02-208dacb1386e",
   "metadata": {},
   "source": [
    "Fig.16: [Source](https://pylessons.com/Logistic-Regression-part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fcae3-88a9-4e15-9f4a-50aca72bb2b4",
   "metadata": {},
   "source": [
    "- Putting all together:<br>\n",
    "The term **`convolution`** refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f957d0-aa88-433d-b94c-057f624ef6be",
   "metadata": {},
   "source": [
    ">We perform a series `convolution + pooling operations, followed by a number of fully connected layers`. If we are performing multiclass classification the output is softmax.[fig.*source*](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2) <br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0b8ef-4619-45ed-8d00-53961983d96d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/architecture.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a5968-2574-40e2-8944-d99969edd395",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/CNN_architecture.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679371a9-6000-495e-b6a1-12e9b896ffaf",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/CNN_from_Scratch.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476a8c9-796b-4920-a56a-ee8533436a81",
   "metadata": {},
   "source": [
    "*Fig.17: CNN Architecture ([Source](https://www.mathworks.com/videos/introduction-to-deep-learning-what-are-convolutional-neural-networks--1489512765771.html)).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21de92-3902-455b-9b9b-fb63a168fc29",
   "metadata": {},
   "source": [
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6202edf-ea04-4841-8235-b43a7ed0bd9b",
   "metadata": {},
   "source": [
    ">It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 .\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97308b8b-8fbd-494a-aea5-0b72aa7335b5",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv14.png\" width=\"500\"/> \n",
    "<div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7048de-fc7a-49cc-9aa8-e30009aaf9bd",
   "metadata": {},
   "source": [
    "Fig.18 [Source](https://cs231n.github.io/convolutional-networks/#conv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d4951-a9d1-4308-a367-ed36191fb962",
   "metadata": {},
   "source": [
    ">Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square)(Source:[Convolutional Neural Networks for Visual recognition](https://cs231n.github.io/convolutional-networks/#conv/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a969a-92a2-46b2-801a-d60d42a483ac",
   "metadata": {},
   "source": [
    "* We'll get back to the [code notebook](https://github.com/sthirpa/Data_Scince_Immersive-at-General-Assembly-/blob/Hirpa/CIFAR-10-SH.ipynb) for the implementation of this theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176043b-1129-485d-9ebd-4a1852c64ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

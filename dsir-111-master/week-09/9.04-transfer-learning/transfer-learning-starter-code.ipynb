{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "_Author:_ Tim Book\n",
    "\n",
    "### Learning Objectives\n",
    "*After this lesson, students will be able to:*\n",
    "\n",
    "1. Define transfer learning\n",
    "1. Carry out transfer learning with and without pipelines and gridsearching\n",
    "1. Identify scenarios in which transfer learning can benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Transfer Learning from PCA\n",
    "#### aka, _principal components regression (PCR)_\n",
    "Often we have many columns, and too few rows. We've seen that PCA can extract only the pertinent information out of a data frame. Why don't we use that in a regression? This is the first and most common variety of transfer learning. It is also the most common thing to do with principal components beyond EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santander bank data. All columns are censored - we have no idea what we're\n",
    "# looking at! The target is whether or not a transaction occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate into X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is our class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression - overfit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a pipeline that feeds PCA results into a logistic regression.\n",
    "# In order to do this, we need to:\n",
    "# Step 1: Scale data\n",
    "# Step 2: Decompose data into PCs\n",
    "# Step 3: Perform logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a gridsearcher and fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about the test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(THREAD)**: Why did I do a train-test-split AND cross-validation here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tuning parameter curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Transfer Learning with Clusters\n",
    "Oftentimes, we'd actually like to _add_ dimensionality to our data to give our model more information. In this example, we will use clustering to use lattitude/longitude data effectively.\n",
    "\n",
    "It's not shown in this example, but using clustering with transfer learning is more often used as dimensionality reduction similar to PCA. Use clusters as an x-variable to replace several other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data \"cleaning\"\n",
    "# NOTE: This shouldn't be considered best, or even good, practice.\n",
    "# This is merely to get the data into a workable shape so we don't\n",
    "# spend our lesson cleaning missing and categorical columns.\n",
    "mel = pd.read_csv(\"data/melbourne.csv\")\n",
    "keepvars = [\n",
    "    \"Price\", \"Rooms\", \"Bedroom2\", \"Bathroom\",\n",
    "    \"Car\", \"Landsize\", \"Lattitude\", \"Longtitude\"\n",
    "]\n",
    "mel = mel[keepvars].dropna()\n",
    "mel.columns = [\"price\", \"rooms\", \"bed\", \"bath\", \"car\", \"land\", \"lat\", \"long\"]\n",
    "mel = mel.loc[mel[\"price\"] < np.quantile(mel[\"price\"], 0.99), :]\n",
    "mel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the highly prices houses?\n",
    "mel.plot(kind=\"scatter\", x=\"long\", y=\"lat\", c=\"price\",\n",
    "         cmap=\"RdYlGn\", figsize=(14, 10), s=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of house prices - skew?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check histogram of log-price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's winnow our data down to only these quantitative variables.\n",
    "\n",
    "# House prices are skew - so let's do log regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.copy(), y.copy(), random_state=42, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carry out a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How'd it do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The regular log-model performed kinda badly...\n",
    "But as we saw in our map, home prices are not distributed uniformly about Melbourne. What if we clustered by lat/long, and used those clusters in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scoop lat/long up in a matrix so we can use them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cluster our observations by lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do these clusters look like visually?\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(mel.long, mel.lat, c=km.labels_, s=1, cmap=\"tab20\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neat! Now let's append these clusters back onto X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.copy(), y.copy(), random_state=42, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How'd we do now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1: How do we tune $k$?\n",
    "Two choices:\n",
    "1. Bottle all the above into a function and iterate, finding $k$ that gives the best testing error.\n",
    "2. Force this into a gridsearchable class.\n",
    "    * We can't use `GridSearchCV` now because clusterers aren't _transformers_.\n",
    "    * In order to fix this, we'll need some OOP skills outside the scope of today's lesson. In short, you can subclass a scikit-learn mixin and create your own class that acts like a scikit-learn transformer. I wrote a blog post about how to do this [here](https://towardsdatascience.com/building-a-custom-model-in-scikit-learn-b0da965a1299)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this wrapped up!\n",
    "def transfer_tune(X, y, k):\n",
    "    location_data = mel[[\"long\", \"lat\"]]\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(location_data)\n",
    "    X.loc[:, \"cluster\"] = km.predict(location_data)\n",
    "    X_dummy = pd.get_dummies(columns=[\"cluster\"], data=X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dummy.copy(), y.copy(), random_state=42, test_size=0.5\n",
    "    )\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(f\"{k} : {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2, 103, 5):\n",
    "    transfer_tune(X, y, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2: The Train-Test-Split Dilemma\n",
    "If you have many clusters, it is possible that your test data will have some clusters not represented. Using `pd.get_dummies()` won't help you here - it won't make columns for unrepresented categories.\n",
    "\n",
    "**Example:**\n",
    "Suppose you do 10-means clustering on your training data. Your training data now has labels 1 through 10. There's no guarantee that every cluster would be represented in your test data. Maybe no testing data points are put into cluster 5. When you use `pd.get_dummies()` on your testing data, it won't make a `cluster_5` column, and scikit-learn will complain about dimension mismatches (since you have one fewer column in `X_test` now).\n",
    "\n",
    "In a production setting, you might prefer to use the `OneHotEncoder` scikit-learn class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = pd.DataFrame({\"cluster\": ['A', 'B', 'C', 'D', 'E']})\n",
    "D_test = pd.DataFrame({\"cluster\": ['A', 'B', 'C', 'E']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(columns=[\"cluster\"], data=D_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(columns=[\"cluster\"], data=D_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh = OneHotEncoder(categories=\"auto\", sparse=False)\n",
    "oh.fit(km.labels_.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_matrix = oh.transform(km.labels_.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "ohe.fit(D_train[['cluster']])\n",
    "ohe.transform(D_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(D_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

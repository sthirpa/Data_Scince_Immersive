{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733eec06-6973-4d3e-8eb0-745e9eea455e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19563ad0-e64b-48a2-9618-d867b27e5e23",
   "metadata": {},
   "source": [
    "Summary\n",
    "Goals:\n",
    "Using Reddit or Pushshift.io API, collect posts from two subreddits of my choosing (r/cats and r/dogs).\n",
    "Use NLP to train a classifier model on which subreddit a given post came from.\n",
    "Stretch goal: Use sentiment analysis to solve the age-old debate of cats vs dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59667636-7db8-40dc-b8f1-aa23d6e5c5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cae3ca2-6e80-498a-992f-b998c69c6a90",
   "metadata": {},
   "source": [
    "### What is Reddit?\n",
    "\n",
    "Founded by University of Virginia roommates **Steve Huffman** and **Alexis Ohanian**, with **Aaron Swartz** - in 2005, **Reddit** is a website comprising user-generated content—including photos, videos, links, and text-based posts—and discussions of this content in what is essentially a bulletin board system. The name \"Reddit\" is a play-on-words with the phrase \"read it\", i.e., \"I read it on Reddit.\" According to Reddit, in 2019, there were approximately 430 million monthly users, who are known as \"redditors\". The site's content is divided into categories or communities known on-site as \"subreddits\", of which there are more than 138,000 active communities.\n",
    "\n",
    "As a network of communities, Reddit's core content consists of posts from its users. Users can comment on others' posts to continue the conversation. A key feature to Reddit is that users can cast positive or negative votes, called upvotes and downvotes respectively, for each post and comment on the site. The number of upvotes or downvotes determines the posts' visibility on the site, so the most popular content is displayed to the most people. Users can also earn \"karma\" for their posts and comments, a status that reflects their standing within the community and their contributions to Reddit. Posts are automatically archived after six months, meaning they can no longer be commented or voted on.\n",
    "\n",
    "### Subreddits\n",
    "\n",
    "Subreddits are user-created areas of interest where discussions on Reddit are organized. There are about 138,000 active subreddits (among a total of 1.2 million) as of July 2018. Subreddit names begin with \"r/\"; for instance, \"r/science\" is a community devoted to discussing scientific topics, while \"r/television\" is a community devoted to discussing TV shows and \"r/Islam\", a community dedicated for Islam oriented topics([source](https://en.wikipedia.org/wiki/Reddit). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1326a0-b409-4c35-8d25-3e6e49c8f930",
   "metadata": {},
   "source": [
    "## Problem Statements\n",
    "\n",
    "\n",
    "\n",
    "`What characteristics of a post on Reddit contribute most to what subreddit it belongs to?\n",
    "\n",
    "Predict what Posts/comments belong to which subreddit By using NLP and Classification models\n",
    "`\n",
    "\n",
    "* Reddit posts are extremely large, it is near to impossible to sort all the posts manually.\n",
    "* How to utilise machine learning ability(NLP) to organise the posts into a logical acceptable way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a13c6d-e91f-4522-874c-c30c37d07e11",
   "metadata": {},
   "source": [
    "## Three parts are:\n",
    "\n",
    "### Part I: Data wrangling/gathering/acquisition\n",
    "### Part II: Natural Language Processing\n",
    "### Part III:Classification Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b00a8-aea3-4e3a-9cf7-2c4468df1591",
   "metadata": {},
   "source": [
    "In this section we will carry out:\n",
    "\n",
    "* API Scrapping\n",
    "* Data Cleaning\n",
    "* Modeling\n",
    "* Model Evaluation\n",
    "* Conclusion and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1f35eb-4c5d-453b-b803-00aa8522268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "import time \n",
    "import requests\n",
    "import json\n",
    "np.random.seed (42)\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfaf2ac-1ff1-49df-9bf4-891c602d4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "# from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "# from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import confusion_matrix as cm\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.externals.six import StringIO \n",
    "# from IPython.display import Image  \n",
    "# from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205b727-71ce-4b6a-aa2c-9403375f2001",
   "metadata": {},
   "source": [
    "### API Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94091cc-3963-401a-a055-61596dd54b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the reddit title that we are intrested in\n",
    "url_1 = 'https://www.reddit.com/r/marvel.json'  \n",
    "url_2 = 'https://www.reddit.com/r/dccomics.json'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6095dc6f-2cd2-4009-806a-4adf42501b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to capture posts\n",
    "def get_post(url, csv_name):\n",
    "\n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    for a in range(35):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'Data Inc'})\n",
    "    \n",
    "#if unable to access reddit, print the error and 'after' so we know where should we continue from\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            print(after)\n",
    "            break\n",
    "    \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "    \n",
    "        if a > 0:\n",
    "#read the previous posts, concat it with the current posts and save \n",
    "            prev_posts = pd.read_csv(csv_name)\n",
    "            current_df = pd.DataFrame(current_posts) \n",
    "            current_df = pd.concat([prev_posts, current_df])\n",
    "            pd.DataFrame(current_df).to_csv(csv_name, index = False)\n",
    "        \n",
    "        else:\n",
    "            pd.DataFrame(posts).to_csv(csv_name, index = False)\n",
    "\n",
    "# generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,10)\n",
    "        time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8cf393-7267-4c96-b292-0b54be41509a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

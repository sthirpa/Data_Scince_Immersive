{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Web Scraping and Spiders With Scrapy\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Sam Stack(DC)_\n",
    "\n",
    "_Modified for DSI-EAST-2 by Justin Pounders_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Decipher the structure and content of HTML\n",
    "- Use Beautiful Soup to parse HTML\n",
    "- Use XPath to select HTML elements\n",
    "- Practice using Scrapy to get data from Craigslist\n",
    "- Walk through the construction of a spider built using Scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "![What is HTML?](http://designshack.designshack.netdna-cdn.com/wp-content/uploads/htmlbasics-0.jpg)\n",
    "\n",
    "One of the largest sources of data in the world is all around us — the web. Most people consume the web in some form every day. One of the most powerful Python tool sets we'll learn allows us to extract and normalize data from unstructured sources such as web pages.  \n",
    "\n",
    "**If you can see it, it can be scraped, mined, and put into a DataFrame.**\n",
    "\n",
    "Before we begin the actual process of web scraping with Python, it's important to cover the basic constructs that describe HTML as unstructured data. \n",
    "\n",
    "We'll then cover a powerful selection technique called XPath and look at a basic workflow using a framework called [Scrapy](http://www.scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html'></a>\n",
    "\n",
    "## Hypertext Markup Language (HTML)\n",
    "\n",
    "---\n",
    "\n",
    "In the HTML document object model (DOM), everything is a node:\n",
    " * The document itself is a document node.\n",
    " * All HTML elements are element nodes.\n",
    " * All HTML attributes are attribute nodes.\n",
    " * Text inside HTML elements are text nodes.\n",
    " * Comments are comment nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elements'></a>\n",
    "### Elements\n",
    "Elements begin and end with opening and closing \"tags,\" which are defined by namespaced, encapsulated strings. These namespaces, which begin and end the elements, must be the same. \n",
    "\n",
    "```html\n",
    "<title>I am a title.</title>\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "\n",
    "When you have several different titles or paragraphs on a single page, you can assign ID values to namespaces to make more unique reference points. IDs are also useful for labelling nested elements.\n",
    "```html\n",
    "<title id ='title_1'>I am the first title.</title>\n",
    "<p id ='para_1'>I am the first paragraph.</p>\n",
    "<title id ='title_2'>I am the second title.</title>\n",
    "<p id ='para_2'>I am the second paragraph.</p>\n",
    "```\n",
    "\n",
    "\n",
    "**Elements can have parents and children.**\n",
    "It's important to remember that an element can be both a parent and a child — whether an element is referred to as a parent or child depends on the specific element you're referencing.\n",
    "\n",
    "\n",
    "```html\n",
    "<body id = 'parent'>\n",
    "    <div id = 'child_1'>I am the child of 'parent.'\n",
    "        <div id = 'child_2'>I am the child of 'child_1.'\n",
    "            <div id = 'child_3'>I am the child of 'child_2.'\n",
    "                <div id = 'child_4'>I am the child of 'child_4.'</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "**or**\n",
    "```html\n",
    "<body id = 'parent'>\n",
    "    <div id = 'child_1'>I am the parent of 'child_2.'\n",
    "        <div id = 'child_2'>I am the parent of 'child_3.'\n",
    "            <div id = 'child_3'> I am the parent of 'child_4.'\n",
    "                <div id = 'child_4'>I am not a parent. </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>\n",
    "### Attributes\n",
    "\n",
    "HTML elements can also have attributes. They describe the properties and characteristics of elements. Some affect how the element behaves or looks in terms of the output rendered by the browser.\n",
    "\n",
    "The most common element is an anchor element. Anchor elements often have an \"href\" element, which tells the browser where to go after it's clicked. An anchor element is typically formatted in bold type and is sometimes underlined as a visual cue to differentiate it.\n",
    "\n",
    "**The markup that describes an element with attributes literally looks like this:**\n",
    "\n",
    "```html\n",
    "<a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">An Awesome Website</a>\n",
    "```\n",
    "\n",
    "**However, this element, once rendered, looks like this:**\n",
    "\n",
    "[An Awesome Website](https://www.youtube.com/watch?v=dQw4w9WgXcQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='element-hierarchy'></a>\n",
    "### Element Hierarchy\n",
    "\n",
    "![Nodes](http://www.computerhope.com/jargon/d/dom1.jpg)\n",
    "\n",
    "**Literally represented as:**\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    \n",
    "    <head>\n",
    "        <title>Example</title>\n",
    "    </head>\n",
    "    \n",
    "    <body>\n",
    "        <h1>Example Page</h1>\n",
    "        <p>This is an example page.</p>\n",
    "    </body>\n",
    "    \n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html-resources'></a>\n",
    "### Additional HTML Resources\n",
    "\n",
    "Read all about the different elements supported by modern browsers:\n",
    " * [HTML5 cheat sheet](http://websitesetup.org/html5-cheat-sheet/).\n",
    " * [Mozilla HTML element reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element).\n",
    " * [HTML5 visual cheat sheet](http://www.unitedleather.biz/PDF/HTML5-Visual-Cheat-Sheet1.pdf).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a>\n",
    "\n",
    "## Using Requests and Beautiful Soup to Extract Information From a Web Page\n",
    "\n",
    "---\n",
    "\n",
    "Beautiful Soup is a Python library that's useful for pulling data out of HTML and XML files. It works with many parsers, such as XPath, and can be executed in an IDE, meaning it can be easier to work with when first extracting information from HTML.\n",
    "\n",
    "Please make sure that the required packages are installed: \n",
    "\n",
    "```bash\n",
    "# Beautiful Soup:\n",
    "> conda install beautifulsoup4\n",
    "> conda install lxml\n",
    "\n",
    "# Or if conda doesn't work:\n",
    "> pip install beautifulsoup4\n",
    "> pip install lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"../sample.html\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "<title>Hello, World!</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>Header 1</h1>\n",
       "<h2>Header 2</h2>\n",
       "<p>This is a paragraph</p>\n",
       "<a href=\"https://www.google.com/\">Google it!</a>\n",
       "<h3>What's in a div?</h3>\n",
       "<div class=\"divvy-it-up\" id=\"foobar\">\n",
       "<p id=\"layer1\">I'm in a div.  Yeah!</p>\n",
       "<div>\n",
       "<p id=\"layer2\">I'm in a div, too!</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"todo\">\n",
       "<ul>\n",
       "<li> Take out trash</li>\n",
       "<li> Walk dog</li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"something\">\n",
       "<ol>\n",
       "<li>One</li>\n",
       "<li>Two</li>\n",
       "</ol>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Hello, World!</title>\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n"
     ]
    }
   ],
   "source": [
    "olist = soup.find('ol')\n",
    "for list_item in olist.find_all('li'):\n",
    "    print(list_item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"layer2\">I'm in a div, too!</p>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_results = soup.body.find_all('div', {'class':'divvy-it-up'})\n",
    "div_results[0].find_all('p', {'id':'layer2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's buy a car!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](assets/craigslist.jpg)\n",
    "\n",
    "https://atlanta.craigslist.org/atl/wan/d/cheap-or-free-running-car-or/6485015376.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Try this!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempted Fix for 1) Fetch the content by URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# You'll need the requests library in order to fully utilize bs4.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Target web page:\n",
    "url = \"https://www.datatau.com/\"\n",
    "\n",
    "# Establishing the connection to the web page:\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "# Ex., 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found.\n",
    "print(response.status_code)\n",
    "\n",
    "# Pull the HTML string out of requests and convert it to a Python string.\n",
    "html = response.text\n",
    "\n",
    "# The first 700 characters of the content.\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><link rel=\"stylesheet\" type=\"text/css\" href=\"news.css\">\n",
      "<link rel=\"shortcut icon\" href=\"https://www.iconj.com/ico/d/x/dxo02ap56v.ico\">\n",
      "<script>\n",
      "function byId(id) {\n",
      "  return document.getElementById(id);\n",
      "}\n",
      "\n",
      "function vote(node) {\n",
      "  var v = node.id.split(/_/);   // {'up', '123'}\n",
      "  var item = v[1]; \n",
      "\n",
      "  // adjust score\n",
      "  var score = byId('score_' + item);\n",
      "  var newscore = parseInt(score.innerHTML) + (v[0] == 'up' ? 1 : -1);\n",
      "  score.innerHTML = newscore + (newscore == 1 ? ' point' : ' points');\n",
      "\n",
      "  // hide arrows\n",
      "  byId('up_'   + item).style.visibility = 'hidden';\n",
      "  byId('down_' + item).style.visibility = 'hidden';\n",
      "\n",
      "  // ping server\n",
      "  var ping = new Image();\n",
      "  ping.src = node.href;\n",
      "\n",
      "  return false; // cancel browser nav\n",
      "} </script><script>\n",
      "\n",
      "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
      "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
      "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
      "  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n",
      "\n",
      "  ga('create', 'UA-46326769-1', 'datatau.com');\n",
      "  ga('send', 'pageview');\n",
      "\n",
      "</script><title>DataTau</title></head><body><center><table border=0 cellpadding=0 cellspacing=0 width=\"85%\" bgcolor=#f6f6ef><tr><td bgcolor=#00b4b4><table border=0 cellpadding=0 cellspacing=0 width=\"100%\" style=\"padding:2px\"><tr><td style=\"width:18px;padding-right:4px\"><a href=\"https://www.datatau.com\"><img src=\"arc.png\" width=18 height=18 style=\"border:1px #b4b400 solid;\"></img></a></td><td style=\"line-height:12pt; height:10px;\"><span class=\"pagetop\"><b><a href=\"news\">DataTau</a></b><img src=\"s.gif\" height=1 width=10><a href=\"newest\">new</a> | <a href=\"newcomments\">comments</a> | <a href=\"leaders\">leaders</a> | <a href=\"submit\">submit</a></span></td><td style=\"text-align:right;padding-right:4px;\"><span class=\"pagetop\"><a href=\"/x?fnid=DrpY8p5sR1\">login</a></span></td></tr></table></td></tr><tr style=\"height:10px\"></tr><tr><td><table border=0 cellpadding=0 cellspacing=0><tr><td align=right valign=top class=\"title\">1.</td><td><center><a id=nil href=\"vote?for=30726&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30726></span></center></td><td class=\"title\"><a href=\"https://www.narrator.ai/blog/using-postgresql-as-a-data-warehouse/\" rel=\"nofollow\">Using PostgreSQL as a Data Warehouse</a><span class=\"comhead\"> (narrator.ai) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30726>2 points</span> by <a href=\"user?id=ceddus\">ceddus</a> 13 days ago  | <a href=\"item?id=30726\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">2.</td><td><center><a id=nil href=\"vote?for=30676&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30676></span></center></td><td class=\"title\"><a href=\"https://gradientflow.com/2021aihealthsurvey/?utm_source=datatau&utm_medium=organic\" rel=\"nofollow\">FREE Report: AI in Healthcare Survey Results</a><span class=\"comhead\"> (gradientflow.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30676>3 points</span> by <a href=\"user?id=thedataexchange\">thedataexchange</a> 53 days ago  | <a href=\"item?id=30676\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">3.</td><td><center><a id=nil href=\"vote?for=30669&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30669></span></center></td><td class=\"title\"><a href=\"https://github.com/lightly-ai/lightly\" rel=\"nofollow\">Open-source framework for self-supervised learning</a><span class=\"comhead\"> (github.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30669>2 points</span> by <a href=\"user?id=RareGradient\">RareGradient</a> 59 days ago  | <a href=\"item?id=30669\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">4.</td><td><center><a id=nil href=\"vote?for=30590&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30590></span></center></td><td class=\"title\"><a href=\"https://blog.narrator.ai/how-to-calculate-ltv/\" rel=\"nofollow\">The best way to calculate Customer Lifetime Value for Ecommerce</a><span class=\"comhead\"> (narrator.ai) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30590>4 points</span> by <a href=\"user?id=ceddus\">ceddus</a> 109 days ago  | <a href=\"item?id=30590\">2 comments</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">5.</td><td><center><a id=nil href=\"vote?for=30651&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30651></span></center></td><td class=\"title\"><a href=\"https://www.narrator.ai/blog/generating-random-timestamps-in-sql/\" rel=\"nofollow\">Generating realistic user timestamps in SQL</a><span class=\"comhead\"> (narrator.ai) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30651>2 points</span> by <a href=\"user?id=ceddus\">ceddus</a> 67 days ago  | <a href=\"item?id=30651\">1 comment</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">6.</td><td><center><a id=nil href=\"vote?for=30648&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30648></span></center></td><td class=\"title\"><a href=\"https://www.neuraxle.org/\" rel=\"nofollow\">Neuraxle - Build Neat Machine Learning Pipelines</a><span class=\"comhead\"> (neuraxle.org) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30648>2 points</span> by <a href=\"user?id=Neuraxio\">Neuraxio</a> 72 days ago  | <a href=\"item?id=30648\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">7.</td><td><center><a id=nil href=\"vote?for=30578&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30578></span></center></td><td class=\"title\"><a href=\"https://datamahadev.com/10-amazing-python-hacks-with-cool-libraries/\" rel=\"nofollow\">Amazing Python hacks every programmer must know</a><span class=\"comhead\"> (datamahadev.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30578>4 points</span> by <a href=\"user?id=samiksha99\">samiksha99</a> 119 days ago  | <a href=\"item?id=30578\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">8.</td><td><center><a id=nil href=\"vote?for=30646&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30646></span></center></td><td class=\"title\"><a href=\"https://wlook.in/product/women-multi-coloured-printed-top-3/\" rel=\"nofollow\">Women Multi-Coloured Printed Top</a><span class=\"comhead\"> (wlook.in) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30646>2 points</span> by <a href=\"user?id=wlookfashion\">wlookfashion</a> 73 days ago  | <a href=\"item?id=30646\">1 comment</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">9.</td><td><center><a id=nil href=\"vote?for=30638&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30638></span></center></td><td class=\"title\"><a href=\"https://www.ahpmedicals.com/disposables/gloves/nitrile.html\" rel=\"nofollow\">Needles and syringes uk</a><span class=\"comhead\"> (ahpmedicals.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30638>2 points</span> by <a href=\"user?id=ahpmedicals\">ahpmedicals</a> 75 days ago  | <a href=\"item?id=30638\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">10.</td><td><center><a id=nil href=\"vote?for=30567&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30567></span></center></td><td class=\"title\"><a href=\"https://blog.narrator.ai/one-question-to-make-your-data-project-10x-more-valuable/\" rel=\"nofollow\">One question to make your data project 10x more valuable</a><span class=\"comhead\"> (narrator.ai) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30567>4 points</span> by <a href=\"user?id=ceddus\">ceddus</a> 128 days ago  | <a href=\"item?id=30567\">1 comment</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">11.</td><td><center><a id=nil href=\"vote?for=30616&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30616></span></center></td><td class=\"title\"><a href=\"https://neurosys.com/article/adversarial-attacks-for-fooling-deep-neural-networks/\" rel=\"nofollow\">Adversarial Attacks For Fooling Deep Neural Networks</a><span class=\"comhead\"> (neurosys.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30616>2 points</span> by <a href=\"user?id=Neurosys\">Neurosys</a> 88 days ago  | <a href=\"item?id=30616\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">12.</td><td><center><a id=nil href=\"vote?for=30520&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30520></span></center></td><td class=\"title\"><a href=\"https://medium.com/analytics-and-data/the-path-to-learning-sql-and-mastering-it-to-become-a-data-engineer-256ea0fef4e7\" rel=\"nofollow\">The path to learning SQL and mastering it to become a Data Engineer</a><span class=\"comhead\"> (medium.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30520>5 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 176 days ago  | <a href=\"item?id=30520\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">13.</td><td><center><a id=nil href=\"vote?for=30486&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30486></span></center></td><td class=\"title\"><a href=\"https://medium.com/analytics-and-data/5-tips-for-aspiring-and-junior-data-engineers-8b47ef154367\" rel=\"nofollow\">5 tips for aspiring and junior data engineers</a><span class=\"comhead\"> (medium.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30486>5 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 201 days ago  | <a href=\"item?id=30486\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">14.</td><td><center><a id=nil href=\"vote?for=30502&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30502></span></center></td><td class=\"title\"><a href=\"https://medium.com/analytics-and-data/data-science-career-path-progression-ab5140cfbc84\" rel=\"nofollow\">Data Science Career Path & Progression</a><span class=\"comhead\"> (medium.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30502>4 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 189 days ago  | <a href=\"item?id=30502\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">15.</td><td><center><a id=nil href=\"vote?for=30459&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30459></span></center></td><td class=\"title\"><a href=\"https://www.alexfranz.com/posts/personal-python-data-science-toolkit-part-1/\" rel=\"nofollow\">The Personal Python Data Science Toolkit</a><span class=\"comhead\"> (alexfranz.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30459>5 points</span> by <a href=\"user?id=frnz\">frnz</a> 229 days ago  | <a href=\"item?id=30459\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">16.</td><td><center><a id=nil href=\"vote?for=30515&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30515></span></center></td><td class=\"title\"><a href=\"https://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/\" rel=\"nofollow\">Introducing Amazon Managed Workflows for Apache Airflow</a><span class=\"comhead\"> (amazon.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30515>3 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 179 days ago  | <a href=\"item?id=30515\">1 comment</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">17.</td><td><center><a id=nil href=\"vote?for=30512&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30512></span></center></td><td class=\"title\"><a href=\"https://machinelearning.apple.com/updates/ml-compute-training-on-mac\" rel=\"nofollow\">Leveraging ML Compute for Accelerated Training on Mac</a><span class=\"comhead\"> (apple.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30512>3 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 180 days ago  | <a href=\"item?id=30512\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">18.</td><td><center><a id=nil href=\"vote?for=30511&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30511></span></center></td><td class=\"title\"><a href=\"https://shopify.engineering/build-production-grade-workflow-sql-modelling\" rel=\"nofollow\">How to Build a Production Grade Workflow with SQL Modelling</a><span class=\"comhead\"> (shopify.engineering) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30511>3 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 180 days ago  | <a href=\"item?id=30511\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">19.</td><td><center><a id=nil href=\"vote?for=30561&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30561></span></center></td><td class=\"title\"><a href=\"https://2muchcoffee.com/blog/how-to-build-a-fitness-app-ui-ux-design-case-study/\" rel=\"nofollow\">How to Build a Fitness App? UI-UX Design Case Study</a><span class=\"comhead\"> (2muchcoffee.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30561>2 points</span> by <a href=\"user?id=kate2mc\">kate2mc</a> 132 days ago  | <a href=\"item?id=30561\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">20.</td><td><center><a id=nil href=\"vote?for=30467&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30467></span></center></td><td class=\"title\"><a href=\"https://towardsdatascience.com/what-to-do-when-you-cant-ab-test-4e1dff692bf7\" rel=\"nofollow\">What To Do When You Can't AB Test</a><span class=\"comhead\"> (towardsdatascience.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30467>4 points</span> by <a href=\"user?id=jtloong\">jtloong</a> 216 days ago  | <a href=\"item?id=30467\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">21.</td><td><center><a id=nil href=\"vote?for=30464&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30464></span></center></td><td class=\"title\"><a href=\"https://www.tooploox.com/blog/pytorch-vs-tensorflow-a-detailed-comparison\" rel=\"nofollow\">PyTorch vs. TensorFlow – a detailed comparison</a><span class=\"comhead\"> (tooploox.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30464>4 points</span> by <a href=\"user?id=glinu\">glinu</a> 223 days ago  | <a href=\"item?id=30464\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">22.</td><td><center><a id=nil href=\"vote?for=30526&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30526></span></center></td><td class=\"title\"><a href=\"https://aws.amazon.com/s3/consistency/\" rel=\"nofollow\">Amazon S3 - Strong Consistency Support</a><span class=\"comhead\"> (amazon.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30526>2 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 171 days ago  | <a href=\"item?id=30526\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">23.</td><td><center><a id=nil href=\"vote?for=30503&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30503></span></center></td><td class=\"title\"><a href=\"https://skillenai.com/2020/11/14/popular-python-packages-for-data-science/\" rel=\"nofollow\">Popular Python Packages for Data Science</a><span class=\"comhead\"> (skillenai.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30503>2 points</span> by <a href=\"user?id=datablogger\">datablogger</a> 189 days ago  | <a href=\"item?id=30503\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">24.</td><td><center><a id=nil href=\"vote?for=30560&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30560></span></center></td><td class=\"title\"><a href=\"item?id=30560\">Text Classification using Transformers in PyTorch</a></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30560>2 points</span> by <a href=\"user?id=vatsalsaglani\">vatsalsaglani</a> 133 days ago  | <a href=\"item?id=30560\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">25.</td><td><center><a id=nil href=\"vote?for=29230&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_29230></span></center></td><td class=\"title\"><a href=\"https://blog.insightdatascience.com/why-nyc-is-a-great-place-to-break-into-ai-4acc97133391\">Why NYC is a Great Place to Break into AI</a><span class=\"comhead\"> (insightdatascience.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_29230>10 points</span> by <a href=\"user?id=ddrum001\">ddrum001</a> 816 days ago  | <a href=\"item?id=29230\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">26.</td><td><center><a id=nil href=\"vote?for=29312&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_29312></span></center></td><td class=\"title\"><a href=\"https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e7e54?source=friends_link&sk=3d1a5a32a19d611fbd81028cfd4f23fd\">A Simple Guide to Semantic Segmentation</a><span class=\"comhead\"> (medium.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_29312>9 points</span> by <a href=\"user?id=thatbrguy_\">thatbrguy_</a> 811 days ago  | <a href=\"item?id=29312\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">27.</td><td><center><a id=nil href=\"vote?for=30198&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_30198></span></center></td><td class=\"title\"><a href=\"https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86\">Overview of the different approaches to putting ML models in production</a><span class=\"comhead\"> (medium.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_30198>7 points</span> by <a href=\"user?id=linkerzx\">linkerzx</a> 755 days ago  | <a href=\"item?id=30198\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">28.</td><td><center><a id=nil href=\"vote?for=29318&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_29318></span></center></td><td class=\"title\"><a href=\"https://blog.insightdatascience.com/tunable-and-explainable-recommender-systems-cd52b6287bad\">Better Preference Predictions: Tunable and Explainable Recommender Systems</a><span class=\"comhead\"> (insightdatascience.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_29318>8 points</span> by <a href=\"user?id=e_ameisen\">e_ameisen</a> 810 days ago  | <a href=\"item?id=29318\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">29.</td><td><center><a id=nil href=\"vote?for=29272&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_29272></span></center></td><td class=\"title\"><a href=\"https://www.interviewqs.com/ddi_code_snippets/prophet_intro_\">Intro to forecasting with FB's Prophet (python)</a><span class=\"comhead\"> (interviewqs.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_29272>8 points</span> by <a href=\"user?id=erikrood\">erikrood</a> 813 days ago  | <a href=\"item?id=29272\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr><td align=right valign=top class=\"title\">30.</td><td><center><a id=nil href=\"vote?for=29543&dir=up&whence=%6e%65%77%73\"><img src=\"grayarrow.gif\" border=0 vspace=3 hspace=2></a><span id=down_29543></span></center></td><td class=\"title\"><a href=\"https://medium.com/@webdavidpage/how-to-run-a-b-testing-a-checklist-youll-want-to-bookmark-99c75aa9860b\">How to Do A/B Testing: A Checklist You’ll Want to Bookmark</a><span class=\"comhead\"> (medium.com) </span></td></tr><tr><td colspan=2></td><td class=\"subtext\"><span id=score_29543>7 points</span> by <a href=\"user?id=webdavidpage\">webdavidpage</a> 795 days ago  | <a href=\"item?id=29543\">discuss</a></td></tr><tr style=\"height:5px\"></tr><tr style=\"height:10px\"></tr><tr><td colspan=2></td><td class=\"title\"><a href=\"/x?fnid=xAcJRkFxat\" rel=\"nofollow\">More</a></td></tr></table></td></tr><tr><td><img src=\"s.gif\" height=10 width=0><table width=\"100%\" cellspacing=0 cellpadding=1><tr><td bgcolor=#00b4b4></td></tr></table><br>\n",
      "<center></center></td></tr></table></center><center><a href=\"https://www.datatau.com/rss\">RSS\n",
      "</a><a href=\"https://www.datatau.com/item?id=1\">| Announcements\n",
      "</a></center></body></html>\n"
     ]
    }
   ],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try this! 2) Parse the HTML doc with BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Using PostgreSQL as a Data Warehouse',\n",
       " 'link': 'https://www.narrator.ai/blog/using-postgresql-as-a-data-warehouse/',\n",
       " 'url': 'narrator.ai'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "all_td = soup.find_all('td', {'class':'title'})\n",
    "\n",
    "for element in all_td:\n",
    "    result = {}\n",
    "    \n",
    "    a_href = element.find('a')\n",
    "    \n",
    "    if a_href:\n",
    "        result['title'] = a_href.text\n",
    "        result['link'] = a_href['href']\n",
    "        \n",
    "    span = element.find('span', {'class': 'comhead'})\n",
    "    if span:\n",
    "        result['url'] = span.text.strip()[1:-1]\n",
    "        \n",
    "    if len(result) == 3:\n",
    "        results_list.append(result)\n",
    "        \n",
    "results_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Using PostgreSQL as a Data Warehouse</td>\n",
       "      <td>https://www.narrator.ai/blog/using-postgresql-...</td>\n",
       "      <td>narrator.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FREE Report: AI in Healthcare Survey Results</td>\n",
       "      <td>https://gradientflow.com/2021aihealthsurvey/?u...</td>\n",
       "      <td>gradientflow.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Open-source framework for self-supervised lear...</td>\n",
       "      <td>https://github.com/lightly-ai/lightly</td>\n",
       "      <td>github.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The best way to calculate Customer Lifetime Va...</td>\n",
       "      <td>https://blog.narrator.ai/how-to-calculate-ltv/</td>\n",
       "      <td>narrator.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generating realistic user timestamps in SQL</td>\n",
       "      <td>https://www.narrator.ai/blog/generating-random...</td>\n",
       "      <td>narrator.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Neuraxle - Build Neat Machine Learning Pipelines</td>\n",
       "      <td>https://www.neuraxle.org/</td>\n",
       "      <td>neuraxle.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amazing Python hacks every programmer must know</td>\n",
       "      <td>https://datamahadev.com/10-amazing-python-hack...</td>\n",
       "      <td>datamahadev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Women Multi-Coloured Printed Top</td>\n",
       "      <td>https://wlook.in/product/women-multi-coloured-...</td>\n",
       "      <td>wlook.in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Needles and syringes uk</td>\n",
       "      <td>https://www.ahpmedicals.com/disposables/gloves...</td>\n",
       "      <td>ahpmedicals.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One question to make your data project 10x mor...</td>\n",
       "      <td>https://blog.narrator.ai/one-question-to-make-...</td>\n",
       "      <td>narrator.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Adversarial Attacks For Fooling Deep Neural Ne...</td>\n",
       "      <td>https://neurosys.com/article/adversarial-attac...</td>\n",
       "      <td>neurosys.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The path to learning SQL and mastering it to b...</td>\n",
       "      <td>https://medium.com/analytics-and-data/the-path...</td>\n",
       "      <td>medium.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5 tips for aspiring and junior data engineers</td>\n",
       "      <td>https://medium.com/analytics-and-data/5-tips-f...</td>\n",
       "      <td>medium.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Science Career Path &amp; Progression</td>\n",
       "      <td>https://medium.com/analytics-and-data/data-sci...</td>\n",
       "      <td>medium.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Personal Python Data Science Toolkit</td>\n",
       "      <td>https://www.alexfranz.com/posts/personal-pytho...</td>\n",
       "      <td>alexfranz.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Introducing Amazon Managed Workflows for Apach...</td>\n",
       "      <td>https://aws.amazon.com/blogs/aws/introducing-a...</td>\n",
       "      <td>amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Leveraging ML Compute for Accelerated Training...</td>\n",
       "      <td>https://machinelearning.apple.com/updates/ml-c...</td>\n",
       "      <td>apple.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How to Build a Production Grade Workflow with ...</td>\n",
       "      <td>https://shopify.engineering/build-production-g...</td>\n",
       "      <td>shopify.engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How to Build a Fitness App? UI-UX Design Case ...</td>\n",
       "      <td>https://2muchcoffee.com/blog/how-to-build-a-fi...</td>\n",
       "      <td>2muchcoffee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What To Do When You Can't AB Test</td>\n",
       "      <td>https://towardsdatascience.com/what-to-do-when...</td>\n",
       "      <td>towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PyTorch vs. TensorFlow – a detailed comparison</td>\n",
       "      <td>https://www.tooploox.com/blog/pytorch-vs-tenso...</td>\n",
       "      <td>tooploox.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Amazon S3 - Strong Consistency Support</td>\n",
       "      <td>https://aws.amazon.com/s3/consistency/</td>\n",
       "      <td>amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Popular Python Packages for Data Science</td>\n",
       "      <td>https://skillenai.com/2020/11/14/popular-pytho...</td>\n",
       "      <td>skillenai.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Why NYC is a Great Place to Break into AI</td>\n",
       "      <td>https://blog.insightdatascience.com/why-nyc-is...</td>\n",
       "      <td>insightdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A Simple Guide to Semantic Segmentation</td>\n",
       "      <td>https://medium.com/beyondminds/a-simple-guide-...</td>\n",
       "      <td>medium.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Overview of the different approaches to puttin...</td>\n",
       "      <td>https://medium.com/analytics-and-data/overview...</td>\n",
       "      <td>medium.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Better Preference Predictions: Tunable and Exp...</td>\n",
       "      <td>https://blog.insightdatascience.com/tunable-an...</td>\n",
       "      <td>insightdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Intro to forecasting with FB's Prophet (python)</td>\n",
       "      <td>https://www.interviewqs.com/ddi_code_snippets/...</td>\n",
       "      <td>interviewqs.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How to Do A/B Testing: A Checklist You’ll Want...</td>\n",
       "      <td>https://medium.com/@webdavidpage/how-to-run-a-...</td>\n",
       "      <td>medium.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                Using PostgreSQL as a Data Warehouse   \n",
       "1        FREE Report: AI in Healthcare Survey Results   \n",
       "2   Open-source framework for self-supervised lear...   \n",
       "3   The best way to calculate Customer Lifetime Va...   \n",
       "4         Generating realistic user timestamps in SQL   \n",
       "5    Neuraxle - Build Neat Machine Learning Pipelines   \n",
       "6     Amazing Python hacks every programmer must know   \n",
       "7                    Women Multi-Coloured Printed Top   \n",
       "8                             Needles and syringes uk   \n",
       "9   One question to make your data project 10x mor...   \n",
       "10  Adversarial Attacks For Fooling Deep Neural Ne...   \n",
       "11  The path to learning SQL and mastering it to b...   \n",
       "12      5 tips for aspiring and junior data engineers   \n",
       "13             Data Science Career Path & Progression   \n",
       "14           The Personal Python Data Science Toolkit   \n",
       "15  Introducing Amazon Managed Workflows for Apach...   \n",
       "16  Leveraging ML Compute for Accelerated Training...   \n",
       "17  How to Build a Production Grade Workflow with ...   \n",
       "18  How to Build a Fitness App? UI-UX Design Case ...   \n",
       "19                  What To Do When You Can't AB Test   \n",
       "20     PyTorch vs. TensorFlow – a detailed comparison   \n",
       "21             Amazon S3 - Strong Consistency Support   \n",
       "22           Popular Python Packages for Data Science   \n",
       "23          Why NYC is a Great Place to Break into AI   \n",
       "24            A Simple Guide to Semantic Segmentation   \n",
       "25  Overview of the different approaches to puttin...   \n",
       "26  Better Preference Predictions: Tunable and Exp...   \n",
       "27    Intro to forecasting with FB's Prophet (python)   \n",
       "28  How to Do A/B Testing: A Checklist You’ll Want...   \n",
       "\n",
       "                                                 link                     url  \n",
       "0   https://www.narrator.ai/blog/using-postgresql-...             narrator.ai  \n",
       "1   https://gradientflow.com/2021aihealthsurvey/?u...        gradientflow.com  \n",
       "2               https://github.com/lightly-ai/lightly              github.com  \n",
       "3      https://blog.narrator.ai/how-to-calculate-ltv/             narrator.ai  \n",
       "4   https://www.narrator.ai/blog/generating-random...             narrator.ai  \n",
       "5                           https://www.neuraxle.org/            neuraxle.org  \n",
       "6   https://datamahadev.com/10-amazing-python-hack...         datamahadev.com  \n",
       "7   https://wlook.in/product/women-multi-coloured-...                wlook.in  \n",
       "8   https://www.ahpmedicals.com/disposables/gloves...         ahpmedicals.com  \n",
       "9   https://blog.narrator.ai/one-question-to-make-...             narrator.ai  \n",
       "10  https://neurosys.com/article/adversarial-attac...            neurosys.com  \n",
       "11  https://medium.com/analytics-and-data/the-path...              medium.com  \n",
       "12  https://medium.com/analytics-and-data/5-tips-f...              medium.com  \n",
       "13  https://medium.com/analytics-and-data/data-sci...              medium.com  \n",
       "14  https://www.alexfranz.com/posts/personal-pytho...           alexfranz.com  \n",
       "15  https://aws.amazon.com/blogs/aws/introducing-a...              amazon.com  \n",
       "16  https://machinelearning.apple.com/updates/ml-c...               apple.com  \n",
       "17  https://shopify.engineering/build-production-g...     shopify.engineering  \n",
       "18  https://2muchcoffee.com/blog/how-to-build-a-fi...         2muchcoffee.com  \n",
       "19  https://towardsdatascience.com/what-to-do-when...  towardsdatascience.com  \n",
       "20  https://www.tooploox.com/blog/pytorch-vs-tenso...            tooploox.com  \n",
       "21             https://aws.amazon.com/s3/consistency/              amazon.com  \n",
       "22  https://skillenai.com/2020/11/14/popular-pytho...           skillenai.com  \n",
       "23  https://blog.insightdatascience.com/why-nyc-is...  insightdatascience.com  \n",
       "24  https://medium.com/beyondminds/a-simple-guide-...              medium.com  \n",
       "25  https://medium.com/analytics-and-data/overview...              medium.com  \n",
       "26  https://blog.insightdatascience.com/tunable-an...  insightdatascience.com  \n",
       "27  https://www.interviewqs.com/ddi_code_snippets/...         interviewqs.com  \n",
       "28  https://medium.com/@webdavidpage/how-to-run-a-...              medium.com  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### 1) Fetch the content by URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  404\n",
      "\n",
      "First part of HTML document fetched as string:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\">\n",
      "    <head>\n",
      "        <title>craigslist | Page Not Found</title>\n",
      "        <link type=\"text/css\" rel=\"stylesheet\" media=\"all\" href=\"//www.craigslist.org/styles/simple-page.css?v=2c3feefd3d908128d711c09b8efaaf59\">\n",
      "        <link type=\"text/css\" rel=\"stylesheet\" media=\"all\" href=\"//www.craigslist.org/styles/jquery-ui-clcustom.css?v=3b05ddffb7c7f5b62066deff2dda9339\">\n",
      "        <link type=\"text/css\" rel=\"stylesheet\" media=\"all\" href=\"//www.craigslist.org/styles/jquery.qtip-2.2.1.css?v=cd202aead4d1dd4894fbae4ade23fcf8\">\n",
      "        <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
      "    </head>\n",
      "\n",
      "    <body class=\"simple header footer\">\n",
      "        <script type=\n"
     ]
    }
   ],
   "source": [
    "# You'll need the requests library in order to fully utilize bs4.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Target web page:\n",
    "url = \"https://atlanta.craigslist.org/atl/wan/d/cheap-or-free-running-car-or/6485015376.html\"\n",
    "\n",
    "# Establishing the connection to the web page:\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "# Ex., 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found.\n",
    "print('Status Code: ',response.status_code)\n",
    "\n",
    "# Pull the HTML string out of requests and convert it to a Python string.\n",
    "html = response.text\n",
    "\n",
    "# The first 700 characters of the content.\n",
    "print(\"\\nFirst part of HTML document fetched as string:\\n\")\n",
    "print(html[:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on [request status codes](http://www.restapitutorial.com/httpstatuscodes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### 2) Parse the HTML document with Beautiful Soup.\n",
    "\n",
    "This step allows us to access the elements of the document by XPath expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>DataTau</title>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular element:\n",
    "soup.html.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataTau\n"
     ]
    }
   ],
   "source": [
    "# Just the text between elements:\n",
    "print(soup.html.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4e160506e061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# First parameter:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"header-logo\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Find single or multiple elements.\n",
    "# First parameter:\n",
    "element = soup.find_all(\"a\", {\"class\": \"header-logo\"})\n",
    "element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0f8d7d734476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprice_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"price\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprice_search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "price_search = soup.findAll('span', {\"class\": \"price\"})\n",
    "price_search[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about all car listings in ATL:\n",
    "response = requests.get(\"https://atlanta.craigslist.org/search/cto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "result_list = soup.find_all('p', {'class':'result-info'})\n",
    "\n",
    "results = []\n",
    "for result in result_list:\n",
    "    car = {}\n",
    "    car['text'] = result.find('a', {'class':'hdrlnk'}).text\n",
    "    car['price'] = int(result.find('span', {'class':'result-price'}).text.replace('$',''))\n",
    "    hood = result.find('span', {'class':'result-hood'})\n",
    "    car['hood'] = hood.text.replace('(','').replace(')','') if hood else None\n",
    "    results.append(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**\n",
    "\n",
    "- How would you get the next 120 results?\n",
    "- How would you get the text associated with a particular car?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xpath'></a>\n",
    "\n",
    "## What is XPath?\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/obama_wiki.png)\n",
    "\n",
    "Understanding how to identify elements and attributes within HTML documents gives us the ability to write simple expressions that create structured data. We can think of XPath like a query language for HTML.\n",
    "\n",
    "To simplify this process, we'll be using the Chrome extension XPath Helper. It's not necessary but highly recommended when building XPath expressions.\n",
    "\n",
    "[XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en).\n",
    "\n",
    "XPath expressions can select elements, element attributes, and element text. These selections can apply to a single item or multiple items. Generally, if you're not specific enough, you'll end up selecting multiple elements.\n",
    "\n",
    "\n",
    "<a id='multiple-selections'></a>\n",
    "### Multiple Selections\n",
    "\n",
    "***Multiple selections*** are useful for capturing search results or any repeating element. For instance, the _titles_ from apartment listing search results on Craigslist.\n",
    "\n",
    "\n",
    "**URL**\n",
    "\n",
    "[http://sfbay.craigslist.org/search/sfc/apa](http://sfbay.craigslist.org/search/sfc/apa)\n",
    "\n",
    "\n",
    "**Example HTML Markup**\n",
    "```html\n",
    "...\n",
    "<span class=\"pl\"> \n",
    "    <time datetime=\"2016-01-12 23:27\" title=\"Tue 12 Jan 11:27:35 PM\">Jan 12</time> \n",
    "    <a href=\"/sfc/apa/5400584579.html\" data-id=\"5400584579\" class=\"hdrlnk\">Welcome home to a sweetly renovated four-bedroom, one-and-a-half bath.</a> \n",
    "</span>\n",
    "...\n",
    "```\n",
    "\n",
    "**XPath:: Multiple Titles** _Copy this into the XPath Helper Query box_:\n",
    "```\n",
    "//a[@class='result-title hdrlnk']\n",
    "```\n",
    "\n",
    "**Returns (Ad Titles)**\n",
    "```\n",
    "***New Remodeled two bedroom Apartment***\n",
    "WONDERFUL ONE BR APARTMENT HOME\n",
    "Beautiful 1bed/1bath Apartment in Russian Hill NO SECURITY DEPOSIT\n",
    "Knockout SF View|Green Oasis|Private Driveway|Furnished\n",
    "3BR/3BA Spacious, Beautiful SOMA Loft: 5 month lease\n",
    "Nob Hill Large Studio - Light, Quiet, Lovely Building\n",
    "etc...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='singular-selections'></a>\n",
    "\n",
    "### Singular Selections\n",
    "\n",
    "***Singular selections*** are necessary when you want to grab specific, unique text within elements. Here's an example of a details page on Craigslist:\n",
    "\n",
    "**URL**\n",
    "\n",
    "[https://sfbay.craigslist.org/sfc/apa/6161864063.html](https://sfbay.craigslist.org/sfc/apa/6161864063.html)\n",
    "\n",
    "**HTML Markup**\n",
    "\n",
    "```html\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "**XPath: Single Item**\n",
    "\n",
    "```\n",
    "//p[@class='postinginfo'][2]/time\n",
    "```\n",
    "**Returns (Time of Posting or Age of Post)**\n",
    "```\n",
    "2016-01-12 11:23pm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "\n",
    "## A Simple Example Using Scrapy and XPath\n",
    "\n",
    "---\n",
    "\n",
    "Below is an example of how to get information out of fake HTML using the XPath capabilities of the Scrapy package. You'll likely need to install the Scrapy package using `conda install scrapy`.   \n",
    "\n",
    "**Note:** `conda install` will install the necessary dependent packages needed for Scrapy; `pip install` will **not**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `selector` class from the Scrapy library to help us construct our query.\n",
    "\n",
    "`Selector` classes take the HTML target as an argument and can then utilize several query types to extract information. In this case, we'll specify `XPath`, as our query will utilize XPath language. \n",
    "\n",
    "Just like with writing Python scripts, there are several ways you can access the exact same information in HTML. Let's try a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.5.0-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[K     |████████████████████████████████| 254 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 26.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting h2<4.0,>=3.0\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 27.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (4.6.1)\n",
      "Collecting PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\"\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting Twisted[http2]>=17.9.0\n",
      "  Downloading Twisted-21.2.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 120.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (3.1.1)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.1-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (5.1.2)\n",
      "Requirement already satisfied: six in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 63.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 20.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting priority<2.0,>=1.1.0; extra == \"http2\"\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Requirement already satisfied: setuptools in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: idna>=2.5 in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: pycparser in /Users/henrybutler/opt/anaconda3/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=24c775a43fd372f2147fd967972c1d10404e4ccc38ba75d1647d8f969b91091c\n",
      "  Stored in directory: /Users/henrybutler/Library/Caches/pip/wheels/91/64/36/bd0d11306cb22a78c7f53d603c7eb74ebb6c211703bc40b686\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=41190554d0a15fb626c53626dc953e67b68c211f1ce92c870b1977d6c0a8669c\n",
      "  Stored in directory: /Users/henrybutler/Library/Caches/pip/wheels/d1/d7/61/11b5b370ee487d38b5408ecb7e0257db9107fa622412cbe2ff\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: protego, w3lib, cssselect, parsel, pyasn1, pyasn1-modules, service-identity, hyperframe, hpack, h2, PyDispatcher, jmespath, itemadapter, itemloaders, incremental, Automat, hyperlink, constantly, priority, Twisted, queuelib, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.2.0 constantly-15.1.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.1 scrapy-2.5.0 service-identity-21.1.0 w3lib-1.22.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best of']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "# HTML structure string:\n",
    "HTML = \"\"\"\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Option 1: Use the exact class name to get its associated text.\n",
    "best = Selector(text=HTML).xpath(\"//span[@class='bestof-text']/text()\").extract()\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best of']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Use the contains() function to extract any text that includes the text 'best of.'\n",
    "best = Selector(text=HTML).xpath(\"//span[contains(text(), 'best of')]/text()\").extract()\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best of']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 3: Grabs the entire HTML post where class='bestof-link'.\n",
    "best =  Selector(text=HTML).xpath(\"/html/body/div/p/a[@class='bestof-link']\")\n",
    "        # Parse the first grabbed chunk of the text for the specific element with class='bestof-text'.\n",
    "nested_best =  best.xpath(\"./span[@class='bestof-text']/text()\").extract()\n",
    "nested_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Option 3 will probably be the most common because there's a good chance you'll want to grab information from several child elements that exist within one parent element._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where's Waldo? — XPath Edition\n",
    "\n",
    "In this example, we'll find Waldo together. Find Waldo as:\n",
    "\n",
    "- An element.\n",
    "- An attribute.\n",
    "- A text element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        \n",
    "        <ul id=\"waldo\">\n",
    "            <li class=\"waldo\">\n",
    "                <span> yo I'm not here</span>\n",
    "            </li>\n",
    "            <li class=\"waldo\">Height:  ???</li>\n",
    "            <li class=\"waldo\">Weight:  ???</li>\n",
    "            <li class=\"waldo\">Last Location:  ???</li>\n",
    "            <li class=\"nerds\">\n",
    "                <div class=\"alpha\">Bill Gates</div>\n",
    "                <div class=\"alpha\">Zuckerberg</div>\n",
    "                <div class=\"beta\">Theil</div>\n",
    "                <div class=\"animal\">Parker</div>\n",
    "            </li>\n",
    "        </ul>\n",
    "        \n",
    "        <ul id=\"tim\">\n",
    "            <li class=\"tdawg\">\n",
    "                <span>yo im here</span>\n",
    "            </li>\n",
    "        </ul>\n",
    "        <li>stuff</li>\n",
    "        <li>stuff2</li>\n",
    "        \n",
    "        <div id=\"cooldiv\">\n",
    "            <span class=\"dsi-rocks\">\n",
    "               YO!\n",
    "            </span>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <waldo>Waldo</waldo>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** We can use the asterisk character `*` as a placeholder for \"all possible.\"\n",
    "\n",
    "```python\n",
    "# All elements where class='alpha':\n",
    "Selector(text=HTML).xpath('//*[@class=\"alpha\"]').extract()\n",
    "\n",
    "\n",
    "\n",
    "# Returns:\n",
    "\n",
    "[u'<div class=\"alpha\">Bill gates</div>',\n",
    " u'<div class=\"alpha\">Zuckerberg</div>']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<div class=\"alpha\">Bill Gates</div>', '<div class=\"alpha\">Zuckerberg</div>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=HTML).xpath('//*[@class=\"alpha\"]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find the element 'Waldo': **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waldo']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text contents of the element Waldo:\n",
    "Selector(text=HTML).xpath('/html/body/waldo/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the attribute(s) 'Waldo':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ul id=\"waldo\">\\n            <li class=\"waldo\">\\n                <span> yo I\\'m not here</span>\\n            </li>\\n            <li class=\"waldo\">Height:  ???</li>\\n            <li class=\"waldo\">Weight:  ???</li>\\n            <li class=\"waldo\">Last Location:  ???</li>\\n            <li class=\"nerds\">\\n                <div class=\"alpha\">Bill Gates</div>\\n                <div class=\"alpha\">Zuckerberg</div>\\n                <div class=\"beta\">Theil</div>\\n                <div class=\"animal\">Parker</div>\\n            </li>\\n        </ul>',\n",
       " '<li class=\"waldo\">\\n                <span> yo I\\'m not here</span>\\n            </li>',\n",
       " '<li class=\"waldo\">Height:  ???</li>',\n",
       " '<li class=\"waldo\">Weight:  ???</li>',\n",
       " '<li class=\"waldo\">Last Location:  ???</li>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of all attributes named Waldo:\n",
    "Selector(text=HTML).xpath('//*[@*=\"waldo\"]').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<li class=\"waldo\">\\n                <span> yo I\\'m not here</span>\\n            </li>',\n",
       " '<li class=\"waldo\">Height:  ???</li>',\n",
       " '<li class=\"waldo\">Weight:  ???</li>',\n",
       " '<li class=\"waldo\">Last Location:  ???</li>']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of all class attributes named Waldo:\n",
    "Selector(text=HTML).xpath('//*[@class=\"waldo\"]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the text element Waldo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<waldo>Waldo</waldo>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets everything around the text element Waldo:\n",
    "Selector(text=HTML).xpath(\"//*[text()='Waldo']\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "<a scrapy-spiders></a>\n",
    "## What is a Scrapy Spider?\n",
    "\n",
    "---\n",
    "\n",
    "> *\"[Scrapy](http://scrapy.org/) is an application framework for writing web spiders that \"crawl\" around websites and extract data from them.\"*\n",
    "\n",
    "Below we'll walk through the creation of a **spider** using Scrapy. Spiders are automated processes that will crawl through a web page or web pages to collect information.\n",
    "\n",
    "> **Note:** This code should be written in a script outside of Jupyter.\n",
    "\n",
    "<a id='scrapy-project'></a>\n",
    "### 1) Create a new Scrapy project.\n",
    "\n",
    "In your terminal, `cd` into a directory where you want to create your spider's folder. We recommend the desktop for easy access to the files.\n",
    "> `scrapy startproject craigslist`\n",
    "\n",
    "**It should create an output that looks like this:**\n",
    "<blockquote>\n",
    "```\n",
    "New Scrapy project 'craigslist', using template directory '/Users/jmpounders/anaconda3/lib/python3.6/site-packages/scrapy/templates/project', created in:\n",
    "    /Users/jmpounders/dsi-east-2/scrapy/craigslist\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd craigslist\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**That command generates a set of project files:**\n",
    "<blockquote>\n",
    "```\n",
    "├── craigslist\n",
    "│   ├── __init__.py\n",
    "│   ├── __pycache__\n",
    "│   ├── items.py\n",
    "│   ├── middlewares.py\n",
    "│   ├── pipelines.py\n",
    "│   ├── settings.py\n",
    "│   └── spiders\n",
    "│       ├── __init__.py\n",
    "│       └── __pycache__\n",
    "└── scrapy.cfg\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Generally, these are our files. We'll go into more detail on these soon.\n",
    "\n",
    " * **`scrapy.cfg`:** The project's configuration file.\n",
    " * **`craigslist/`:** The project’s Python module — you’ll import your code from here later.\n",
    " * **`craigslist/items.py`:** The project’s items file.\n",
    " * **`craigslist/pipelines.py`:** The project’s pipelines file.\n",
    " * **`craigslist/settings.py`:** The project’s settings file.\n",
    " * **`craigslist/spiders/`:** A directory where you’ll store your spiders.\n",
    " \n",
    "Please also add this line to your `craigslist/settings.py` file before continuing:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " ```\n",
    " </blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='define-item'></a>\n",
    "### 2) Define an \"item.\"\n",
    "\n",
    "When we define an item, we're telling our new application what it will be collecting. In essence, an item is an entity that has attributes (\"title,\" \"description,\" \"price,\" etc.) that are descriptive and relate to elements on pages we'll be scraping.  \n",
    "\n",
    "In more precise terms, this is a model (for those who are familiar with object-relational mapping or relational database terms). Don't worry if this is a foreign concept.  The main idea is to understand that a model has attributes that closely resemble or relate to elements on our target web page(s).\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items.\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CraigslistItem(scrapy.Item):\n",
    "    # Define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='spider-crawl'></a>\n",
    "### 3) A spider that crawls.\n",
    "\n",
    "An item is a model that resembles data on a web page. A spider is something that crawls pages and uses our item model to get and hold items for us.\n",
    "\n",
    "**Scrapy spiders are Python classes. Let's write our first file, called `craigslist_spider.py`, and put it in our `/spiders` directory.**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = \"craigslist\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "        \"https://atlanta.craigslist.org/search/cto\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "\n",
    "**Next, let's dive in and crawl from our `/craigslist/craigslist` directory.**\n",
    "\n",
    "```\n",
    "> scrapy crawl craigslist\n",
    "```\n",
    "\n",
    "**What just happened?**\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * It parsed over the content containing the HTML markup of each request URL.\n",
    " * What else?\n",
    " \n",
    "```python\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.body)\n",
    "```\n",
    "\n",
    "It saved a file in our base project directory. It should be named based on the end of the URL. In our case, it should create a file called \"sfc.\" This is taken directly from the Scrapy docs and its only point is to illustrate the workflow so far. It's nice to have a reference to our HTML file.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='xpath-spider'></a>\n",
    "### 4) XPath and parsing with our spider.\n",
    "\n",
    "So far, we've defined the fields we'll get, some URLs to fetch, and saved some content to a file. Now, it's about to get interesting.\n",
    "\n",
    "**We should let our spider know about the item model we created earlier. In the head of the `craigslist/craigslist/spiders/craigslist_spider.py`, let's add a new import.**\n",
    "\n",
    "```python\n",
    "from craigslist.items import CraigslistItem\n",
    "```\n",
    "\n",
    "> **Check:** Why won't it work otherwise?\n",
    "\n",
    "<br><br><br>\n",
    "**Let's replace our `parse()` method to find some data from our Craigslist spider response and map them to our item model, `CraigslistItem`.**\n",
    "\n",
    "\n",
    "```python\n",
    "def parse(self, response): # Define parse() function. \n",
    "    items = [] # Element for storing scraped information.\n",
    "\thxs = Selector(response) # Selector allows us to grab HTML from the response (target website).\n",
    "\tfor sel in hxs.xpath(\"//li[@class='result-row']/p\"): # Because we're using XPath language, we need to specify that the paragraphs we're trying to isolate are expressed via XPath.\n",
    "\t\titem = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"a/text()\").extract() # Title text from the 'a' element. \n",
    "\t\titem['link']  =  sel.xpath(\"a/@href\").extract() # Href/URL from the 'a' element. \n",
    "\t\titem['price'] =  sel.xpath('span/span[@class=\"result-price\"]/text()').extract()[0]\n",
    "                # Price from the result price class nested in a few span elements.\n",
    "        items.append(item)\n",
    "\treturn items # Shows scraped information as terminal output.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='save-examine'></a>\n",
    "### 5) Save and examine our scraped data.\n",
    "\n",
    "By default, we can save our crawled data in a CSV format. To save our data, we just need to pass a few optional parameters to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "> scrapy crawl craigslist -o items.csv -t csv\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "It's always good to iteratively check the data when developing a spider to make sure the set is close to what we want. \n",
    "\n",
    "> *Pro tip: The longer your iterations are between checks, the harder it's going to be to understand what's not working and fix bugs.*\n",
    "\n",
    "You should now have a file called '`items.csv`' in the directory from which you ran the `scrapy crawl` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addendum'></a>\n",
    "## Addendum: Leveraging XPath to Get More Results\n",
    "\n",
    "---\n",
    "\n",
    "Generally, a workflow that's useful in this context is to load the page in your Chrome browser, check out the page using the XPath Helper plugin, and, from that, derive your own XPath expressions based on the output.\n",
    "\n",
    "`text()` selects only the text of a given element (between the tags), and `@attribute_name` is used to select attributes.\n",
    "\n",
    "**Here are a few examples of `text()`:**\n",
    "<blockquote>\n",
    "```\n",
    "<h1>Darwin - The Evolution Of An Exhibition</h1>\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "The XPath selector for this:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "//h1/text()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**Here are a few examples of attributes:**\n",
    "\n",
    "And the description is contained inside a `<div>` tag with `id=\"description\"`:\n",
    "<blockquote>\n",
    "```\n",
    "<h2>Description:</h2>\n",
    "\n",
    "<div id=\"description\">\n",
    "Short documentary made for the Plymouth City Museum and Art Gallery regarding the set up of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.\n",
    "</div>\n",
    "...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "XPath\n",
    "<blockquote>\n",
    "```\n",
    "//div[@id='description']\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following Links for More Results\n",
    "\n",
    "One hundred results is pretty good, but what if we want more? We need to follow the \"next\" links and find new pages to grab. Using the **`parse()`** method of our spider class, we need to return another type of object.\n",
    "\n",
    "See [Stack Overflow](https://stackoverflow.com/questions/30152261/make-scrapy-follow-links-and-collect-data) for details!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

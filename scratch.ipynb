{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c023073-a48d-404d-b5ae-2237ff46861c",
   "metadata": {},
   "source": [
    "### What is computer vision?\n",
    "Computer Vision is a subfield of [Deep Learning](https://github.com/letspython3x/Books/blob/master/Deep%20Learning%20with%20Python.pdf) and Artificial Intelligence, also refer [here](https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/chapter-1/66)  where humans teach computers to see and interpret the world around them.\n",
    "\n",
    "\n",
    "Image Classification is one of the most fundamental tasks in computer vision. It has revolutionized and propelled technological advancements in the most prominent fields, including the automobile industry, healthcare, manufacturing, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d8297-426a-4fad-bc74-f1e1329c5777",
   "metadata": {},
   "source": [
    "### How does image classification work?\n",
    "Image Classification (often referred to as Image Recognition) is the task of associating one (single-label classification) or more (multi-label classification) labels to a given image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86602406-a0a0-462f-a313-bc30b102b718",
   "metadata": {},
   "source": [
    "### Basic steps in computer vision:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89577d4-f50e-4fcf-8569-f7d73926efab",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "   - Loading images or videos (image frames)\n",
    "   \n",
    "### Preprocessing: \n",
    "\n",
    "   - Normalization, rescaling pixels/resizing, color transformation, one Hot encoding etc\n",
    "    \n",
    "### Feature Extraction: \n",
    "   - Finding unique characteristic/features of an image\n",
    "    \n",
    "### Modeling\n",
    "   - Learn from the extracted features to predict and classify object (source:[here](https://github.com/moelgendy/deep_learning_for_vision_systems))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77a578-9a84-4d48-b2ec-8a69e66f84f1",
   "metadata": {},
   "source": [
    "## How Computers \"See\" images?\n",
    "When we look at an image, we see objects, landscape, colors, and so on. But that’s not the case with computers. Images are just a 2D arrays of numbers of pixels (for grayscale pictures); where a pixel is simply a number represented by a range of either zero to one or in 0 to 255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169e859-d010-4849-b1e2-8507968df8a3",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv1.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf428eb3-a498-47a1-90e7-511a1e6f158b",
   "metadata": {},
   "source": [
    "Fig.1 Source: [Computer vision: The power of seeing and interpreting images](https://datascience.aero/computer-vision-seeing-interpreting-images/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a7311-96d9-4542-82a5-d06798087941",
   "metadata": {},
   "source": [
    ">In grayscale images, each pixel represents the intensity of only one color, whereas in the standard RGB system, color images have three channels (red, green, and blue). In other words, color images are represented by three matrices: one represents the intensity of red in the pixel, one represents green, and one represents blue ([source](https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/chapter-1/125))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c10bd-4ed0-4564-bfb5-702a4db1c93c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv2.png\" width=\"600\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Fig.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c4cb0-7f65-4c4e-87e1-4cf781f22504",
   "metadata": {},
   "source": [
    ">As you can see in figure above, the color image is composed of three channels: red, green, and blue. Now the question is, how do computers see this image? Again, they see the matrix, unlike grayscale images, where we had only one channel. In this case, we will have three matrices stacked on top of each other; that’s why it’s a 3D matrix. The dimensionality of 700 × 700 color images is (700, 700, 3). Let’s say the first matrix represents the red channel; then each element of that matrix represents an intensity of red color in that pixel, and likewise with green and blue. Each pixel in a color image has three numbers (0 to 255) associated with it. These numbers represent intensity of red, green, and blue color in that particular pixel. <br>\n",
    "If we take the pixel $F(0,0)$ as an example, we will see that it represents the top-left pixel of the image of green grass. When we view this pixel in the color images, it looks like figure below which shows some shades of the color green and their RGB values ([source](https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/chapter-1/https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/chapter-1/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6ab60-65e2-4b7e-968d-d3b54d5ef3b0",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"images/cv3.png\" width=\"300\"/>\n",
    "</div>\n",
    "<div>\n",
    "    <img src=\"images/cv17.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afff8ba-fb9a-47a1-896e-559b6221d083",
   "metadata": {},
   "source": [
    "Fig.3:  `RGB` color cobination formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653774f1-b5ed-47bb-ba61-77f220ddf05e",
   "metadata": {},
   "source": [
    "# 2. Image Preprocessing\n",
    "- Normalization, standardization, one Hot encoding etc \n",
    "Any adjustments that you need to apply to your dataset are part of preprocessing.The good news is that, unlike traditional machine learning, DL algorithms require minimum data preprocessing because neural networks do most of the heavy lifting in processing an image and extracting features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c800548-01e7-470a-a4f2-0234ed7ad49d",
   "metadata": {},
   "source": [
    "- Image processing could involve simple tasks like image resizing. In order to feed a dataset of images to a convolutional network, the images all have to be the same size.\n",
    "- Converting color images to grayscale to reduce computation complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d67de-1393-4d17-a428-7350dfdd8239",
   "metadata": {},
   "source": [
    "- Standardizing images: images must be preprocessed and scaled to have identical widths and heights before being fed to the learning algorithm.\n",
    "\n",
    "- Data augmentation --Another common preprocessing technique involves augmenting the existing dataset with modified versions of the existing images. Scaling, rotations, and other affine transformations are typically used to enlarge your dataset and expose the neural network to a wide variety of variations of your images. This makes it more likely that your model will recognize objects when they appear in any form and shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05a812-9bd9-40b2-94be-5da0a9162c6f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cnn16.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Fig.6. [source](https://www.kaggle.com/kedarsai/cifar-10-88-accuracy-using-keras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d2002-0981-44b8-bab1-2f271d06a8cc",
   "metadata": {},
   "source": [
    "# Building Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a0d19-c29b-467f-bf94-70ec43edf82c",
   "metadata": {},
   "source": [
    "### Let's start with building an Artificial Neural Network(ANN)\n",
    ">In figure below, we can see an analogy between biological neurons and artificial systems. Both contain a main processing element, a `neuron`, with input signals $(x_1, x_2, ..., x_n)$ and an output (Mohammed 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba744b-7497-4c21-89de-3c0b7db80896",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv15.png\" width=\"400\"/> \n",
    "<div>\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c689d3-a112-4b15-8404-10cca1747f08",
   "metadata": {},
   "source": [
    "Fig.ANN1: Single Neuron - Single layer Network (`Perceptron`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd0d69-2199-4e04-b862-348fb75bd643",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"images/cv16.png\" width=\"600\"/> \n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4067a7-4148-4a05-8375-d77a39921aac",
   "metadata": {},
   "source": [
    "Fig.ANN2: Multilayer Neuron\n",
    "Deep learning involves layers of neurons in a network or  `Multilayer perceptron` <br>\n",
    "Fig.7 ( for both `ANN1` and `ANN2`, above) source from (Mohammed 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ca0ab-890b-4fe3-9584-6172c69fcdff",
   "metadata": {},
   "source": [
    "* **ANN is imitation of how information is processed in human brain; when millions of neurons (`perceptrons in ANN`) are stacked in layers and connected together,a multilayer neural network is called `deep learning`.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92dbff-06a3-435c-9330-8196ca846728",
   "metadata": {},
   "source": [
    "# So, what is a `Perceptron`? \n",
    "* **Let's zoom in to a `Multilayer perceptron (MLP)` above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036bac58-5373-4622-92f2-687a61e9a8fb",
   "metadata": {},
   "source": [
    "### Perceptron is the fundamental building blocks of `Neural Networks` in Deep Learning. \n",
    "- If we really want to know how neural network works, we better learn closely how perceptron works.<br>\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Perceptron) definition, a `Perceptron` is an algorithm for learning a binary classifier called a `threshold function`: a function that maps its input $X$ (a real-valued vector) to an output value $f(X)$ (a single binary value):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b324e-4e3c-4e85-9f3a-c161a43b0d3d",
   "metadata": {},
   "source": [
    "$$\n",
    "f(X) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1  \\quad \\text{if} \\quad W.X + b > 0, \\\\ 0 \\quad \\text{otherwise} \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165f890-8715-40f7-b50d-55b447f04c90",
   "metadata": {},
   "source": [
    "where $W$ is a vector of real-valued weights, $W.X$ is the dot product $\\sum_{i=1}^{m}w_ix_i$, where $m$ is the number of inputs to the perceptron, and $b$ is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value. <br>\n",
    "The value of $f(X)$ ($0$ or $1$) is used to classify $X$  as either a positive or a negative instance, in the case of a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e143e8-7e0c-4ac7-b7f1-8a02915f04bf",
   "metadata": {},
   "source": [
    "In the context of neural networks, a perceptron is an `artificial neuron` using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest feedforward neural network (source: [Wikipedia](https://en.wikipedia.org/wiki/Perceptron))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fab4f-9e89-4281-9f44-3b76d632963a",
   "metadata": {},
   "source": [
    "## Notation:\n",
    "The dot product of two vectors `A` and `B`: <br>\n",
    "$A = [a_1, a_2, ..., a_n]$ and $B = [b_1, b_2, ..., b_n]$ is given by:<br>\n",
    "$$\n",
    "A.B = \\sum_{i=1}^{n}a_i*b_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15536f84-0a77-406d-b959-54ad9ddb4612",
   "metadata": {},
   "source": [
    "which is simply $A^T*B$, a matrix multiplication (source: [Dot product in matrix notation](https://mathinsight.org/dot_product_matrix_notation))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b53b8-9b03-4972-bec4-c3ae9f80c9f4",
   "metadata": {},
   "source": [
    "Now, getting back to our perceptron concept, assume we have the following vectors: `X` and `W` where $X= [x_1, x_2, x_3]$ for input vectors and $W = [w_1, w_2, w_3]$ for weight vector. For the sake of simplicity, let's assume $x_1 = 3, x_2 = -2$ and $w_0 = 1$ be the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63556f09-43f8-4fba-8eeb-2942b138f7f2",
   "metadata": {},
   "source": [
    ">You might get the impression that neural networks only understand the most useful features, but that’s not entirely true. Neural networks scoop up all the features available and give them random weights. During the training process, the neural network adjusts these weights to reflect their importance and how they should impact the output prediction. The patterns with the highest appearance frequency will have higher weights and are considered more useful features. Features with the lowest weights will have very little impact on the output (Mohammed 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea590c-44b6-442d-8900-7a20e1422915",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv18.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee8eaa-f647-4564-bb6f-25991b848243",
   "metadata": {},
   "source": [
    "   Fig.8: Source from (Mohammed 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242fb3e-4669-4428-8afb-953711e65471",
   "metadata": {},
   "source": [
    ">In both artificial and biological neural networks, a neuron does not just output the bare input it receives. Instead, there is one more step, called an activation function; this is the decision-making unit of the brain. In ANNs, the activation function takes the same weighted sum input from before ($z = Σxi · wi + b$) and activates (fires) the neuron if the weighted sum is higher than a certain `threshold`. This activation happens based on the activation function calculations (Mohammed 42). <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dce01f-7db2-40a6-9384-214313727c1a",
   "metadata": {},
   "source": [
    "### As we can see, a perceptron consists of 4 parts:\n",
    "    1. Input values or One input layer\n",
    "    1. Weights and Bias\n",
    "    1. Net sum\n",
    "    1. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03c6f4-af10-4736-869b-4e8b3b8cf934",
   "metadata": {},
   "source": [
    "According to `Mohammed`, the perceptron's learning logic goes like this:<br>\n",
    ">1. The neuron calculates the weighted sum and applies the activation function to\n",
    "make a prediction $\\hat y$. This is called the `feedforward process`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bacf78d-41cc-403b-b31f-f983bd4189bb",
   "metadata": {},
   "source": [
    "$$\\hat y = activation(\\sum x_i · w_i + b)$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c883b-63c3-47ef-99bd-4cd88b887929",
   "metadata": {},
   "source": [
    ">2. It compares the output prediction with the correct label to calculate the error:<br> $e r r o r = y – \\hat y$. <br>\n",
    "\n",
    "\n",
    ">3. It then updates the weight. If the prediction is too high, it adjusts the weight to make a lower prediction the next time, and vice versa. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14110c2d-04b7-44eb-8000-a7729ab18656",
   "metadata": {},
   "source": [
    ">4. Repeat! <br>\n",
    ">This process is repeated many times, and the neuron continues to update the weights to improve its predictions until step 2 produces a very small error (close to zero), which means the neuron’s prediction is very close to the correct value. At this point, we can stop the training and save the weight values that yielded the best results to apply to future cases where the outcome is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f7b09-d3e5-46ff-a70e-d17fb5956d7b",
   "metadata": {},
   "source": [
    "## Activation Functions:\n",
    "**a) Hiden layer activayion functions:** <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ff945-2e24-430b-b18c-8f80a592e733",
   "metadata": {},
   "source": [
    "*Example 1*: <br>\n",
    "*Sigmoid Activation Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5aabe-4dd6-4fc1-9990-58656dd3774c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60257b9-c249-4196-b459-cf48f90308ac",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph1.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27faf22-6a99-47fe-9f52-da48aa894a6a",
   "metadata": {},
   "source": [
    "*Graph1. Sigmoid function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fd71a-c8e1-4fb7-91cf-9c7f9ddc0493",
   "metadata": {},
   "source": [
    "*Example 2:* <br>\n",
    "ReLU (Rectified Linear Unit): ReLU is prefereable activation function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf86af9-a898-4b8e-a970-ca8d62879774",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = max\\{0, z\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f731e-8c35-40a5-8952-d3b1754cbce8",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph2.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f5fc8-806d-410c-86d1-90fe18c79e02",
   "metadata": {},
   "source": [
    "*Graph2. ReLU function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d980c-d5d4-46ca-abc5-74241d4eca16",
   "metadata": {},
   "source": [
    "**b) Output layer activation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10775ad0-b941-44da-bd14-ed00461dad97",
   "metadata": {},
   "source": [
    "*Exampe*: <br>\n",
    " **Softmax**:for multiclass classification <br>\n",
    "$$\n",
    " \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d434ed-f957-4031-8b0b-76554f8f3993",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph3.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33fe528-911f-43fa-97b3-08c28f5b8650",
   "metadata": {},
   "source": [
    "*Graph3. SoftMax function (source: DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0eb60-8758-42e6-aca8-9a830679a4c0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph4.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280705e6-bb5f-4f62-b59d-68a224d95086",
   "metadata": {},
   "source": [
    "Graph4. SoftMaxfunction explained ([source](https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60))\n",
    "where, according to wikipedia, it applies the standard exponential function to each element $z_i$ of the input vector $z$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma(z)$ is 1(source: [Softmax function](https://en.wikipedia.org/wiki/Softmax_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd92ed1-fce6-46e4-9d85-13770e579bff",
   "metadata": {},
   "source": [
    ">Activations in the hidden layer provide a transformation that allow the neural net\n",
    "to learn more complex relationships as calculations propagate through the\n",
    "network(Source: DSIR-111 Classnote)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b9b30-7e3d-4db4-80f8-8c87e8e0d4ae",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e023916-cb1e-46b5-8228-c91fb12f562e",
   "metadata": {},
   "source": [
    "**Loss** is a measure of performance of a model. The lower, the better. When learning, the model aims to get the lowest loss possible. We use `crossentropy` for multiclass classification([source](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496743f3-b08f-490a-9d9e-5ecd58265de1",
   "metadata": {},
   "source": [
    "## Examples:\n",
    "a) For Binary Classification: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7e8cb-766a-478c-ab11-821667a90afb",
   "metadata": {},
   "source": [
    "$$\n",
    "LOSS = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i * log(\\hat y_i) + (1-y_i) * log(1 - \\hat y_i)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c613d69-73ab-4963-b863-45df716a7bf1",
   "metadata": {},
   "source": [
    "b) For Multi-class Classification:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac10626-4c89-46b1-a4b8-f7d9c03aa260",
   "metadata": {},
   "source": [
    "$$\n",
    "LOSS = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i * log(\\hat y_i)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71790623-428b-4b5a-9b90-f86dc5c5aed9",
   "metadata": {},
   "source": [
    "Source: [Most Common Loss Functions in Machine Learning](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffcd6f-3daa-4d5e-88b7-b9ffe280c426",
   "metadata": {},
   "source": [
    "### Getting back to the perceptron: \n",
    " **Note that, the input layer is given biase, `b`, by introducing an extra input node that always has a value 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96453d38-dd33-4918-a8fd-b72b8ed2ad5b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph5.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07324027-b400-47d0-8020-c1479650890b",
   "metadata": {},
   "source": [
    "*Graph drawn by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874208f-672d-49a4-a72c-edb07c751ecb",
   "metadata": {},
   "source": [
    "Simplifying our example, let's assume $X = [x_1, x_2]$ and $W = [3, -2]$.<br>\n",
    "Then our $\\hat y = \\sigma(b + X^TW)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8ea98-c658-477f-8bc7-b23204ca35c7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/eqn1.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dc828-8003-4364-9c45-2d4f0829c703",
   "metadata": {},
   "source": [
    "*Picture by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0b1be-f20b-49d1-bd35-48e40ef45873",
   "metadata": {},
   "source": [
    "* Now, let's draw the equation of the hyperplane ( a line in 2D):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98112-8710-443c-b23b-35403190019a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph6.png\" width=\"400\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb7dab-4f0f-4041-8272-5c2e71681163",
   "metadata": {},
   "source": [
    "*Graph drawn by author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7694f8b-9f43-482b-826b-122c1f1be3c1",
   "metadata": {},
   "source": [
    "* The hyperplane (labeled by broken-line) corresponds to the decesion line that the Neural network makes to classify a given input from the ($X_1, X_2$) plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38f7e5-69d5-4fb5-8c4f-30185b054711",
   "metadata": {},
   "source": [
    "Example: Assume we have an input value $X = [-1, 2]$, and substituting $x_1 = -1$ and $x_2 = 2$ into the previous equation, $\\sigma (1 + 3x_1 - 2x_2) = \\sigma(-6)$, assume our activation function is `sigmoid`, we have: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cfc28-0adf-4993-baed-6eb8b6a722a7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    " = \\frac{1}{1+e^6} = 0.0025\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c1737-13ce-42d0-ad79-88df13470120",
   "metadata": {},
   "source": [
    "Now, since $\\hat y = \\sigma (1 + 3x_1 - 2x_2) \\approx 0.0025 < 0.5$, the activation function, `Sigmoid`, assigns our $X$ to the left of our linear classifier (the broken-line in the $(x_1,x_2)$ plane above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d148a2-0f4e-4863-b526-85501980a7bc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graph1.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83500418-2872-47e4-98de-841e4ab583fa",
   "metadata": {},
   "source": [
    "*Graph1. Sigmoid function (source from DSIR-111 presentation slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a0fa2-b187-4060-b849-f74450f5352c",
   "metadata": {},
   "source": [
    "* With this basic concept of perceptron, we intuitively conclude that multilayer perceptrons are just a stack of single layer perceptrons and hence the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966858d-af6a-4210-b36e-0f9f15e9a404",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv20.png\" width=\"600\"/> \n",
    "<div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f45210-cbf0-4c6b-b409-81cdd99d366a",
   "metadata": {},
   "source": [
    "*Fig.9: source: [Multi-Layer Neural Networks with Sigmoid Function](https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23130e-9882-4b28-8b74-0be3dfff7c73",
   "metadata": {},
   "source": [
    "- Now if we want to define a multi-output NN, we can simply add another perceptron to this above picture so instead of having one perceptron now we have two perceptrons and so on. Here is an example of a multi-output perceptron. Note that perceptron is stacked and there are two outputs ([source](https://vitalflux.com/how-do-we-build-deep-neural-network-using-perceptron/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2233a30-9d6b-4e2f-bbe4-9446e356fd55",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv21.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7777f86-c1bc-41dd-958b-9df64b215d22",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv22.png\" width=\"600\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5f5f3-11f7-4b57-b177-f05702d1930f",
   "metadata": {},
   "source": [
    "*Fig.10 Source: [Data Analytics](https://vitalflux.com/how-do-we-build-deep-neural-network-using-perceptron/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940339c1-b9ae-40ff-8210-11a7b1f655f3",
   "metadata": {},
   "source": [
    "* Activation and Loss Functions are discussed in the next subtopic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621bb66-e864-428b-b800-3fe100c6e353",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d7eda-894d-479b-b22e-f87e9e8e5589",
   "metadata": {},
   "source": [
    "The entire DL model works around the idea of extracting useful features that clearly define the objects in the image. Machine learning models are only as good as the features you provide. That means coming up with good features is an important job in building ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d64e3-cce4-4677-ba96-2b1fad232777",
   "metadata": {},
   "source": [
    ">`DEFINITION`: <br>\n",
    "A feature in machine learning is an individual measurable property or characteristic of an observed phenomenon. Features are the input that you feed to your ML model to output a prediction or classification. Suppose you want to predict the price of a house: your input features (properties) might include `square_foot`, `number_of_rooms`, `bathrooms`, and `so on`, and the model will output the predicted price based on the values of your features. Selecting good features that clearly distinguish your objects increases the predictive power of ML algorithms. <br> - In Computer Vision, a feature is a measurable piece of data in your image that is unique to that specific object. It may be a distinct color or a specific shape such as a line, edge, or image segment. A good feature is used to distinguish objects from one another (Mohammed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f1d8e3-3b24-45d1-bec2-c1654ee75ee8",
   "metadata": {},
   "source": [
    "**FEATURE GENERALIZABILITY**: A very important characteristic of a feature is repeatability.BUT, WHAT MAKES A GOOD FEATURE FOR OBJECT RECOGNITION? \n",
    "* Identifiable\n",
    "\n",
    "* Easily tracked and compared\n",
    "\n",
    "* Consistent across different scales, lighting conditions, and viewing angles\n",
    "\n",
    "* Still visible in noisy images or when only part of an object is visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a5a37-c828-43b9-8b60-43250adb777b",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "I would like to start with an example from a book, `Deep Learning for Vision Systems`, by Mohammed. <br> \n",
    "Suppose we have a database of U.S presidents and we want to build a classification pipeline to tell us which president this image is of. So we feed this image that we can see on the left hand side (`fig.7` below) to our model and we wanted to output the probability that this image is of any of these particular presidents that this dataset consists of.\n",
    "In order to classify these images correctly though, our pipeline needs to be able to tell what is actually unique about a picture of Abraham Lincoln vs a picture of any other president like George Washington or Jefferson, or Obama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480caeb-db6f-45c6-85cf-408fa0273eab",
   "metadata": {},
   "source": [
    "* Remember, **Features make pictures unique**. <br>\n",
    "Let's identify high level key features in the human, auto, and house image categories: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74007acb-9600-4814-aa2a-ece14f513687",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"images/cv6.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640ad0f-3a51-46b8-ab48-65e459ad0ac5",
   "metadata": {},
   "source": [
    "*Fig.12: Source from [Convolutional Neural Networks](http://introtodeeplearning.com)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3d29b-e308-41ff-b16a-53a76e31ad88",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv5.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83341c-53ee-4a3d-981e-e1793f9c52d8",
   "metadata": {},
   "source": [
    "*Fig.11: Source from [Convolutional Neural Networks](http://introtodeeplearning.com)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686407b-bc14-4fca-b079-6b3ede0dbbd1",
   "metadata": {},
   "source": [
    "- This way computers classify images by assigning the corresponding probabilities based on features of pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da84934-68da-40cf-b74a-85e14d867bc8",
   "metadata": {},
   "source": [
    "## Convolution Layers\n",
    "Now, suppose each feature is like a mini image; it's a patch. It's also a small 2D array of values and we'll use `filters` to pick up on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a791bf8-b4cd-4f33-be3d-6b4579a250db",
   "metadata": {},
   "source": [
    ">Convolution Layer:<br>\n",
    "The convolution layer is where we pass a filter over an image and do some calculation at each step. Specifically, we take pixels that are close to one another, then summarize them with one number. The goal of the convolution layer is to identify important features in our images, like edges.\n",
    "Source: [Here](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9f21e-de55-4d1c-ac2b-daefe289e972",
   "metadata": {},
   "source": [
    "Let's  use a $3X3$ `edge-detection` filter that amplifies the edges to the image below, then this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc3bf9-9e5a-4523-9dde-da6d2bbaf1bf",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 0\\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a5d68-157e-4de5-89cc-0b5ebd3811a6",
   "metadata": {},
   "source": [
    "`kernel` is convoluted with the input image, say $F(x,y)$, it creates a new convolved image (a feature map) that amplifies the edges (See `Fig.9` below). Zooming-in, we see `Fig.10` where a small piece of an image shows how the convolution operation is applied to get the new pixel value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560c5f9-5780-4ac6-ba1e-ad3390be39c5",
   "metadata": {},
   "source": [
    "![](images/cv7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb351676-79d2-424d-be4f-5ea199f682b9",
   "metadata": {},
   "source": [
    "![](images/cv8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2bb5a-345a-4488-a434-ff26d9b6d6c5",
   "metadata": {},
   "source": [
    "`Fig.conv: Applying Filter - source from (Mohammed 109-110)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b56ec-0882-4041-9a27-7e81140205c4",
   "metadata": {},
   "source": [
    ">Other filters can be applied to detect different types of features. For example, some filters detect `horizontal edges`, others detect `vertical edges`, still others detect more complex shapes like corners, and so on. The point is that these filters, when applied in the convolutional layers, yield feature-learning behavior: first they learn simple features like edges and straight lines, and later layers learn more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7e428-99dc-4f14-9626-038e95ee4084",
   "metadata": {},
   "source": [
    "Here are the three elements that enter into the convolution operation:\n",
    "\n",
    "* Input image\n",
    "* Feature detector or `kernel`, or `filter` used interchangeably \n",
    "* Feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5ac18-59c2-492a-bdee-c0643a48166a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv9.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d1783-3a5c-4e5e-b435-415ae178877e",
   "metadata": {},
   "source": [
    "Fig.15: [source](https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3bc501-cd6c-4d2a-8d16-f60c50782c3f",
   "metadata": {},
   "source": [
    "The example we gave above is a very simplified one, though. In reality, convolutional neural networks develop multiple feature detectors and use them to develop several feature maps which are referred to as convolutional layers.\n",
    "Through training, the network determines what features it finds important in order for it to be able to scan images and categorize them more accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187b545-2b8f-433e-a93f-6a1381ab61df",
   "metadata": {},
   "source": [
    "![](https://media3.giphy.com/media/i4NjAwytgIRDW/200.webp?cid=ecf05e471vftp51bx55s3lbh1el698xc1bv7l7rhy0igcpz3&rid=200.webp&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92825c-a06b-47a0-a265-3ceb5e49d3c6",
   "metadata": {},
   "source": [
    "### For a 3D array convolution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea78b0-fa3f-4932-85a0-6eb1f3d3aebc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/conv.gif\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a201c-895e-49d4-87b6-8cd5bfcc9acc",
   "metadata": {},
   "source": [
    "Source for the two `gif` images: [A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948cc45-462d-48ee-b397-10a828bb16ce",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"img/cnn15.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb638a-d27a-465b-ae02-208dacb1386e",
   "metadata": {},
   "source": [
    "Fig.16: [Source](https://pylessons.com/Logistic-Regression-part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fcae3-88a9-4e15-9f4a-50aca72bb2b4",
   "metadata": {},
   "source": [
    "- Putting all together:<br>\n",
    "The term **`convolution`** refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f957d0-aa88-433d-b94c-057f624ef6be",
   "metadata": {},
   "source": [
    ">We perform a series `convolution + pooling operations, followed by a number of fully connected layers`. If we are performing multiclass classification the output is softmax.[fig.*source*](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2) <br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0b8ef-4619-45ed-8d00-53961983d96d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/architecture.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a5968-2574-40e2-8944-d99969edd395",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/CNN_architecture.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679371a9-6000-495e-b6a1-12e9b896ffaf",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/CNN_from_Scratch.png\" width=\"500\"/> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476a8c9-796b-4920-a56a-ee8533436a81",
   "metadata": {},
   "source": [
    "*Fig.17: CNN Architecture ([Source](https://www.mathworks.com/videos/introduction-to-deep-learning-what-are-convolutional-neural-networks--1489512765771.html)).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21de92-3902-455b-9b9b-fb63a168fc29",
   "metadata": {},
   "source": [
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6202edf-ea04-4841-8235-b43a7ed0bd9b",
   "metadata": {},
   "source": [
    ">It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 .\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97308b8b-8fbd-494a-aea5-0b72aa7335b5",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cv14.png\" width=\"500\"/> \n",
    "<div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7048de-fc7a-49cc-9aa8-e30009aaf9bd",
   "metadata": {},
   "source": [
    "Fig.18 [Source](https://cs231n.github.io/convolutional-networks/#conv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d4951-a9d1-4308-a367-ed36191fb962",
   "metadata": {},
   "source": [
    ">Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square)(Source:[Convolutional Neural Networks for Visual recognition](https://cs231n.github.io/convolutional-networks/#conv/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a969a-92a2-46b2-801a-d60d42a483ac",
   "metadata": {},
   "source": [
    "* We'll get back to the [code notebook](https://github.com/sthirpa/Data_Scince_Immersive-at-General-Assembly-/blob/Hirpa/CIFAR-10-SH.ipynb) for the implementation of this theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176043b-1129-485d-9ebd-4a1852c64ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
